<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>In-Depth Quantum Mechanics</title>
  <meta name="description" content=" This is a guide to quantum mechanics beyond the basics, and is a follow-up to introductory quantum physics. Topics convered include state-vectors, Hilbert spaces, intrinsic spin and the Pauli matrices, the hydrogen atom in detail, the quantum harmonic oscillator, time-independent perturbation theory, and a basic overview of second quantization and relativistic quantum mechanics.
">

  
      <link rel="icon" type="image/svg+xml" href="https://jackysci.com/favicon.svg">
      <link rel="icon" type="image/png" href="https://jackysci.com/favicon.ico">
  

  
      <link rel="stylesheet" href="https://jackysci.com/site.css">
      <!--KaTeX-->
      <link rel="stylesheet" href="https://jackysci.com/katex/katex.min.css">

      <!--Inter-->
      <link rel="stylesheet" href="https://jackysci.com/inter/inter.css">
  

</head>

<body>

  <nav>
    <a href="/" class="active">Home</a>
    <!--incomplete about page
    <a href="about.html">About</a>
    -->
    <a href="https://jackysci.com/notes/">Notes</a>
    <a href="https://lightofhope.site/">Music</a>
    <a href="https://github.com/Songtech-0912">GitHub</a>
  </nav>

  
    <article class="post">
        <h1>In-Depth Quantum Mechanics</h2>
        
        <p id="print-notice">This page is print-friendly. Simply press Ctrl + P (or Command + P if you use a Mac) to print the page and download it as a PDF. <a href="https://forms.gle/msb5wKErDJCjeL5k6">Report an issue or error here</a>.</p>

        
        
        <!-- table of contents -->
        
        <details class="toc" id="toc" open>
        <summary>Table of contents</summary>
        <p class="toc-info">Note: it is highly recommended to navigate by clicking links in the table of contents! It means you can use the back button in your browser to go back to any section you were reading, so you can jump back and forth between sections!</p>
        <div>
            <ul>
            
                <li>
                    <a href="#prerequisites">Prerequisites</a>
                    
                </li>
            
                <li>
                    <a href="#foreword">Foreword</a>
                    
                </li>
            
                <li>
                    <a href="#basics-of-wave-mechanics">Basics of wave mechanics</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#introduction-to-wave-mechanics">Introduction to wave mechanics</a>
                                </li>
                            
                                <li>
                                    <a href="#the-free-particle">The free particle</a>
                                </li>
                            
                                <li>
                                    <a href="#interlude-the-fourier-transform">Interlude: the Fourier transform</a>
                                </li>
                            
                                <li>
                                    <a href="#another-look-at-the-wave-packet">Another look at the wave packet</a>
                                </li>
                            
                                <li>
                                    <a href="#the-heisenberg-uncertainty-principle">The Heisenberg uncertainty principle</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#exact-solutions-of-the-schrodinger-equation">Exact solutions of the Schrödinger equation</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#bound-and-scattering-states">Bound and scattering states</a>
                                </li>
                            
                                <li>
                                    <a href="#normalizability">Normalizability</a>
                                </li>
                            
                                <li>
                                    <a href="#the-particle-in-a-box">The particle in a box</a>
                                </li>
                            
                                <li>
                                    <a href="#the-rectangular-potential-barrier">The rectangular potential barrier</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#the-state-vector-and-its-representations">The state-vector and its representations</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#basis-representation-of-vectors">Basis representation of vectors</a>
                                </li>
                            
                                <li>
                                    <a href="#the-outer-product">The outer product</a>
                                </li>
                            
                                <li>
                                    <a href="#interlude-classifications-of-quantum-systems">Interlude: classifications of quantum systems</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#quantum-operators">Quantum operators</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#eigenstates-of-the-momentum-operator">Eigenstates of the momentum operator</a>
                                </li>
                            
                                <li>
                                    <a href="#eigenstates-of-the-position-operator">Eigenstates of the position operator</a>
                                </li>
                            
                                <li>
                                    <a href="#generalized-operators-in-bra-ket-notation">Generalized operators in bra-ket notation</a>
                                </li>
                            
                                <li>
                                    <a href="#examples-of-abstract-linear-operators">Examples of abstract linear operators</a>
                                </li>
                            
                                <li>
                                    <a href="#the-adjoint-and-hermitian-operators">The adjoint and Hermitian operators</a>
                                </li>
                            
                                <li>
                                    <a href="#matrix-representations-of-operators">Matrix representations of operators</a>
                                </li>
                            
                                <li>
                                    <a href="#representations-of-continuous-operators">Representations of continuous operators</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#observables">Observables</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#the-born-rule">The Born rule</a>
                                </li>
                            
                                <li>
                                    <a href="#degeneracy-and-cscos">Degeneracy and CSCOs</a>
                                </li>
                            
                                <li>
                                    <a href="#the-tensor-product">The tensor product</a>
                                </li>
                            
                                <li>
                                    <a href="#mathematical-properties-of-operators">Mathematical properties of operators</a>
                                </li>
                            
                                <li>
                                    <a href="#the-trace">The trace</a>
                                </li>
                            
                                <li>
                                    <a href="#commutators-and-commutation-relations">Commutators and commutation relations</a>
                                </li>
                            
                                <li>
                                    <a href="#a-summary-of-the-state-vector-formalism">A summary of the state-vector formalism</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#the-density-operator-and-density-matrix">The density operator and density matrix</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#mixed-states">Mixed states</a>
                                </li>
                            
                                <li>
                                    <a href="#the-von-neumann-equation">The Von Neumann equation</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#introduction-to-intrinsic-spins">Introduction to intrinsic spins</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#spin-and-the-stern-gerlach-experiment">Spin and the Stern-Gerlach experiment</a>
                                </li>
                            
                                <li>
                                    <a href="#repeated-measurements-of-spin">Repeated measurements of spin</a>
                                </li>
                            
                                <li>
                                    <a href="#larmor-precession">Larmor precession</a>
                                </li>
                            
                                <li>
                                    <a href="#generalized-two-level-systems">Generalized two-level systems</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#the-quantum-harmonic-oscillator">The quantum harmonic oscillator</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#the-ladder-operator-approach">The ladder operator approach</a>
                                </li>
                            
                                <li>
                                    <a href="#expectation-values-of-the-quantum-harmonic-oscillator">Expectation values of the quantum harmonic oscillator</a>
                                </li>
                            
                                <li>
                                    <a href="#the-quantum-harmonic-oscillator-in-higher-dimensions">The quantum harmonic oscillator in higher dimensions</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#time-evolution-in-quantum-systems">Time evolution in quantum systems</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#unitary-operators">Unitary operators</a>
                                </li>
                            
                                <li>
                                    <a href="#the-unitary-time-evolution-operator">The unitary time-evolution operator</a>
                                </li>
                            
                                <li>
                                    <a href="#the-heisenberg-picture">The Heisenberg picture</a>
                                </li>
                            
                                <li>
                                    <a href="#the-correspondence-principle-and-the-classical-limit">The correspondence principle and the classical limit</a>
                                </li>
                            
                                <li>
                                    <a href="#the-interaction-picture">The interaction picture</a>
                                </li>
                            
                                <li>
                                    <a href="#summary-of-time-evolution">Summary of time evolution</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#angular-momentum">Angular momentum</a>
                    
                </li>
            
                <li>
                    <a href="#stationary-perturbation-theory">Stationary perturbation theory</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#non-degenerate-perturbation-theory">Non-degenerate perturbation theory</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
                <li>
                    <a href="#advanced-quantum-theory">Advanced quantum theory</a>
                    
                        <ul>
                            
                                <li>
                                    <a href="#relativistic-wave-equations-and-the-dirac-equation">Relativistic wave equations and the Dirac equation</a>
                                </li>
                            
                                <li>
                                    <a href="#second-quantization-and-quantum-electrodynamics">Second quantization and quantum electrodynamics</a>
                                </li>
                            
                                <li>
                                    <a href="#relativity-the-dirac-equation-and-the-road-to-qft">Relativity, the Dirac equation, and the road to QFT</a>
                                </li>
                            
                                <li>
                                    <a href="#the-emergence-of-field-quanta">The emergence of field quanta</a>
                                </li>
                            
                                <li>
                                    <a href="#second-quantization">Second quantization</a>
                                </li>
                            
                                <li>
                                    <a href="#the-spin-statistics-theorem">The spin-statistics theorem</a>
                                </li>
                            
                        </ul>
                    
                </li>
            
            </ul>
        </div>
        </details>
        

        <!-- page content -->
        <p>This is a guide to quantum mechanics beyond the basics, and is a follow-up to <a href="https://jackysci.com/intro-quantum-phys/">introductory quantum physics</a>. Topics convered include state-vectors, Hilbert spaces, intrinsic spin and the Pauli matrices, the hydrogen atom in detail, the quantum harmonic oscillator, time-independent perturbation theory, and a basic overview of second quantization and relativistic quantum mechanics.</p>
<span id="continue-reading"></span>
<p>I thank <a href="https://www.xmeng.io">Professor Meng</a> at Rensselaer Polytechnic Institute, without whom this guide would not have been possible.</p>
<h2 id="prerequisites">Prerequisites</h2>
<p>This guide will assume considerable prior knowledge, including multivariable &amp; vector calculus, linear algebra, basic quantum mechanics, integration with delta functions, classical waves, Fourier series, solving boundary-value problems, and (in later chapters) tensors and special relativity. If you don't know some (or all) of these, that's okay! There are dedicated guides about each of these topics on this site:</p>
<ul>
<li>For a review of calculus (in particular multivariable and vector calculus), see the <a href="https://jackysci.com/calculus-series/">calculus series</a></li>
<li>For a review of the classical theory of waves, see the <a href="https://jackysci.com/waves-and-oscillations/">waves and oscillations guide</a></li>
<li>For a review of basic quantum mechanics, see the <a href="https://jackysci.com/intro-quantum-phys/">introductory quantum mechanics guide</a></li>
<li>For a review of electromagnetic theory, see the <a href="https://jackysci.com/electromagnetism/">fundamentals of electromagnetism guide</a> as well as the <a href="https://jackysci.com/classical-electromagnetism/">in-depth electromagnetism guide</a></li>
<li>For a review of boundary-value problems, Fourier series, and see the <a href="https://jackysci.com/intro-pdes/">PDEs guide</a></li>
<li>For a review of special relativity and tensors, see the <a href="https://jackysci.com/advanced-classical-mech/">advanced classical mechanics guide</a></li>
</ul>
<h2 id="foreword">Foreword</h2>
<p>Quantum mechanics is a fascinating subject. Developed primarily in the early 20th-century, it is a theory that (at the time of writing) is barely a hundred years old, but its impact on physics and technology is immense. Without quantum mechanics, we would not have solid-state hard drives, phones, LED lights, or MRI scanners. Quantum mechanics has revolutionized the world we live in, and this is despite the fact that it governs the behavior of particles smaller than we could ever possibly see. But understanding how matter behaves at tiny, microscopic scales is the key to understanding how matter on macroscopic scales behaves. Quantum mechanics unlocks the secrets of the microscopic world, and however unintuitive it may be, it is the best (nonrelativistic) theory we have to understand this strange, mysterious world.</p>
<h2 id="basics-of-wave-mechanics">Basics of wave mechanics</h2>
<p>In classical physics, it was well-known that there was a clear distinction between two phenomena: <em>particles</em> and <em>waves</em>. Waves are oscillations in some medium and are not localized in space; particles, by contrast, are able to move freely and are localized. We observe, however, in the quantum world, that "waves" and "particles" are both <em>incomplete descriptions</em> of matter at its most fundamental level. Quantum mechanics (at least without the inclusion of quantum field theory) does <strong>not</strong> answer <em>why</em> this is the case, but it offers a powerful theoretical framework to <em>describe</em> the wave nature of matter, called <strong>wave mechanics</strong>. In turn, understanding the wave nature of matter allows us to make powerful predictions about how matter behaves at a fundamental level, and is the foundation of modern physics.</p>
<h3 id="introduction-to-wave-mechanics">Introduction to wave mechanics</h3>
<p>From the classical theory of waves and classical dynamics, we can build up some basic (though not entirely correct) intuition for quantum theory. Consider a hydrogen atom, composed of a positively-charged nucleus, and a negatively-charged electron. Let us assume that the nucleus is so heavy compared to the electron that it may be considered essentially a classical point charge. Let us also assume that the electron orbits the nucleus at some known distance $R$. For the electron to not decay and fall into the nucleus, classical mechanics tells us that its potential energy and kinetic energy must balance each other. Thus, the electron must both be moving and have some amount of potential energy, which comes from the electrostatic attraction of the electron to the nucleus. This bears great resemblance to another system: the orbital motion of the planets around the solar system.</p>
<p>However, experiments conducted in the early 20th century revealed that atoms emit light of specific wavelengths, meaning that they could only carry discrete energies, and therefore only be found at certain locations from the nucleus. This would not be strange in and of itself, but these experiments <em>also</em> found that electrons could "jump" seemingly randomly between different orbits around the nucleus. That is to say, an electron might initially be at radius $R_1$, but then it suddenly jumps to $R_3$, then jumps to $R_2$, but cannot be found anywhere between $R_1$ and $R_2$ or between $R_2$ and $R_3$. This meant that electrons could <em>not</em> be modelled in the same way as planets orbiting the Sun - this jumpy "orbit" would be a very strange one indeed!</p>
<p>Instead, physicists wondered if it would make more sense to model electrons as <em>waves</em>. This may seem absolutely preposterous on first inspection, but it makes more sense when you think about it more deeply. First, a wave isn't localized in space; instead, it <em>fills all space</em>, so if the electron was indeed a wave, it would be possible to find the electron at different points in space ($R_1, R_2, R_3$) throughout time. In addition, waves also <em>oscillate</em> through space and time at very particular frequencies. It is common to package the <em>spatial frequency</em> of a wave via a <strong>wavevector</strong> $\mathbf{k} = \langle k_x, k_y, k_z\rangle$ and the <em>temporal frequency</em> of a wave via an <strong>angular frequency</strong> $\omega$ (for reasons we'll soon see). Since these spatial and temporal frequencies can only take particular values, the shape of the wave is also restricted to particular functions, meaning the pulses of the wave could only be found at particular locations. If we guess the pulses of the wave to be somehow linked to the position of the electron (and this is a correct guess!), that would neatly give an explanation for why electrons could only be found at particular orbits around the nucleus and <em>never</em> between two orbits.</p>
<p>The simplest types of waves are <strong>plane waves</strong>, which may be described by complex-valued exponentials $e^{i(\mathbf{k} \cdot \mathbf{x} + \omega t)}$, or by equivalent real-valued sinusoids $\cos(\mathbf{k}\cdot \mathbf{x} + \omega t)$. Thus, it would make sense to describe such waves with a complex-valued wave equation, where we use complex numbers for mathematical convenience (it is much easier to take derivatives of complex exponentials than real sinusoids). To start, let's assume that our electron is a wave whose spatial and time evolution are described by a certain function, which we'll call a <strong>wavefunction</strong> and denote $\Psi(x, t)$. We'll also assume for a free electron far away from any atom, the wavefunction has the mathematical form of a plane wave:</p>
<p class="mathcell">
$$
\Psi(x,t) = e^{i(\mathbf{k} \cdot \mathbf{x} - \omega t)}
$$
</p>
<p>Note that if we differentiate our plane-wave with respect to time, we find that:</p>
<p class="mathcell">
$$
\dfrac{\partial \Psi}{\partial t} = \dfrac{\partial}{\partial t}e^{i(\mathbf{k} \cdot \mathbf{x} - \omega t)} = \dfrac{\partial}{\partial t} e^{i\mathbf{k} \cdot \mathbf{x}} e^{-i\omega t} = -i\omega  e^{i(\mathbf{k} \cdot \mathbf{x} - \omega t)} = -i\omega \Psi(x, t)
$$
</p>
<p>We'll now introduce a historical discovery in physics that transforms this interesting but not very useful result into a powerful lead for quantum mechanics. In 1905, building on the work by German physicist Max Planck, Albert Einstein found that atoms emit and absorb energy in the form of light with a fixed amount of energy. This energy is given by the equation $E = h\nu$ (the <strong>Planck-Einstein relation</strong>), where $h = \pu{6.62607015E-34 J*s}$ is known as the <a href="https://en.wikipedia.org/wiki/Planck_constant">Planck constant</a>, and $\nu$ is the frequency of the light wave. Modern physicists usually like to write the Planck-Einstein relation in the equivalent form $E = \hbar \omega$, where $\omega = 2\pi \nu$. Armed with this information, we find that we can slightly rearrange our previous result to obtain an expression for the energy!</p>
<p class="mathcell">
$$
\omega \Psi(x, t) = \frac{1}{-i}\dfrac{\partial}{\partial t} \Psi(x, t) = i\dfrac{\partial}{\partial t} \Psi(x, t) \quad \Rightarrow \quad \hbar \omega = i\hbar\dfrac{\partial}{\partial t} \Psi(x, t) = E
$$
</p>
<blockquote>
<p><strong>Note:</strong> here, we used the fact that $\dfrac{1}{-i} = i$. You can prove this by multiplying $\dfrac{1}{-i}$ by $\dfrac{i}{i}$, which gives you $\dfrac{i}{1} = i$.</p>
</blockquote>
<p>Meanwhile, we know that the classical expression for the total energy is given by $E = K + V$, where $K$ is the kinetic energy and $V$ is the potential energy (in quantum mechanics, we often just call this the <em>potential</em> for short). The kinetic energy is related to the <em>momentum</em> $p$ of a classical particle by $K = \mathbf{p}^2/2m$ (where $\mathbf{p}^2 = \mathbf{p} \cdot \mathbf{p}$). This may initially seem relatively useless - we are talking about a quantum particle, not a classical one! - but let's assume that this equation still holds true in the quantum world.</p>
<p>Now, from experiments done in the early 20th-century, we found that all quantum particles have a fundamental quantity known as their <a href="https://en.wikipedia.org/wiki/Matter_wave">de Broglie wavelength</a> (after the French physicist Louis de Broglie who first theorized their existence), which we denote as $\lambda$. This wavelength is <em>tiny</em> - for electrons, $\lambda = \pu{167pm} = \pu{1.67E-10m}$, which is about ten million times smaller than a grain of sand. The momentum of a <em>quantum particle</em> is directly related to the de Broglie wavelength; in fact, it is given by $\mathbf{p} = \hbar \mathbf{k}$, where $|\mathbf{k}| = 2\pi/\lambda$. Combining $\mathbf{p} = \hbar \mathbf{k}$ and $K = \mathbf{p}^2/2m$, we have:</p>
<p class="mathcell">
$$
E = K + V = \dfrac{\mathbf{p}^2}{2m} + V(x) = \dfrac{(\hbar \mathbf{k})^2}{2m} + V(x)
$$
</p>
<p>Can we find another way to relate $\Psi$ and the energy $E$ using this formula? In fact, we can! If we take the <em>gradient</em> of our wavefunction, we have:</p>
<p class="mathcell">
$$
\nabla\Psi = \nabla e^{i\mathbf{k} \cdot \mathbf{x}} e^{-i\omega t} = e^{-i\omega t} \nabla e^{i\mathbf{k} \cdot \mathbf{x}}= e^{-i\omega t}i\mathbf{k} (e^{i\mathbf{k} \cdot \mathbf{x}}) = i\mathbf{k}  \Psi
$$
</p>
<p>Then, taking the divergence of the gradient (which is the <em>Laplacian operator</em> $\nabla^2 = \nabla \cdot \nabla$) we have:</p>
<p class="mathcell">
$$
\nabla^2 \Psi = (i\mathbf{k})^2 \Psi = -\mathbf{k}^2 \Psi \quad \Rightarrow \quad \mathbf{k}^2 = -\nabla^2 \Psi
$$
</p>
<p>Combining this with our classical-derived expression for the total energy, we have:</p>
<p class="mathcell">
$$
E = \dfrac{(\hbar \mathbf{k})^2}{2m} + V(x) = -\dfrac{\hbar^2}{2m}\nabla^2 \Psi + V \Psi
$$
</p>
<p>Where $V \Psi$ is some term that we presume (rightly so) to capture the <em>potential energy</em> of the quantum particle. Equating our two expressions for $E$, we have:</p>
<p class="mathcell">
$$
E = i\hbar\dfrac{\partial}{\partial t} \Psi(x, t) = -\dfrac{\hbar^2}{2m} \nabla^2 \Psi + V\Psi
$$
</p>
<p>Which gives us the <strong>Schrödinger equation</strong>:</p>
<p class="mathcell">
$$
i\hbar\dfrac{\partial}{\partial t} \Psi(x, t) = \left(-\dfrac{\hbar^2}{2m} \nabla^2 + V\right)\Psi(x,t)
$$
</p>
<p>What we have been calling $\Psi(x, t)$ can now be properly termed the <strong>wavefunction</strong>. But what is it? The predominant opinion is that the wavefunction should be considered a <strong>probability wave</strong>. That is to say, the wave is not a physically-observable quantity! We'll discuss the implications (and consequences) of this in time, but for now, we'll discuss 2 <em>real-valued</em> quantities that can be found</p>
<ol>
<li>The <strong>amplitude</strong> $|\Psi(x, t)|$ is the magnitude of the wavefunction</li>
<li>The <strong>phase</strong> $\phi = \text{arg}(\Psi) = \tan^{-1}\left(-\dfrac{\text{Im}(\Psi)}{\text{Re}(\Psi)}\right)$ describes how far along each oscillation the wavefunction has elapsed in space and time. (Here, $\text{arg}$ is the <a href="https://en.wikipedia.org/wiki/Argument_(complex_analysis)">complex-valued argument function</a>.)</li>
</ol>
<blockquote>
<p><strong>Note:</strong> In quantum field theory, the probability interpretation of the wavefunction is no longer the case; rather, $\Psi(x, t)$ is reinterpreted as a <strong>field</strong> and its real and imaginary parts are required to describe both particles and anti-particles. However, we will wait until later to introduce quantum field theory.</p>
</blockquote>
<h3 id="the-free-particle">The free particle</h3>
<p>We already know one basic solution to the Schrodinger equation (in the case $V = 0$): the case of plane waves $e^{i(kx - \omega t)}$. Because the Schrodinger equation is a linear PDE, it is possible to sum several different solutions together to form a new solution; indeed, the <em>integral</em> of a solution is also a solution! Thus, we have arrived at the solution to the Schrodinger equation for a free particle (in one dimension):</p>
<p class="mathcell">
$$
\Psi(x, t) = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dk~ g(k) e^{i(k x - \omega(k) t)}
$$
</p>
<p>This is known as a <strong>wave packet</strong>, since it is a superposition of multiple waves and in fact it  looks like a bundle of waves (as shown by the animation below)!</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/d/d8/Wave_packet_propagation_%28phase_faster_than_group%2C_nondispersive%29.gif" alt="Wavepacket animation, showing a &quot;pulse&quot; propagating along the positive x-axis" /></p>
<p><em>Source: <a href="https://en.m.wikipedia.org/wiki/File:Wave_packet_propagation_(phase_faster_than_group,_nondispersive).gif">Wikipedia</a></em></p>
<blockquote>
<p><strong>Reader's note:</strong> Another nice animation can be found at <a href="https://www.cond-mat.de/teaching/QM/JSim/wpack.html">this website</a>.</p>
</blockquote>
<p>Note that $g(k)$ is an arbitrary function that is determined by the initial conditions of the problem. In particular, using Fourier analysis we can show that it is given by:</p>
<p class="mathcell">
$$
g(k) = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dx ~\Psi(x, 0) e^{-ikx}
$$
</p>
<p>In the wave packet solution, $\omega(k)$ is called the <strong>dispersion relation</strong> and is a fundamental object of study in many areas of condensed-matter physics and diffraction theory. It relates the angular frequency (which governs the <em>time oscillation</em> of the free particle's wavefunction) to the wavenumber (which governs the <em>spatial oscillation</em> of the free particle's wavefunction). The reason we use the angular frequency rather than the "pure" frequency $\nu$ is because $\omega$ is technically the frequency at which the <em>phase</em> of the wavefunction evolves, and complex-exponentials in quantum field theory almost always take the phase as their argument. But what is $\omega(k)$? To answer this, we note that the speed of a wave is given by $v = \omega/k$. For massive particles (e.g. electrons, "massive" here means "with mass" <em>not</em> "very heavy") we can use the formula for the kinetic energy of a free particle, the de Broglie relation $p = \hbar k$ (in one dimension), and the Planck-Einstein relation $E = \hbar \omega$:</p>
<p class="mathcell">
$$
K = \dfrac{1}{2} mv^2 =  \dfrac{p^2}{2m} = \dfrac{\hbar^2 k^2}{2m} = \hbar \omega
$$
</p>
<p>Rearranging gives us:</p>
<p class="mathcell">
$$
\omega(k) = \dfrac{\hbar k^2}{2m}
$$
</p>
<p>And thus the wave packet solution becomes:</p>
<p class="mathcell">
$$
\Psi_\text{massive}(x, t) = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dk~ g(k) e^{i(k x -\frac{\hbar k^2}{2m} t)}
$$
</p>
<p>By contrast, for massless particles, the result is much simpler: we <em>always</em> have $\omega(k) = kc$ for massless particles in vacuum (the situation is more complicated for particles inside a material, but we won't consider that case for now). Thus the wave packet solution becomes:</p>
<p class="mathcell">
$$
\begin{align*}
\Psi_\text{massless}(x, t) &amp;= \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dk~ g(k) e^{i(k x -kc t)} \\
&amp;= \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dk~ g(k) e^{ik(x-ct)}
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> This is actually identical to the solution of the classical wave equation for an electromagnetic (light) wave, providing us with our first glimpse into how quantum mechanics is related to classical mechanics. The difference is that the quantum wavefunction is a probability wave, whereas the classical wave solution is a physically-measurable wave (that you can actually see!).</p>
</blockquote>
<p>Now, let's consider the case where $g(k) = \delta(k - k_0)$, where $k_0$ is a constant and is related to the particle's momentum by $p = \hbar k_0$. This physically corresponds to a particle that has an <strong>exactly-known momentum</strong>. We'll later see that such particles are actually physically impossible (because of something known as the Heisenberg uncertainty principle that we'll discuss later), but they serve as good mathematical idealizations for simplifying calculations. Placing the explicit form for $g(k)$ into the integral, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\Psi(x, t) &amp;\approx \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dk~ \delta(k - k_0) e^{i(k x - \omega t)} \\
&amp;= \dfrac{1}{\sqrt{2\pi}} e^{i(k_0 x - \omega _0 t)}, \quad \omega_0 = \omega(k_0)
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> This comes from the principal identity of the Dirac delta function, which is that $\displaystyle \int_{-\infty}^\infty dx~\delta(x - x_0) f(x) = f(x_0)$.</p>
</blockquote>
<p>Where the approximate equality is due to the fact that, again, particles with exactly-known momenta are physically impossible. This is (up to an amplitude factor) the <strong>plane-wave solution</strong> that we started with, when deriving the Schrödinger equation! From this, we have a confirmation that our derivation is indeed physically-sound and describes (idealized) quantum particles.</p>
<p>Now, let us consider the case where $g(k)$ is given by a Gaussian function:</p>
<p class="mathcell">
$$
g(k) = \dfrac{1}{(2\pi)^{1&#x2F;4} \sigma} e^{-(k-k_0)^2&#x2F;4\sigma}, \quad \sigma = \text{const.}
$$
</p>
<p>This may <em>look</em> complicated, but the constant factors are there to simplify calculations. If we substitute this into the integral, this gives us:</p>
<p class="mathcell">
$$
\Psi(x, t) = \dfrac{1}{\sqrt{2 \pi \sigma}} e^{-x^2&#x2F;\sigma^2}e^{i(k_0 x - \omega_0 t)}
$$
</p>
<p>This is <em>also</em> a Gaussian function! Notice, however, that our solution now depends on a constant $\sigma$, which controls the "width" of the wave-packet. We will soon learn that it physically corresponds to the <em>uncertainty in position</em> of the quantum particle. Keep this in mind - it will be very important later.</p>
<h4 id="the-classical-limit-of-the-free-particle">The classical limit of the free particle</h4>
<p>Now, let us discuss the classical limit of the wavefunction. We know that $\Psi(x, t)$ describes some sort of probability wave (though we haven't exactly clarified what this probability is meant to represent). We can take a guess though - while quantum particles are waves (and all matter, as far as we know, behave like quantum particles at microscopic scales), classical particles are point-like. This means that they are almost 100% likely to be present at one and exactly one location in space, which is what we classically call the <em>trajectory</em> of the particle. Thus, a classical particle would have the wavefunction approximately given by:</p>
<p class="mathcell">
$$
\Psi(x, t) \sim \delta (x - vt)
$$
</p>
<p>This is a second example of the <strong>correspondence principle</strong>, which says that in the appropriate limits, <em>quantum mechanics approximately reproduces the predictions of classical mechanics</em>. This is important since we don't usually observe quantum mechanics in everyday life, so it has to reduce to classical mechanics (which we do observe) at macroscopic scales!</p>
<h3 id="interlude-the-fourier-transform">Interlude: the Fourier transform</h3>
<p>In our analysis of the free quantum particle, we relied on a powerful mathematical tool: the <strong>Fourier transform</strong>. The Fourier transform allows us to decompose complicated functions as a sum of complex exponentials $e^{\pm ikx}$. It gives us a straightforward way to relate a particle's wavefunction in terms of its possible momenta, and vice-versa.</p>
<p>A confusing fact in physics is that there are actually <em>two</em> common conventions for the Fourier transform. The first convention, often used in electromagnetism, writes the 1D Fourier transform and inverse Fourier transform (in $k$-space, or loosely called <em>frequency space</em>) as:</p>
<p class="mathcell">
$$
\tilde f(k) = \dfrac{1}{2\pi} \int_{-\infty}^\infty f(x)e^{-ikx} dx, \quad f(x) = \dfrac{1}{2\pi} \int_{-\infty}^\infty dk \tilde f(k) e^{ikx}
$$
</p>
<p>Or equivalently, for $N$ spatial dimensions:</p>
<p class="mathcell">
$$
f(k) = \dfrac{1}{(2\pi)^N} \int_{-\infty}^\infty d^n k ~\tilde f(x)e^{-i\vec k \cdot \vec x}, \quad f(x) = \dfrac{1}{(2\pi)^N} \int_{-\infty}^\infty d^n k ~\tilde f(k)e^{i\vec k \cdot \vec x}
$$
</p>
<p>The other convention, more commonly used in quantum mechanics, writes the 1D Fourier transform as:</p>
<p class="mathcell">
$$
\tilde f(k) = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty f(x)e^{-ikx} dx, \quad f(x) = \dfrac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty dk \tilde f(k) e^{ikx}
$$
</p>
<p>The equivalent for $N$ spatial dimensions in this convention is given by:</p>
<p class="mathcell">
$$
f(k) = \dfrac{1}{(2\pi)^{N&#x2F;2}} \int_{-\infty}^\infty d^n k ~\tilde f(x)e^{-i\vec k \cdot \vec x}, \quad f(x) = \dfrac{1}{(2\pi)^{N&#x2F;2}} \int_{-\infty}^\infty d^n k ~\tilde f(k)e^{i\vec k \cdot \vec x}
$$
</p>
<p>For reasons we'll see, $k$-space in quantum mechanics is directly related to <em>momentum space</em>. We will stick with the quantum-mechanical convention (unless otherwise stated) for the rest of this guide.</p>
<h3 id="another-look-at-the-wave-packet">Another look at the wave packet</h3>
<p>Recall that our wave packet solution was given by a continuous sum of plane waves, that is:</p>
<p class="mathcell">
$$
\Psi(x, t) = \int_{-\infty}^\infty dk~ g(k) e^{i(kx - \omega(k) t)}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Throughout this guide we will frequently omit the integration bounds from $-\infty$ to $\infty$ for simplicity. Unless otherwise specified, you can safely assume that any integral written without bounds is an integral over all space (that is, over $-\infty &lt; x &lt; \infty$).</p>
</blockquote>
<p>Performing its Fourier transform yields:</p>
<p class="mathcell">
$$
\Psi(x, 0) = \dfrac{1}{\sqrt{2\pi}} \int g(k) e^{ikx} ~ dk, \quad g(k) = \dfrac{1}{\sqrt{2\pi}} \int \Psi(x&#x27;, 0) e^{-ikx&#x27;}dx&#x27;
$$
</p>
<p>Let us verify this calculation by now taking the inverse Fourier transform:</p>
<p class="mathcell">
$$
\begin{align*}
\Psi(x, 0) &amp;= \dfrac{1}{2\pi} \int dk \int dx&#x27; \Psi(x&#x27;, 0) e^{ik(x - x&#x27;)} \\
&amp;= \dfrac{1}{2\pi} \int dx&#x27; \Psi(x&#x27;, 0) \underbrace{\int dk e^{ik(x - x&#x27;)}}_{= 2\pi \delta(x - x&#x27;)} \\
&amp;= \dfrac{1}{2\pi} \int dx&#x27; \Psi(x&#x27;, 0) [2\pi \delta(x - x&#x27;)] \\
&amp;= \Psi(x, 0)
 \end{align*}
$$
</p>
<p>Where we used the Dirac function identity that $\displaystyle \int f(x')\delta(x - x') dx' = f(x)$ and the fact that the Fourier transform of $e^{ik(x - x')}$ is a delta function.</p>
<h3 id="the-heisenberg-uncertainty-principle">The Heisenberg uncertainty principle</h3>
<p>We're now ready to derive one of the most mysterious results of quantum mechanics: the <strong>Heisenberg uncertainty principle</strong>. There are a billion different ways to derive it, but the derivation we'll use is a more formal mathematical one (feel free to skip to the end of this section if this is not for you!). To start, we'll use the <strong>Bessel-Parseval relation</strong> from functional analysis, which requires that:</p>
<p class="mathcell">
$$
\int_{-\infty}^\infty dx |\Psi(x, 0)|^2 = 1 = \int dk|g(k)|^2
$$
</p>
<p>Using the de Broglie relation $p = \hbar k \Rightarrow k = p/\hbar$ we may perform a change of variables for the integral; using $dk = dp/\hbar$, and thus:</p>
<p class="mathcell">
$$
\int dk|g(k)|^2 = \dfrac{1}{\hbar}\int dp|\underbrace{g(k)&#x2F;\hbar|^2}_{|\tilde \Psi(p)|^2}  = 1
$$
</p>
<p>This allows us to now write our Fourier-transformed expressions for $\Psi(x, 0)$, which were in <strong>position-space</strong> (as they depended on $x$), now in <strong>momentum space</strong> (depending on $p$):</p>
<p class="mathcell">
$$
\begin{align*}
\underbrace{\Psi(x, 0)}_{\psi(x)} &amp;= \dfrac{1}{\sqrt{2\pi \hbar}} \int dp~\tilde \psi(p) e^{ipx&#x2F;\hbar} \\
\tilde \psi(p) &amp;= \dfrac{1}{\sqrt{2\pi \hbar}} \int dx~ \Psi(x, 0) e^{-ipx&#x2F;\hbar}
\end{align*}
$$
</p>
<p>Recognizing that $\Psi(x, 0) = \psi(x)$ (the time-independent wavefunction) we may equivalently write:</p>
<p class="mathcell">
$$
\begin{align*}
\psi(x) &amp;= \dfrac{1}{\sqrt{2\pi \hbar}} \int dp~\tilde \psi(p) e^{ipx&#x2F;\hbar} \\
\tilde \psi(p) &amp;= \dfrac{1}{\sqrt{2\pi \hbar}} \int dx~ \psi(x) e^{-ipx&#x2F;\hbar}
\end{align*}
$$
</p>
<p>Where $\tilde{\psi}(p)$ (also confusingly often denoted $\psi(p)$) is called the <strong>momentum-space wavefunction</strong>, and is the Fourier transform of the position-space wavefunction!</p>
<blockquote>
<p><strong>Note:</strong> It is a common (and extremely confusing!) convention in physics to represent the Fourier transform of a function with the same symbol. That is, it is common to write that the Fourier transform of $\psi(x)$ as simply $\psi(p)$ as opposed to a different symbol (like here, where we use $\tilde \psi(p)$). Due to its ubiquity in physics, we will adopt this convention from this time forward. However, remember that $\psi(x)$ and $\psi(p)$ are actually <strong>distinct functions</strong> that are Fourier transforms of each other, not the same function!</p>
</blockquote>
<h2 id="exact-solutions-of-the-schrodinger-equation">Exact solutions of the Schrödinger equation</h2>
<p>We'll now solve the Schrödinger equation for a greater variety of systems that have <em>exact solutions</em>. Since exact solutions to the Schrödinger equation are quite rare, these are systems that definitely worth studying! To start, recall that the Schrödinger equation reads:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t} \Psi(x, t) = -\dfrac{\hbar^2}{2m} \nabla^2\Psi + V\Psi
$$
</p>
<p>Now, the potential $V$ in the Schrödinger equation can be any function of space and time - that is, in general, $V = V(x, t)$. However, in practice, it is much easier to first start by considering only <strong>stationary states</strong> - that is, time-independent potentials. Thus, the Schrödinger equation now reads:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t} \Psi(x, t) = -\dfrac{\hbar^2}{2m} \nabla^2\Psi + V(x)\Psi
$$
</p>
<p>We will now explain a way to write out a <strong>general solution</strong> of the Schrödinger equation. But how is this possible? The reason why is that the Schrödinger equation is a <strong>linear partial differential equation (PDE)</strong>. Thus, as can be proven in it is possible to sum individual solutions together to arrive at the general solution for the Schrödinger equation for any time-independent problem.</p>
<p>Let us start by assuming that the wavefunction $\Psi(x, t)$ in one dimension can be written as the product $\Psi(x, t) = \psi(x) T(t)$. We can now use the method of <strong>separation of variables</strong>. To do so, we note that:</p>
<p class="mathcell">
$$
\begin{align*}
(\nabla^2 \Psi)_\text{1D} &amp;= \dfrac{\partial^2 \Psi}{\partial t^2} = T(t) \dfrac{\partial^2}{\partial x^2} \psi(x) = \psi&#x27;&#x27;(x) T(t) \\
\dfrac{\partial \Psi}{\partial t} &amp;= \psi(x) \dfrac{\partial^2}{\partial t^2} T(t) = \dot T(t) \psi(x)
\end{align*}
$$
</p>
<p>We will now use the shorthand $\psi'' = \psi''(x)$ and $\dot T = \dot T(t)$. Thus the Schrödinger equation becomes:</p>
<p class="mathcell">
$$
i\hbar(\psi \dot T)  = -\dfrac{\hbar^2}{2m}\psi&#x27;&#x27; T + V(x)\psi T
$$
</p>
<p>If we divide by $\psi T$ from all sides we obtain:</p>
<p class="mathcell">
$$
\begin{align*}
\dfrac{1}{\psi T} \left[i\hbar(\psi \dot T)\right] &amp;= \dfrac{1}{\psi T}\left[-\dfrac{\hbar^2}{2m}\psi&#x27;&#x27; T + V(x)\psi T\right] = i\hbar \dfrac{\dot T}{T} \\
&amp;= -\dfrac{\hbar^2}{2m} \dfrac{\psi&#x27;&#x27;}{\psi} + V \\
&amp;= E
\end{align*}
$$
</p>
<p>Where $E$ is <em>some</em> constant that we don't know the precise form of yet (if this is not making sense, you may want to review the <a href="https://jackysci.com/intro-pdes/">PDEs guide</a>), and the reason it is a constant is that two combinations of derivatives (here, $\dot T/T$ and $\psi''/\psi$) can only be equal if they are both equal to a constant. Thus, if we multiply through by $\psi$, we have now reduced the problem of finding $\Psi(x, t)$ to solving 2 simpler differential equations:</p>
<p class="mathcell">
$$
\begin{align*}
-\dfrac{\hbar^2}{2m} \psi&#x27;&#x27; + V\psi = E\psi \\
i\hbar \dfrac{dT}{dt} = E~ T(t)
\end{align*}
$$
</p>
<p>The second ODE is trivial to solve; its solution is given by plane waves in time, that is, $T(t) \sim e^{-iEt/\hbar}$. The first, however, is not so easy to solve, as we need to know what $V$ is given by, and there are some complicated potentials out there! However, we do know that once we have $\psi(x)$ (often called the <strong>time-independent wavefunction</strong> since it represents $\Psi(x, t)$ at a "snapshot" in time), then the full wavefunction $\Psi(x)$ is simply:</p>
<p class="mathcell">
$$
\Psi(x, t) = \psi(x) e^{-iEt&#x2F;\hbar}
$$
</p>
<p>But if $\psi(x)$ is one solution, it must be true that $c_1 \psi(x) + c_2 \psi(x)$ must also be a solution, and thus summing <em>any number of solutions</em> can be used to construct an arbitrary solution. This is what we mean by saying that we have found the <strong>general solution</strong> to the Schrödinger equation (at least for stationary problems, i.e. $V = V(\mathbf{x})$ and is time-independent) by just summing different solutions together! Thus, solving the ODE for $\psi(x)$ is often called the <strong>time-independent Schrödinger equation</strong>, and it is given by:</p>
<p class="mathcell">
$$
-\dfrac{\hbar^2}{2m} \dfrac{d^2 \psi}{dx^2} + V(x)\psi = E\psi
$$
</p>
<p>The <strong>most general form</strong> of the time-independent Schrödinger equation holds in two and three dimensions as well, and is given by:</p>
<p class="mathcell">
$$
\left(-\dfrac{\hbar^2}{2m} \nabla^2 + V(\mathbf{x})\right) \psi = E\psi
$$
</p>
<p>Consider several solutions $\psi_1, \psi_2, \psi_3, \dots \psi_n$ of the time-independent Schrödinger equation. Due to its linearity (as we discussed previously), a weighted sum of these solutions forms a <strong>general solution</strong>, given by:</p>
<p class="mathcell">
$$
\psi(x) = \sum_{n = 1}^\infty c_n \psi_n(x), \quad c_n = \text{const.}
$$
</p>
<p>If we back-substitute each individual solution into the time-independent Schrödinger equation, we find that the right-hand side $E$ takes distinct values $E_1, E_2, E_3, \dots E_n$. Using the fact that $\Psi(x, t) = \psi(x) T(t)$, we obtain the most general form of the solution to the Schrödinger equation:</p>
<p class="mathcell">
$$
\Psi(x, t) = \sum_{n = 1}^\infty c_n \psi_n(x) e^{-iE_n t&#x2F;\hbar} \quad \quad c_n, E_n = \text{const.}
$$
</p>
<p>This solution is <em>very general</em> since does not require us to specify $\psi_n$ and $E_n$; indeed, this general solution is correct for <em>any</em> set of solutions $\psi_n$ of the time-independent Schrödinger equation*. Of course, this form tells us very little about what the $c_n$'s or what the $\psi_n$'s should be. Finding the correct components $c_n$ highly depends on the initial and boundary conditions of the problem, without which it is impossible to determine the form of $\Psi(x, t)$. In the subsequent sections, we will explore a few simple cases where an analytical solution can be found.</p>
<blockquote>
<p>*: There are some assumptions underlying this claim, without which it is not strictly true; we'll cover the details later. For those interested in knowing why right away, the reason is that $\Psi(x, t)$ should actually be understood as a <strong>vector</strong> in an infinite-dimensional space, and $c_n$ are its components when expressed in a particular basis, whose basis vectors are given by $\psi_n$. Since vectors are basis-independent it is possible to write $\Psi(x, t)$ in terms of any chosen basis $\psi_n$ with the appropriate components $c_n$, assuming $\psi_n$ is an orthogonal and complete set of basis vectors.</p>
</blockquote>
<h3 id="bound-and-scattering-states">Bound and scattering states</h3>
<p>Solving the Schrödinger equation can be extremely difficult, if not impossible. Luckily, we often don't need to solve the Schrödinger equation to find information about a quantum system! The key is to focus on the potential $V(x)$ in the Schrödinger equation, which tells us that a solution to the Schrödinger equation comes in one of two forms: <strong>bound states</strong> or <strong>scattering states</strong>.</p>
<p>Roughly speaking, a <strong>bound state</strong> is a state where a particle is in a stable configuration, as it takes more energy to remove it from the system than keeping it in place. Meanwhile, a <strong>scattering state</strong> is a state where a particle is in an inherently unstable configuration, as it takes more energy to keep the particle in place than letting it slip away. Thus, bound states are situations where quantum particle(s) are <em>bound</em> by the potential, while scattering states are situations where quantum particles are <em>unbound</em> and are free to move. A particle in a bound state is essentially trapped in place by a potential; a particle in a scattering state, by contrast, is deflected (but not trapped!) by a potential, a collective phenomenon known as <strong>scattering</strong>.</p>
<p>At its heart, the difference between a bound state and a scattering state is in the <em>total energy</em> of a particle. A bound state occurs when the energy $E$ of a particle satisfies $E &lt; V$ for all $x$. A scattering state occurs when $E \geq V$ for all $x$. One may show this by doing simple algebraic manipulations on Schrödinger equation. A short sketch of this proof (as explained by <a href="https://faculty.rpi.edu/jian-shi">Shi, 2025</a>) is as follows. First, note that the Schrödinger equation in one dimension can be rearranged and written in the form:</p>
<p class="mathcell">
$$
\dfrac{d^2\psi}{dx^2} = -\dfrac{2m}{\hbar^2} (E  - V)\psi = \dfrac{2m}{\hbar^2} (V - E)\psi
$$
</p>
<p>When $E &lt; V$, the second derivative of the wavefunction is positive for all $x$. This means that at far distances, the Schrödinger equation approximately takes the form $\psi'' \approx \beta^2 \psi$ (where $\beta = \frac{\sqrt{2m}}{\hbar} (V(\infty) - E)$ is approximately a constant). This has solutions for $x &gt; 0$ in terms of <em>real</em> exponentials $e^{-\beta x}$, so the wavefunction decays to infinity and is normalizable, and we have a <strong>bound state</strong>. Meanwhile, when $E &gt; V$, the second derivative of the wavefunction is negative for all $x$. This means that are far distances, the Schrödinger equation approximately takes the form $\psi'' \approx - \beta^2 \psi$. This has solutions for $x&gt;0$ in terms of <em>complex exponentials</em> $e^{-i\beta x}$, so the wavefunction continues oscillating even at infinity and <em>never</em> decays to zero. This means that the wavefunction is non-normalizable, and we have a <strong>scattering state</strong>. In theory, the means that scattering-state wavefunctions are unphysical, though in practice we can ignore the normalizability requirement <em>as long as</em> we are aware that we're using a highly-simplified approximation.</p>
<h3 id="normalizability">Normalizability</h3>
<p>To understand another major difference between bound and scattering states, we need to examine the concept of <strong>normalizability</strong>. Bound states are normalizable states, meaning that they satisfy the <strong>normalization condition</strong>:</p>
<p class="mathcell">
$$
\int_{-\infty}^\infty |\psi(x)|^2 dx = 1
$$
</p>
<p>This allows their squared modulus $\rho = |\psi(x)|^2$ to be interpreted as a <strong>probability density</strong> according to the Born rule. An essential part of a bound state is that it admits solutions to $\hat H \psi = E \psi$ where $E &lt; 0$. This is required for the formation of bound states: the bound-state energy must be <em>negative</em> so that the system is stable.</p>
<p>However, scattering states are not necessarily normalizable. In theory, particles undergoing scattering should be described by wave packets. In practice, this leads to unnecessary mathematical complexity. Instead, it is common to use the <strong>non-normalizable</strong> states, typically in the form of plane waves:</p>
<p class="mathcell">
$$
\psi(x) = A e^{\pm ipx&#x2F;\hbar}
$$
</p>
<p>Since these scattering states are not normalizable, $|\psi|^2$ <em>cannot</em> be interpreted as a probability distribution. Instead, they are purely used for <em>mathematical convenience</em> with the understanding that they <em>approximately</em> describe the behavior of particles that have low uncertainty in momentum and thus high uncertainty in position. Of course, real particles are described by wave packets, which are normalizable, but using plane waves gives a good mathematical approximation to a particle whose momentum is close to perfectly-known.</p>
<p>We can in fact show this as follows. Consider a free particle at time $t = 0$, described by the wave packet solution, which we've seen at the beginning of this guide:</p>
<p class="mathcell">
$$
\psi(x) = \Psi(x, 0) = \dfrac{1}{\sqrt{2\pi \hbar}} \int dp&#x27;~\overline \Psi(p&#x27;) e^{ip&#x27;x&#x2F;\hbar}
$$
</p>
<blockquote>
<p><strong>Note:</strong> We changed the integration variable for the wavepacket from $p$ to $p'$ to avoid confusion later on. The integral expressions, however, are equivalent.</p>
</blockquote>
<p>Now, assume that the particle's momentum is confined to a <strong>small</strong> range of values, or equivalently, has a <strong>low uncertainty in momentum</strong>. We can thus make the approximation that $\overline \Psi(p') \approx \delta(p - p')$, where $p$ is the particle's momentum. Thus, performing the integration, we have:</p>
<p class="mathcell">
$$
\psi(x) \approx \dfrac{1}{\sqrt{2\pi \hbar}} \int dp&#x27;~ \delta(p - p&#x27;) e^{ip&#x27;x&#x2F;\hbar} \sim \dfrac{1}{\sqrt{2\pi}} e^{ipx&#x2F;\hbar}
$$
</p>
<p>This is exactly a plane wave in the form $\psi(x) = Ae^{ipx/\hbar}$, and a momentum eigenstate! Thus we come to the conclusion that while momentum eigenstates $\psi \sim e^{\pm ipx/\hbar}$ are non-normalizable and thus unphysical, they are a good approximation for describing particles with low uncertainty in momentum.</p>
<p>Likewise, wavefunctions of the type $\psi(x) \sim \delta(x-x_0)$ are also unphysical, and are just mathematical approximations to highly-localized wavepackets, where the particles' uncertainty in position is very small. Thus, we conclude that eigenstates of the position operator (which are delta functions) are also non-normalizable, which makes sense, since there is no such thing as a particle with exactly-known position (or momentum!).</p>
<h3 id="the-particle-in-a-box">The particle in a box</h3>
<p>We will now consider our first example of a <strong>bound state</strong>: a quantum particle confined within a small region by a step potential, also called the <strong>particle in a box</strong> or the <strong>square well</strong>. Despite its (relative) simplicity, the particle in a box is the basis for the <a href="https://en.wikipedia.org/wiki/Fermi_gas">Fermi gas model</a> in solid-state physics, so it is very important! (It is also used in describing <a href="https://en.wikipedia.org/wiki/Conjugated_system">polymers</a>, <a href="https://en.wikipedia.org/wiki/Quantum_dot">nanometer-scale semiconductors</a>, and <a href="https://en.wikipedia.org/wiki/Quantum_well_laser">quantum well lasers</a>, for those curious), for those curious). The particle in a box is described by the potential:</p>
<p class="mathcell">
$$
\begin{align*}
V(x) = \begin{cases}
V_0, &amp; x &lt; 0 \\
0, &amp; 0 \leq x \leq L \\
V_0, &amp; x &gt; L
\end{cases}
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> it is often common to use the convention that $V = 0$ for $x &lt; 0$ and $x &gt; L$ and $V = -V_0$ in the center region ($0 \leq x \leq L$). These two are <strong>entirely equivalent</strong> since they differ by only a constant energy ($V_0$) and we know that adding a constant to the potential does not change any of the physics.</p>
</blockquote>
<p>We show a drawing of the box potential below:</p>
<img class="diagram"
	
		src="/quantum-mechanics/particle-square-well.excalidraw.svg"
	
	alt="A diagram of the square well, where a particle is constrained to move between two potential barriers" />
<p>We assume that the particle has energy $E &lt; V_0$, meaning that it is a <strong>bound state</strong> and the particle is contained within the well. To start, we'll only consider the case where $V_0 \to \infty$, often called an <strong>infinite square well</strong>. This means that the particle is <em>permanently trapped</em> within the well and cannot possibly escape. In mathematical terms, it corresponds to the <strong>boundary condition</strong> that:</p>
<p class="mathcell">
$$
\begin{align*}
\psi(x) \to 0, \quad x &lt; 0 \\
\psi(x) \to 0, \quad x&gt; L
\end{align*}
$$
</p>
<p>Equivalently, we can write these boundary conditions in more standard form as:</p>
<p class="mathcell">
$$
\psi(0) = \psi(L) = 0
$$
</p>
<p>In practical terms, it simplifies our analysis so that we need only consider a <em>finite interval</em> $0 \leq x \leq L$ rather than the <em>infinite domain</em> $-\infty &lt; x &lt; \infty$, simplifying our normalization requirement.</p>
<h4 id="solving-the-schrodinger-equation-for-the-infinite-square-well">Solving the Schrödinger equation for the infinite square well</h4>
<p>To start, we'll use the tried-and-true method to first assume a form of the wavefunction as:</p>
<p class="mathcell">
$$
\psi(x) = Ae^{ikx} + A&#x27; e^{-ikx}
$$
</p>
<p>Where $A$ is some normalization faction that we will figure out later. This may <em>seem</em> unphysical (since it's made of plane waves), but it is not actually so. The reason why is that if we pick $A' = -A$, Euler's formula $e^{i\phi} = \cos \phi + i\sin \phi$ tells us that:</p>
<p class="mathcell">
$$
\begin{align*}
\psi(x) &amp;= Ae^{ikx} - Ae^{-ikx} \\
&amp;= A(e^{ikx} - e^{-ikx}) \\
&amp;= A(\cos k x + i \sin k x - \underbrace{\cos (-kx)}_{\cos(-\theta) = \cos \theta} - \underbrace{i\sin(-kx)}_{\sin(-\theta) = -\sin \theta}) \\
&amp;= A (\cos kx - \cos k x + i \sin k x - (-i \sin k x)) \\
&amp;= A (i\sin kx + i\sin k x + \cancel{\cos k x - \cos k x}^0) \\
&amp;= \beta\sin k x, \qquad \beta = 2A i
\end{align*}
$$
</p>
<p>We notice that $\psi(x) \sim \sin(kx)$ automatically satisfies $\psi(0) = 0$, which tells us that we're on the right track! Additionally, since cosine is a bounded function over a finite interval, we know it is a normalizable (and thus physically-possible) solution. However, we still need to find $k$ and the normalization factor $\beta = 2Ai$, which is what we'll do next.</p>
<p>Let's first start by finding $k$. Substituting our boundary condition $\psi(L) = 0$, we have:</p>
<p class="mathcell">
$$
\psi(L) = \beta \sin(k L) = 0
$$
</p>
<p>Since the sine function is only zero at intervals of $n\pi = 0, \pi, 2\pi, 3\pi, \dots$ (where $n$ is an integer), our above equation can only be true if $kL = n\pi$. A short rearrangement then yields $k = n\pi/L$, and thus:</p>
<p class="mathcell">
$$
\psi(x) = \beta \sin \dfrac{n\pi}{L}
$$
</p>
<p>To find $\beta$, we use the normalization condition:</p>
<p class="mathcell">
$$
\begin{align*}
1  &amp;= \int_{-\infty}^\infty \psi(x)\psi^* (x) dx \\
&amp;= \underbrace{\int_{-\infty}^0 \psi(x)^2 dx}_{0} + \int_{0}^L \psi(x)^2 dx + \underbrace{\int_L^\infty \psi(x)^2 dx}_0 \\
&amp;= \underbrace{-4A^2}_{\beta^2} \int_{0}^L \sin^2 \left(\dfrac{n\pi x}{L}\right)dx \\
&amp;= A^2 L&#x2F;2
\end{align*}
$$
</p>
<p>Where we used the integral property:</p>
<p class="mathcell">
$$
\int_a^b \sin^2 \beta x = \left[\dfrac{x}{2} - \dfrac{\sin(2\beta x)}{4\beta}\right]_a^b
$$
</p>
<p>From our result, we can solve for $A$:</p>
<p class="mathcell">
$$
A^2 L&#x2F;2 = 1 \quad \Rightarrow \quad A = \sqrt{\dfrac{2}{L}}
$$
</p>
<p>From which we obtain our <strong>position-space wavefunctions</strong>:</p>
<p class="mathcell">
$$
\psi(x) = \sqrt{\frac{2}{L}} \sin \left(\dfrac{n\pi x}{L}\right), \quad n =1, 2, 3, \dots
$$
</p>
<p>We note that a solution is present for every value of $n$. This means that we have technically found an infinite family of solutions $\psi_1, \psi_2, \psi_3, \dots, \psi_n$, each parameterized by a different value of $n$. We show a few of these solutions in the figure below:</p>
<p><img src="https://www.researchgate.net/publication/337259777/figure/fig5/AS:961466974879768@1606242998239/A-graphical-representation-of-the-particle-in-a-box-as-solution-of-the-time-independent.png" alt="Graphical plot of the wavefunction for the infinite square well" /></p>
<p><em>Source: <a href="https://www.researchgate.net/figure/A-graphical-representation-of-the-particle-in-a-box-as-solution-of-the-time-independent_fig5_337259777">ResearchGate</a>. Note that the vertical position of the different wavefunctions is for graphical purposes only.</em></p>
<h4 id="energy-of-particle-in-a-box">Energy of particle in a box</h4>
<p>Now that we have the wavefunctions, we can solve for the possible values of the energies. We can find this by plugging in our solution into the time-independent Schrödinger equation:</p>
<p class="mathcell">
$$
-\dfrac{\hbar^2}{2m}\dfrac{d^2 \psi}{dx^2} = E \psi(x)
$$
</p>
<p>Upon substituting, we have:</p>
<p class="mathcell">
$$
\begin{align*}
-\dfrac{\hbar^2}{2m}\dfrac{d^2 \psi}{dx^2} &amp;= -\frac{\hbar^2}{2m} \left(-\frac{n^2 \pi^2}{L^2}\right) \sqrt{\dfrac{2}{L}} \sin \dfrac{n\pi x}{L} \\
&amp;= \underbrace{\dfrac{n^2 \pi^2 \hbar^2}{2mL^2} \psi(x)}_{E \psi}
\end{align*}
$$
</p>
<p>From which we can easily read off the energy to be:</p>
<p class="mathcell">
$$
E_n = \dfrac{n^2 \pi^2 \hbar^2}{2mL^2}, \quad n = 1, 2, 3, \dots
$$
</p>
<p>We find that the particle always has a nonzero energy, even in its ground state. The lowest energy is called its <strong>ground-state energy</strong>, and the reason it is nonzero is that the energy-time uncertainty principle forbids a particle to have zero energy.</p>
<blockquote>
<p><strong>Note for the advanced reader:</strong> Quantum field theory gives the complete explanation for why a particle can have nonzero energy even in its ground state. The reason is that the vacuum in quantum field theory is never empty; spontaneous energy fluctuations in the vacuum lead to a nonzero energy even in the ground state, and it would take an infinite amount of energy (or equivalently, infinite time) to suppress all of these fluctuations.</p>
</blockquote>
<h3 id="the-rectangular-potential-barrier">The rectangular potential barrier</h3>
<p>We will now tackle solving our first <strong>scattering-state problem</strong>, the famous problem of the <strong>particle at a potential barrier</strong> (which is a simple model that can be used to model, among other things, the mechanics of <a href="https://en.wikipedia.org/wiki/Scanning_tunneling_microscope">scanning electron microscopes</a>). In this example, a quantum particle with energy $E$ is placed at some position in a potential given by:</p>
<p class="mathcell">
$$
V(x) = \begin{cases} 
0 &amp; x &lt; 0 \\ 
V_0 &amp; x &gt; 0 \end{cases}, \quad E &gt; V
$$
</p>
<p>(Note that the discontinuity in the potential at $x = 0$ is unimportant to the problem, although it is convenient to define $V(0) = V_0/2$). You can think of this as a quantum particle hitting a quantum "wall" of sorts; the potential blocks its path and changes its behavior, although what the particle does next defies classical intuition completely.</p>
<p>To solve this problem, we split it into two parts. For the first part, we assume that the particle initially starts from the left (that is, $x = -\infty$) and moves towards the right. This means that the particle can only be found at $x &lt; 0$. Then, the particle's initial wavefunction can (approximately) be represented as a free particle with a plane wave:</p>
<p class="mathcell">
$$
\psi_I(x) = e^{ikx}, \quad x &lt; 0, \quad k = \frac{p}{\hbar} = \dfrac{\sqrt{2mE}}{\hbar}
$$
</p>
<p>Where $\psi_I$ denotes the <em>initial</em> wavefunction (that is, the particle's wavefunction when it starts off from far away), and $k$ comes from $p = \hbar k$ and $E = p^2/2m$. We say "approximately" because we know that real particles are wavepackets, not plane waves (since plane waves are unphysical, as we have seen before); nevertheless, it is a suitable approximation for our case. We can also write the initial wavefunction in the following equivalent form:</p>
<p class="mathcell">
$$
\psi_I(x) = \begin{cases} e^{ikx}, &amp; x &lt; 0 \\ 0, &amp; x&gt; 0 \end{cases}
$$
</p>
<blockquote>
<p><strong>Note on notation:</strong> It is conventional to use positive-phase plane waves $e^{ikx}$ to describe <em>right-going particles</em> and negative-phase plane waves $e^{-ikx}$ to describe <em>left-going particles</em>.</p>
</blockquote>
<p>Now, when the particle hits the potential barrier "wall", you may expect that the particle stops or bounces back. But remember that since quantum particles are probability waves, they don't behave like classical particles. In fact, what <em>actually happens</em> is that they partially reflect (go back to $x \to -\infty$) and partially pass through (go to $x \to \infty$)! This would be like a person walking into a wall, then both passing through and bouncing back from the wall, which is truly bizarre from a classical point of view. However, it is perfectly possible for this to happen in the quantum world!</p>
<blockquote>
<p><strong>Note:</strong> This analogy is a bit oversimplified, because quantum particles are ultimately <em>probability waves</em> and it is not really the <em>particle</em> that "passes through" the "wall" but rather its wavefunction that extends both beyond and behind the potential barrier "wall". When we actually measure the particle, we don't find it "midway" through the wall; instead, we sometimes find that it is behind the wall, and at other times find that it is ahead of the wall. What is significant here is that there is a <strong>nonzero probability</strong> of the particle passing through the potential barrier, even though classical this is impossible.</p>
</blockquote>
<p>Since the particle can (roughly-speaking) exhibit both <em>reflection</em> (bouncing back from the potential barrier and traveling away to $x \to -\infty$) and <em>transmission</em> (passing through the potential barrier and traveling to $x \to \infty$), its <em>final wavefunction</em> would take the form:</p>
<p class="mathcell">
$$
\psi_F = \begin{cases} r e^{-ikx}, &amp; x &lt; 0 \\ te^{ik&#x27;x}, &amp; x &gt; 0 \end{cases}
$$
</p>
<p>Where $k'$ is the momentum of the particle if it passes through the barrier, since passing through the potential barrier saps some of its energy; mathematically, we have:</p>
<p class="mathcell">
$$
k = \dfrac{\sqrt{2mE}}{\hbar}, \quad k&#x27; = \dfrac{\sqrt{2m(V_0 - E)}}{\hbar}
$$
</p>
<p>The total wavefunction is the sum of the initial and final wavefunctions, and is given by:</p>
<p class="mathcell">
$$
\psi(x) = \psi_I + \psi_F = \begin{cases} e^{ikx} + r e^{-ikx}, &amp; x &lt; 0 \\ te^{ik&#x27;x}, &amp; x &gt; 0 \end{cases}
$$
</p>
<p>The coefficients $r$ and $t$ are the <strong>reflection coefficient</strong> and <strong>transmission coefficient</strong> respectively. This is because they represents the <em>amplitudes</em> of the particle reflecting and passing through the barrier. We also define the <strong>reflection probability</strong> $R$ and <strong>transmission probability</strong> $T$ as follows:</p>
<p class="mathcell">
$$
R = |r|^2,\quad R + T = 1
$$
</p>
<blockquote>
<p><strong>Note:</strong> The reason why $R + T = 1$ is because the particle cannot just whizz off or disappear after hitting the potential barrier; it must <em>either</em> be reflected or pass through, so conservation of probability tells us that $R + T = 1$.</p>
</blockquote>
<p>To be able to solve for what $r$ and $t$ should be, we first use the requirement that the wavefunction is <strong>continuous</strong> at $x = 0$. Why? Mathematically, this is because the Schrödinger equation is a differential equation, and the derivative of a function is <em>ill-defined</em> if the wavefunction is not continuous. Physically, this is because any jump in the wavefunction means that the probability of finding a particle in two adjacent areas in space abruptly changes without any probability of finding the particle somewhere in between, which, again, does not make physical sense. This means that at $t = 0$, the left ($x &lt; 0$) and right ($x &gt; 0$) branches of the wavefunction must be equal, or in other words:</p>
<p class="mathcell">
$$
\begin{gather*}
e^{ikx} + r e^{-ikx}  = te^{ik&#x27;x}, \quad x = 0 \\
e^0 + r e^0 = te^0 \\
1 + r = t
\end{gather*}
$$
</p>
<p>Additionally, the <em>first derivatives</em> of the left and right branches of the wavefunctions must also match for the first derivative to be continuous. After all, the Schrödinger equation is a <em>second-order</em> differential equation in space, so for the second derivative to exist, the first derivative must <em>also</em> be continuous. Thus we have:</p>
<p class="mathcell">
$$
\begin{align*}
\dfrac{\partial \psi}{\partial x}\bigg|_{x &lt; 0} &amp;= \dfrac{\partial \psi}{\partial x}\bigg|_{x &gt; 0}, \quad x = 0 \\
ik e^{ikx} - ikre^{-ikx} &amp;= ik&#x27; te^{ik&#x27;x}, \quad x = 0 \\
ik - ikr &amp;= ik&#x27; t \\
k(1 - r) &amp;= k&#x27; t
\end{align*}
$$
</p>
<p>Using these two equations, we can now find $r$ and $t$ explicitly. If we substitute $1 + r = t$, we can solve for the transmission coefficient $t$:</p>
<p class="mathcell">
$$
\begin{gather*}
k(1 - r) = k&#x27;t = k&#x27;(1 + r) \\
k - kr = k&#x27; + k&#x27; r \\
k - k&#x27; = k&#x27;r + kr \\
k - k&#x27; = r(k + k&#x27;) \\
r = \dfrac{k - k&#x27;}{k + k&#x27;}
\end{gather*}
$$
</p>
<p>Thus, we can now find the <strong>reflection probability</strong>, which is the probability the particle will be reflected after "hitting" the potential barrier:</p>
<p class="mathcell">
$$
R = |r|^2 = \left(\dfrac{k - k&#x27;}{k + k&#x27;}\right)^2
$$
</p>
<p>(Note that since $k, k'$ are real-valued, $|r|^2 = r^2$). We can also find the transmission coefficient $t$ from $t = 1+ r$:</p>
<p class="mathcell">
$$
\begin{align*}
t &amp;= 1 + r \\
&amp;= 1 + \dfrac{k - k&#x27;}{k + k&#x27;} \\
&amp;= \dfrac{k + k&#x27;}{k + k&#x27;} + \dfrac{k - k&#x27;}{k + k&#x27;} \\
&amp;= \dfrac{k + k + \cancel{k&#x27; - k&#x27;}}{k + k&#x27;} \\
&amp;= \dfrac{2k}{k + k&#x27;}
\end{align*}
$$
</p>
<p>Thus we can calculate the <strong>transmission probability</strong>:</p>
<p class="mathcell">
$$
T = 1 - R = \dfrac{4kk&#x27;}{(k + k&#x27;)^2}
$$
</p><h2 id="the-state-vector-and-its-representations">The state-vector and its representations</h2>
<p>In quantum mechanics, we have <em>said</em> that particles are probabilistic waves rather than discrete objects. This is actually only a half-truth. The more accurate picture is that quantum particles are represented by <strong>vectors in an complex space</strong>. "What??", you might say. But however strange this may first appear to be, recognizing that particles are represented by vectors is actually crucial, particularly for more advanced quantum physics (e.g. quantum field theory).</p>
<p>The fundamental vector describing a particle (or more precisely, a quantum system) is called the <strong>state-vector</strong>, and is written in the rather funny-looking notation $|\Psi\rangle$. This state-vector is not particularly easy to visualize, but one can think of it as an "arrow" of sorts that points not in real space, but in a complex space. As a particle evolves through time, it traces something akin to a "path" through this complex space. Unlike the vectors we might be used to, which live in $\mathbb{R}^3$ (t/hat is, Euclidean 3D space), this complex space (formally called a <strong>Hilbert space</strong> $\mathcal{H}$) can be of <em>any</em> number of dimensions!</p>
<p>Of course, visualizing all those dimensions in a Hilbert space is next to impossible. However, if we consider only two (complex) dimensions, the state-vector might look something like this:</p>
<p><img src="https://substackcdn.com/image/fetch/$s_!mS9U!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e74d8f2-3a44-4cf5-a1f2-ff1cf36c8eaa_1463x787.png" alt="A visualization of the state-vector" /></p>
<p><em>Credit: <a href="https://www.intoquantum.pub/p/an-introduction-to-the-hilbert-space">Dr. Ashish Bamania</a></em></p>
<blockquote>
<p><strong>Why is this drawn in 3D?</strong> The reason is that <em>each</em> complex dimension is not an <em>axis</em> (as would be the case for real dimensions), but rather a <em>complex plane</em>. This is why a two-dimensional complex space is drawn in 3D, not 2D - it is formed by taking two complex planes and placing them at 90 degrees to each other, so it has to stretch into 3D.</p>
</blockquote>
<p>To practice, let's consider a complex space with <em>three</em> dimensions, which we'll call $x$ and $y$ (though remember, these dimensions are not the physical $x,y,z$ axes). A 3-dimensional complex space is unfortunately not easily drawn, but it is simple enough that the calculations don't get too hairy!</p>
<p>Now, like the vectors we might be used to, like the position vector $\mathbf{r} = \langle x, y, z\rangle$ or momentum vector $\mathbf{p} = \langle p_x, p_y, p_z\rangle$, the state-vector also has <strong>components</strong>, although (as we discussed) these components are in general complex numbers that have no relationship to the physical $x, y, z$ axes. For our three-dimensional example, we can write the state-vector $|\Psi\rangle$ in column-vector form as follows:</p>
<p class="mathcell">
$$
|\Psi\rangle = \begin{pmatrix} c_1 \\ c_2 \\ c_3 \end{pmatrix}, \quad c_i \in \mathbb{C}
$$
</p>
<p>We might ask whether there is a <em>row-vector form</em> of a state-vector, just like classical vectors have, for instance, $\mathbf{r}^T$ and $\mathbf{p}^T$ as their row-vector forms (their <em>transpose</em>). Indeed there is an equivalent of the row-vector form for state-vectors, which we'll write as $\langle \Psi|$ (it can seem to be a funny notation but is actually very important). $\langle \Psi|$ can be written in row-vector form as:</p>
<p class="mathcell">
$$
\langle \Psi| = \begin{pmatrix} c_1^* &amp; c_2^* &amp; c_3^* \end{pmatrix}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Formally, we say that $\langle \Psi|$ is called the <strong>Hermitian conjugate</strong> of $|\Psi\rangle$, and is just a fancy name for taking the transpose of the state-vector and then complex-conjugating every component. We will see this more later.</p>
</blockquote>
<p>We now might wonder if there is some equivalent of the <em>dot product</em> for a state-vector, just like classical vectors can have dot products. Indeed, there is, although we call it the <strong>inner product</strong> as opposed to the dot product. The standard and also quite funny-looking notation is to write the inner ("dot") product of $\langle \Psi|$ and $|\Psi\rangle$ as $\langle \Psi|\Psi\rangle$, which is written as:</p>
<p class="mathcell">
$$
\begin{align*}
\langle \Psi|\Psi\rangle &amp;= \begin{pmatrix} c_1^* &amp; c_2^* &amp; c_3^* \end{pmatrix} \begin{pmatrix} c_1 \\ c_2 \\ c_3 \end{pmatrix} \\
&amp;= c_1 c_1^* + c_2 c_2^* + c_3 c_3^* \\
&amp;= |c_1|^2 + |c_2|^2 + |c_3|^2
\end{align*}
$$
</p>
<p>In quantum mechanics, we impose the restriction that $\langle \Psi|\Psi\rangle = 1$, which also means that $|c_1|^2 + |c_2|^2 + |c_3|^2 = 1$. This is the <strong>normalization condition</strong>. Indeed, it looks suspiciously-similar to our previous requirement of normalizability in wave mechanics:</p>
<p class="mathcell">
$$
\langle \Psi|\Psi\rangle = 1 \quad \Leftrightarrow \quad \int_{-\infty}^\infty \psi(x) \psi^*(x) dx = 1
$$
</p>
<p>We'll actually find later that - surprisingly - these are equivalent statements!</p>
<h3 id="basis-representation-of-vectors">Basis representation of vectors</h3>
<p>Let's return to our state-vector $|\Psi\rangle$, which we wrote in the column vector form as:</p>
<p class="mathcell">
$$
|\Psi\rangle = \begin{pmatrix} c_1 \\ c_2 \\ c_3 \end{pmatrix}, \quad c_i \in \mathbb{C}
$$
</p>
<p>Is there another way that we can write out $|\Psi\rangle$? Indeed there is! Recall that in normal space, vectors can also be written as a linear sum of <strong>basis vectors</strong>. This is also true in quantum mechanics and complex-valued spaces! For instance, we can write it out as follows:</p>
<p class="mathcell">
$$
|\psi\rangle = c_1 \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} +
c_2 \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} + c_3 \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
$$
</p>
<p>Here, $(1, 0, 0)$, $(0, 1, 0)$, and $(0, 0, 1)$ are the <strong>basis vectors</strong> we use to write out the state-vector in basis form - together, we call them a <strong>basis</strong> (plural <em>bases</em>). We can make this more compact and general if we define:</p>
<p class="mathcell">
$$
\begin{align*}
|u_1\rangle &amp;= \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \\
|u_2\rangle &amp;= \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} \\
|u_3\rangle &amp;= \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}
\end{align*}
$$
</p>
<p>Then, the linear sum of the three basis vectors can be written as follows:</p>
<p class="mathcell">
$$
|\psi\rangle = c_1 |u_1\rangle + c_2 |u_2\rangle + c_3 |u_3\rangle
$$
</p>
<p>A basis must be <em>orthonormal</em>, which means that its set of basis vectors are normalized and are orthogonal to each other. That is to say:</p>
<p class="mathcell">
$$
\langle u_i |u_j\rangle = \begin{cases}
1, &amp; |u_i\rangle = |u_j\rangle \\
0, &amp; |u_i\rangle \neq |u_j\rangle
\end{cases}
$$
</p>
<p>A basis must also be <em>complete</em>. This means that any vector in a particular space can be written as a sum of the basis vectors (with appropriate coefficients). In intuitive terms, you can arrange the basis vectors in such a way that they can form any vector you want. For instance, in the below diagram, a vector $\mathbf{u} = (2, 3, 5)$ is formed by the sum of vectors $\mathbf{v}_1$ and $\mathbf{v}_2$:</p>
<p><img src="https://ximera.osu.edu/la/LinearAlgebra/VEC-M-0090/main-figure0.svg" alt="Illustration of vectors spanning a vector space" /></p>
<p><em>Source: <a href="https://ximera.osu.edu/la/LinearAlgebra/VEC-M-0090/main">Ximera</a></em></p>
<p>A complete basis is required because otherwise, the space <em>cannot</em> be fully described by the basis vectors; there are mysterious "unreachable vectors" that exist but are "out of reach" of the basis vectors. This leads to major problems when we want to actually do physics with basis vectors, so we always want a <em>complete basis</em> in quantum mechanics. The set of all basic vectors in the basis is called the <strong>state space</strong> of the system, and physically represents all the possible states the system can be in - we'll discuss more on this later.</p>
<p>Having a <strong>complete and orthonormal basis</strong> allows us to expand an arbitrary vector $|\varphi\rangle$ in the space in terms of the basis vectors of the space:</p>
<p class="mathcell">
$$
|\varphi\rangle = \sum_i c_i |u_i\rangle
$$
</p>
<p>Where $c_i = \langle u_i|\varphi\rangle$ is the <em>probability amplitude</em> of measuring the $|u_i\rangle$ state. The reason we call it a probability <em>amplitude</em> rather than the probability itself is that $c_i$ is in general complex-valued. To get the actual probability (which we denote as $\mathcal{P}_i$), we must take its absolute value (complex norm) and square it:</p>
<p class="mathcell">
$$
\mathcal{P}_i = |c_i|^2 = c_i c_i^*
$$
</p>
<p>Note that this <em>guarantees</em> that the probability is real-valued, since the complex norm $|z|$ of any complex number $z$ is real-valued. Now, all of this comes purely from the math, but let's discuss the <em>physical interpretation</em> of our results. In quantum mechanics, we assign the following interpretations to the mathematical objects from linear algebra we have discussed:</p>
<ul>
<li>The state-vector $|\Psi\rangle$ contains all the information about a quantum system, and "lives" in a vector space called a <em>Hilbert space</em> (we often just call this a "space")</li>
<li>Each basis vector $|u_i\rangle$ in the Hilbert space represents a <strong>possible state</strong> of the system; thus, the set of all basis vectors represents <em>all possible states</em> of the system, which is why basis vectors must <em>span the space</em></li>
<li>The set of all basis vectors is called the <strong>state space</strong> of the system and describes how many states the quantum system has</li>
<li>The state-vector is a sum of the <strong>basis vectors</strong> of the space, since quantum systems (unlike classical systems) are probabilistic mixtures of different states; which particular state the system is in <strong>cannot be determined</strong> without measuring (and fundamentally disrupting) the quantum system</li>
<li>The <strong>probability</strong> of measuring the $i$-th state of the system is given by $\mathcal{P}_i = |c_i|^2$, where $c_i = \langle u_i|\Psi\rangle$</li>
</ul>
<h3 id="the-outer-product">The outer product</h3>
<p>We have already seen one way to take the product of two quantum-mechanical vectors, that being the <em>inner product</em>. But it turns out that there is another way to take the product of two vectors in quantum mechanics, and it is called the <strong>outer product</strong>. The outer product between two vectors $|\alpha\rangle, |\beta\rangle$ is written in one of two ways:</p>
<p class="mathcell">
$$
|\beta\rangle \langle \alpha |\quad \Leftrightarrow \quad |\beta\rangle \otimes \langle \alpha|
$$
</p>
<p>(Note that the $|\beta\rangle \langle \alpha|$ notation is the most commonly used). The outer product is quite a bit different from the inner product because instead of returning a scalar, it returns a <em>matrix</em>. But how do we compute it? Well, if $|\alpha\rangle$ and $|\beta\rangle$ are both three-component quantum vectors (which means they can be complex-valued) their outer product is given by:</p>
<p class="mathcell">
$$
\begin{align*}
|\beta\rangle \langle \alpha| &amp;=
\begin{pmatrix}
\beta_1^* \\ \beta_2^* \\ \beta_3^*
\end{pmatrix}^T \otimes
\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix} \\
&amp;= \begin{pmatrix}
\alpha_1 \beta_1^* &amp; \alpha_1 \beta_2^* &amp; \alpha_1 \beta_3^* \\
\alpha_2 \beta_1^* &amp; \alpha_2 \beta_2^* &amp; \alpha_2 \beta_3^* \\
\alpha_3 \beta_1^* &amp; \alpha_3 \beta_2^* &amp; \alpha_3 \beta_3^*
\end{pmatrix}
\end{align*}
$$
</p>
<p>In general, for two vectors $|\alpha\rangle, |\beta\rangle$ the matrix $C_{ij} = (|\beta\rangle \langle \alpha|)_{ij}$ has components given by:</p>
<p class="mathcell">
$$
(|\beta\rangle \langle \alpha|)_{ij} = C_{ij} = \alpha_i \beta_j^*
$$
</p>
<p>For instance, if we use this formula, the $C_{11}$ component is equal to $\alpha_1 \beta_1^*$, and the $C_{32}$ component is equal to $\alpha_3 \beta_2^*$. The outer product is a bit hard to understand in intuitive terms, so it is okay at this point to just think of the outer product as an operation that takes two vectors and gives you a matrix, just like the inner product takes two vectors and gives you a scalar.</p>
<blockquote>
<p><strong>Note:</strong> For those familiar with more advanced linear algebra, the outer product is formally the <strong>tensor product</strong> of a ket-vector and a bra-vector; in tensor notation one can use the alternate notation with $C_i{}^j = \alpha_i \beta^j$ which shares a <a href="https://www.kattemolle.com/QMvsGR/index.html">correspondence with relativistic tensor notation</a>. Another interesting article to read is this <a href="https://math.stackexchange.com/questions/4183973/intuitive-explanation-of-outer-product">Math StackExchange explanation</a> of the outer product.</p>
</blockquote>
<p>The outer product is very important because, among other reasons, it is used to express the <strong>closure relation</strong> of any vector space:</p>
<p class="mathcell">
$$
\sum_i |u_i\rangle \langle u_i | = \hat I
$$
</p>
<p>Where here, $\hat I$ is the identity matrix, and $|u_i\rangle$ are the basis vectors. What does this mean? Remember that the outer product of two vectors creates a <em>matrix</em>. The closure relation tells us that the sum of all of these matrices - formed from the basis vectors - is the identity matrix $\hat I$. Roughly speaking, this means that summing all the possible matrices formed by basis vectors allows you to get the identity matrix. This is essentially an equivalent restatement of our previous definition of <em>completeness</em>, which tells us that the set of basis vectors must <em>span the space</em> and thus any arbitrary vector can be expressed as a sum of basis vectors. This is because, assuming an arbitrary vector $|\varphi\rangle$:</p>
<p class="mathcell">
$$
\begin{align*}
|\varphi\rangle &amp;= |\varphi\rangle \\
&amp;= \hat I |\varphi\rangle  \\
&amp;= \sum_i |u_i\rangle \underbrace{\langle u_i|\varphi\rangle}_{c_i} \\
&amp;= \sum_i c_i |u_i\rangle
\end{align*}
$$
</p>
<p>Thus we find that indeed, the closure relation tells us that an arbitrary vector $|\varphi\rangle$ can be expressed as a sum of basis vectors, which is just the same thing as the requirement that the basis vectors be complete and orthonormal.</p>
<blockquote>
<p><strong>Note:</strong> It is also common to use the notation $\sum_i |c_i\rangle \langle c_i| = 1$ for the closure relation, with the implicit understanding that $1$ means the identity matrix.</p>
</blockquote>
<h3 id="interlude-classifications-of-quantum-systems">Interlude: classifications of quantum systems</h3>
<p>In quantum mechanics, we use a variety of names to describe different types of quantum systems. There are a lot of different terms we use, but let's go through short number of them. First, we may encounter <em>finite-dimensional</em> systems or <em>infinite-dimensional</em> systems. Here, <em>dimension</em> refers to the dimension of the <strong>state space</strong>, not the dimensions in 3D Cartesian space. A finite-dimensional system is spanned by a <strong>finite number</strong> of basis vectors. This means that the system can only be in a finite number of states. An analogy is that of a <strong>perfect coin toss</strong>: a coin can only be heads-up or heads-down. If we use quantum mechanical notation and denote $|h\rangle$ as the heads-up state and $|d\rangle$ as the heads-down state, we can write the "state-vector" of the coin as:</p>
<p class="mathcell">
$$
|\psi\rangle_\text{coin} = c_1 |h\rangle + c_2 |d\rangle
$$
</p>
<p>Where $c_1, c_2$ are the probability amplitudes of measuring the heads-up and heads-down states respectively. Since we know the probability is found by squaring the probability amplitudes, the probability of measuring the coin to be heads-up is $\mathcal{P}_1 = |c_1|^2$ and likewise the probability of measuring the coin to be heads-down is $\mathcal{P}_2 = |c_2|^2$. We know that a (perfect) coin toss is equally likely to be heads-up and heads-down, or in otherwise, there is a 50% probability for either heads-up or heads down, and thus $\mathcal{P}_1 = \mathcal{P}_2 = 1/2$, so we have $c_1 = c_2 = 1/\sqrt{2}$. This gives us:</p>
<p class="mathcell">
$$
|\psi\rangle_\text{coin} = \dfrac{1}{\sqrt{2}} |h\rangle + \dfrac{1}{\sqrt{2}} |d\rangle
$$
</p>
<p>An infinite-dimensional system, by contrast, is spanned by an <strong>infinite number</strong> of basis vectors. This means that the system can (in principle) be in an infinite number of states.  For instance, consider a free quantum particle moving along a line: its position is unconstrained, so it can be in any position $x \in (-\infty, \infty)$. Thus, there are indeed an infinite number of states $|x_1\rangle, |x_2\rangle, |x_3\rangle, \dots, |x_n\rangle$ (corresponding to positions $x_1, x_2, x_3, \dots, x_n$) that the particle can be in. The particle in a box is also an infinite-dimensional system, since it also has an infinite number of possible states (recall that the eigenstates can be written in the form $\psi_n(x)$, where $n$ can be arbitrarily large).</p>
<p>Another distinction between quantum systems is between <strong>continuous systems</strong> and <strong>discrete systems</strong>. A discrete system has basis vectors with <em>discrete eigenvalues</em>, while a continuous system has basis vectors with <em>continuous eigenvalues</em>. For instance, momentum basis vectors $|p_1\rangle,|p_2\rangle, |p_2\rangle$ have continuous eigenvalues, since the possible values of a particle's momentum can (usually) be any value. However, the vast majority of bases we use in quantum mechanics do <em>not</em> have continuous eigenvalues, and can only take particular values. In fact, the <em>"quantum"</em> in quantum mechanics refers to the fact that a measurement on a quantum particle frequently yields <em>discrete</em> results that are multiples of a fundamental value, called a <em>quanta</em>.</p>
<blockquote>
<p><strong>Note:</strong> It is important to note that an infinite-dimensional system may still be a discrete system. For instance, the eigenstates of the particle in a box form an infinite-dimensional state space, but as they have discrete (energy) eigenvalues, the system is still discrete.</p>
</blockquote>
<p>Differentiating between discrete and continuous systems - as well as between finite-dimensional and infinite-dimensional systems - is very important! This is because they change the way key identities are defined. For instance, in a <strong>continuous</strong> system, we can write the closure relation as:</p>
<p class="mathcell">
$$
\int |\alpha\rangle \langle \alpha| d\alpha = 1
$$
</p>
<p>And likewise, one can write out the basis expansion as:</p>
<p class="mathcell">
$$
|\psi\rangle = \int c(\alpha)|\alpha\rangle d\alpha
$$
</p>
<p>Meanwhile, for a <strong>discrete</strong> system, the basis expansion instead takes the form:</p>
<p class="mathcell">
$$
|\psi\rangle = \sum_\alpha c_\alpha |\alpha\rangle
$$
</p>
<p>And the closure relation is given by:</p>
<p class="mathcell">
$$
\sum_\alpha |\alpha \rangle \langle \alpha| = 1
$$
</p>
<p>The crucial thing here is that these expressions are <strong>extremely general</strong> - they work for any set of continuous basis vectors (for a continuous system) or discrete basis vectors (for a discrete system). It doesn't matter which basis we use!</p>
<h2 id="quantum-operators">Quantum operators</h2>
<p>In classical mechanics, physical quantities like energy, momentum, and velocity are all given by <em>functions</em> (typically of space and of time). For instance, the total energy of a system (more formally known as the <em>Hamiltonian</em>, see the <a href="https://jackysci.com/advanced-classical-mech/part-2/">guide to Lagrangian and Hamiltonian mechanics</a> for more information), is given by a function $H(x, p, t)$, where $x(t)$ is the position of the particle and $p(t)$ is its momentum (roughly-speaking). However, in quantum mechanics, each physical quantity is associated with an <strong>operator</strong> instead of a function. For instance, there is the momentum operator $\hat p$, the position operator $\hat x$, and the Hamiltonian operator $\hat H$, where the hats (represented by the symbol $\hat{}$) tell us that these are <em>operators</em>, not functions.</p>
<p>So what is an operator? An operator is something that takes one vector (or function) and transforms it to another vector (or function). A good example of an operator is a transformation matrix. Applying a transformation matrix on one vector gives us another vector, which is <em>exactly</em> what an operator does. One can also define operators that operate on <em>vectors</em> instead of functions (as a consequence, they are usually <em>differential</em> operators, meaning that they return some combination of the derivative(s) of a function). Some of these include the position operator ($\hat x$), momentum operator ($\hat p$), the kinetic energy operator $\hat K$, and the potential energy operator $\hat V$. They respectively have the forms:</p>
<p class="mathcell">
$$
\begin{align*}
\hat x &amp;= x \\
\hat p &amp;= -i\hbar \nabla \\
\hat K &amp;= \frac{\hat p^2}{2m} = -\frac{\hbar^2}{2m} \nabla^2 \\
\hat V &amp;= V(\mathbf{x})
\end{align*}
$$
</p>
<p>Combining the kinetic and potential energy operators gives us the total energy (or <em>Hamiltonian</em>) operator ($\hat H$):</p>
<p class="mathcell">
$$
\hat H = \hat K + \hat V = -\dfrac{\hbar^2}{2m} \nabla^2 + V
$$
</p><h3 id="eigenstates-of-the-momentum-operator">Eigenstates of the momentum operator</h3>
<p>For some operators, it is straightforward to find their eigenstates. For instance, if we simply solve the eigenvalue equation for the momentum operator, we have:</p>
<p class="mathcell">
$$
\hat p \psi = i\hbar \dfrac{\partial \psi}{\partial x} = p \psi
$$
</p>
<p>This differential equation has the straightforward solution $\psi(x) = e^{\pm ipx/\hbar}$, which is just a plane wave. Of course, momentum eigenstates are physically cannot exist, because real particles, of course, have to be <em>somewhere</em>, and by the Heisenberg uncertainty relation a pure momentum eigenstate means a particle can be <em>anywhere</em>! However, they are a good approximation in many cases to particles with a very small range of momenta.</p>
<h3 id="eigenstates-of-the-position-operator">Eigenstates of the position operator</h3>
<p>Similarly, the position operator's eigenstates can also be found if we write out its eigenvalue equation:</p>
<p class="mathcell">
$$
\hat x \psi = x&#x27; \psi
$$
</p>
<p>Where $x'$ is some eigenvalue of the position operator. The <em>only</em> function that satisfies this equation is the Dirac delta "function":</p>
<p class="mathcell">
$$
\psi = a\delta(x - x&#x27;), \quad a = \text{const.}
$$
</p><h4 id="eigenstates-of-the-hamiltonian">Eigenstates of the Hamiltonian</h4>
<p>The Hamiltonian operator's eigenstates can also be found through its eigenvalue equation:</p>
<p class="mathcell">
$$
\hat H \psi = \left(-\dfrac{\hbar^2}{2m}\nabla^2 + V(x)\right)\psi = E \psi
$$
</p>
<p>Notice how this is the same thing as the time-independent Schrödinger equation! Thus, the eigenstates of the Hamiltonian are the <strong>solutions to the time-independent Schrödinger equation</strong>, and the eigenvalues are the <strong>possible energies</strong> of the system.</p>
<h3 id="generalized-operators-in-bra-ket-notation">Generalized operators in bra-ket notation</h3>
<p>Up to this point, we have seen operators only in wave mechanics. Let us now generalize the notion of an operator on the state-vector, which (as we know) is the more fundamental quantity. In bra-ket notation an operator $\hat A$ is written as:</p>
<p class="mathcell">
$$
\hat A|\psi\rangle = |\psi&#x27;\rangle
$$
</p>
<p>In quantum mechanics, for the most part, we only consider <strong>linear</strong> operators. The formal definition of a <em>linear operator</em> is that the operator satisfies:</p>
<p class="mathcell">
$$
\hat A(\lambda_1 |\psi_1\rangle + \lambda_2 |\psi_2\rangle) = \lambda_1 \hat A|\psi_1\rangle + \lambda_2 |\hat \psi_2\rangle
$$
</p>
<p>As a consequence, all linear operators also satisfy:</p>
<p class="mathcell">
$$
(\hat A \hat B) |\psi\rangle = \hat A(\hat B |\psi\rangle)
$$
</p>
<p>Likewise, "sandwiching" a linear operator between a bra $|\psi\rangle$ and a ket $\langle \varphi|$ always produces a scalar $c$:</p>
<p class="mathcell">
$$
\langle \varphi| \hat A |\psi \rangle = \langle \varphi|(\hat A |\psi\rangle)) = c
$$
</p>
<blockquote>
<p><strong>Note:</strong> The scalar $c$ is frequently called in the literature as a <strong>c-number</strong> (short for "complex number"). This is to distinguish it from vectors, matrices, and operators in quantum mechanics, which are <strong>not</strong> scalars (even if they are complex-valued).</p>
</blockquote>
<p>A frequent use of this "sandwiching" is to calculate the <strong>expectation value</strong> of an operator. The expectation value of a (physically-relevant) operator $\hat A$, denoted $\langle \hat A\rangle$, is the <em>mean measured value</em> of the physical quantity the operator represents. That is to say, if we had a quantum system described by a state-vector $|\Psi\rangle$, and we wanted to measure a certain quantity that is associated with an operator $\hat A$, then the expectation value is the <em>averaged value</em> of repeatedly measuring the system. Mathematically, the expression for the expectation value is given by:</p>
<p class="mathcell">
$$
\langle A\rangle = \langle \Psi|\hat A|\Psi\rangle
$$
</p>
<blockquote>
<p><strong>Note:</strong> the expectation value of a vector operator $\mathbf{A}$ (for instance, the 3D position operator or 3D momentum operator) is a vector of its expectation values in each direction, i.e. $\langle \hat{\mathbf{A}} \rangle = (\langle \hat A_x\rangle, \langle \hat A_{y}\rangle, \langle \hat A_{z}\rangle)^T$.</p>
</blockquote>
<p>The idea of an expectation value is a very nuanced one, because the "average value" of a series of measurements has to be very precisely defined in the context of an expectation value. The expectation value is the <em>averaged value</em> you would get for some measurement of a system (position, momentum, energy, etc.) if you measure a billion identical copies of the same system <em>or</em> if you repeatedly reset the system to an identical initial state prior to each measurement. Quantum-mechanically, if we just make an arbitrary set of measurements without taking care to make sure the system starts off in the same original state, the measurements will themselves change the state of the system and spoil the average of any measurement!</p>
<h3 id="examples-of-abstract-linear-operators">Examples of abstract linear operators</h3>
<p>The first example of a linear operator we will consider is called the <strong>projection operator</strong> $\hat P$. The projection operator is defined by:</p>
<p class="mathcell">
$$
\hat P = |\alpha\rangle \langle \alpha|
$$
</p>
<p>If you have studied linear algebra, you may notice that this is very similar to the idea of a <strong>vector projection</strong>. Essentially, the projection operator tells us <em>how much</em> of one vector exists along a particular axis (although the axis is, again, in some direction in a complex Hilbert space, not real space). The component of a ket $|\psi\rangle$ in the direction of the basis vector $|\alpha\rangle$ is then given by $\hat P|\psi\rangle$. To demonstrate, let's consider a 3D ket $|\psi\rangle$ in a Hilbert space, which, in column vector form, is given by:</p>
<p class="mathcell">
$$
|\psi\rangle = \begin{pmatrix}
c_\alpha \\
c_\beta \\
c_\gamma
\end{pmatrix}
$$
</p>
<p>Let us choose an orthonormal basis ${|\alpha\rangle, |\beta\rangle, |\gamma\rangle}$ in which we can write $|\psi\rangle$ in basis-vector form as:</p>
<p class="mathcell">
$$
|\psi\rangle = c_\alpha|\alpha\rangle + c_\beta |\beta\rangle + c_\gamma|\gamma\rangle
$$
</p>
<p>Now, let us <em>operate</em> the projection operator on $|\psi\rangle$. This gives us:</p>
<p class="mathcell">
$$
\begin{align*}
\hat P |\psi\rangle &amp;= |\alpha\rangle \langle \alpha|\psi\rangle \\
&amp;= |\alpha\rangle \big[ c_\alpha \langle \alpha|\alpha\rangle + c_\beta\langle \alpha|\beta\rangle + c_\gamma \langle \alpha|\gamma\rangle\big]\\
&amp;= |\alpha\rangle \big[ c_\alpha \underbrace{\langle \alpha|\alpha\rangle}_{1} + c_\beta\cancel{\langle \alpha|\beta\rangle}^0 + c_\gamma \cancel{\langle \alpha|\gamma\rangle}^0\big]\\
&amp;= c_\alpha|\alpha\rangle
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> The above derivation works because our basis is <strong>orthonormal</strong>, meaning that the basis vectors are normalized ($\langle \alpha| \alpha\rangle = \langle \beta| \beta \rangle = \langle \gamma|\gamma \rangle = 1$) and orthogonal ($\langle i|j\rangle = 0$, for instance $\alpha|\beta\rangle = 0$).</p>
</blockquote>
<p>Another one of the <em>essential</em> properties that defines a projection operator is that $\hat P^2 = \hat P$, which is called <strong>idempotency</strong>. We can show this as follows:</p>
<p class="mathcell">
$$
\begin{align*}
\hat P^2|\psi\rangle &amp;= \hat P(\hat P|\psi\rangle) \\
&amp;= \hat P (c_\alpha|\alpha\rangle) \\
&amp;= c_\alpha \underbrace{|\alpha\rangle \langle \alpha|}_{\hat P}\alpha\rangle \\
&amp;= c_\alpha |\alpha\rangle \underbrace{\langle \alpha|\alpha\rangle}_1 \\
&amp;= c_\alpha |\alpha\rangle
\end{align*}
$$
</p>
<p>This <em>only</em> works if the projection operator is the outer product of the <strong>same state</strong> i.e. $|\alpha\rangle \langle \alpha|$ and that $|\alpha\rangle$ is a <strong>normalized vector</strong>. Indeed, $\hat A = |\psi\alpha \langle \beta |$ is <strong>not</strong> a valid projection operator, and neither is $\hat B = |\alpha\rangle \langle \alpha|)$ if $\langle a|a\rangle \neq 1$. Likewise, it is also only true if the projection operator is <strong>linear</strong>, which is what allowed us to say that $\hat P^2|\psi\rangle = (\hat P \hat P)|\psi\rangle = \hat P(\hat P |\psi\rangle)$.</p>
<h3 id="the-adjoint-and-hermitian-operators">The adjoint and Hermitian operators</h3>
<p>Another important property of nearly all operators we consider in quantum mechanics is that they are <strong>Hermitian operators</strong>. What does that mean? Well, consider an arbitrary operator. The <strong>adjoint</strong> of an operator is defined as its <em>transpose</em> with all of its components <em>complex-conjugated</em>. We notate the adjoint of an operator $\hat A$ as $\hat A^\dagger$ and read it as "A-dagger" (this is for <a href="https://hsm.stackexchange.com/questions/11426/who-introduced-the-daggersymbol-as-conjugate-transpose-in-quantum-mechanics">historical reasons</a> as physicists wanted a symbol that wouldn't be confused with the complex conjugate symbol, <em>not</em> because physicists like to swordfight!). The idea of adjoints may sound quite abstract, so let's see an example for a 2D Hilbert space. Let us assume that we have some operator $\hat A$, whose matrix representation is as follows:</p>
<p class="mathcell">
$$
\hat A = \begin{pmatrix}
c_{11} &amp; c_{12} \\
c_{21} &amp; c_{22}
\end{pmatrix}
$$
</p>
<p>Then, the adjoint $\hat A^\dagger$ of $\hat A$ is given by:</p>
<p class="mathcell">
$$
\hat A^\dagger =\begin{pmatrix}
c_{11}^* &amp; c_{12}^* \\
c_{21}^* &amp; c_{22}^*
\end{pmatrix}^T 
= \begin{pmatrix}
c_{11}^* &amp; c_{21}^* \\
c_{12}^* &amp; c_{22}^*
\end{pmatrix}
$$
</p>
<p>For example, let's take the adjoint of a very famous matrix operator in quantum mechanics (the Pauli $y$-matrix):</p>
<p class="mathcell">
$$
\begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \end{pmatrix}^\dagger = 
\begin{pmatrix}
0&amp; -i \\
i &amp; 0
\end{pmatrix}
$$
</p>
<p>Notice here that to find the adjoint of $\hat A$ we <strong>complex-conjugated</strong> every component of the matrix, and then transposed the matrix. This procedure works for <em>any</em> operator when it is in matrix representation. This is a straightforward rule to remember - if we're given an operator in matrix form, complex-conjugation and transposing gives us the adjoint.</p>
<p>Unfortunately, if we consider an <em>abstract operator</em> (for instance, the projection operator) where we don't know its matrix form, there is usually <strong>no general formula</strong> that relates $\hat A$ and $\hat A^\dagger$. However, there are some special cases:</p>
<ul>
<li>A <strong>constant</strong> $c$ has adjoint $c^\dagger = c^*$</li>
<li>A <strong>ket</strong> $|a\rangle$ has adjoint $|a\rangle^\dagger = \langle a|$</li>
<li>A <strong>bra</strong> $\langle b|$ has adjoint $\langle b|^\dagger = |b\rangle$</li>
<li>An <strong>operator</strong> $\hat A$ satisfying $\hat A|\alpha\rangle = |\beta\rangle$ has $\langle \alpha|\hat A^\dagger = \langle \beta|$</li>
<li>An <strong>inner product</strong> $\langle a|b\rangle$ has adjoint $\langle a|b\rangle^\dagger = \langle b|a\rangle$</li>
<li>An <strong>operator inner product</strong> $\langle a|\hat A|b\rangle$ has adjoint $\langle b|\hat A^\dagger|a\rangle^*$</li>
<li>An <strong>outer product</strong> $|a\rangle \langle b|$ has adjoint $(|a\rangle \langle b|)^\dagger = |b\rangle \langle a|$</li>
</ul>
<p>A few useful properties of the adjoint are also listed below:</p>
<ul>
<li>$(\hat A^\dagger)^\dagger = \hat A$</li>
<li>$(\lambda \hat A)^\dagger = \lambda^* A^\dagger$</li>
<li>$(\hat A + \hat B)^\dagger = \hat A^\dagger + \hat B^\dagger$</li>
<li>$(\hat A \hat B)^\dagger = \hat B^\dagger \hat A^\dagger$</li>
</ul>
<p>We will now introduce a special class of operators, known as <strong>Hermitian operators</strong>. The essential property of a Hermitian operator $\hat A$ is that it is <strong>equal</strong> to its adjoint:</p>
<p class="mathcell">
$$
\hat A = \hat A^\dagger
$$
</p>
<p>An operator with this property is said to be <em>self-adjoint</em>, and thus Hermitian operators are often also called <em>self-adjoint operators</em> (mathematicians are more careful with the terminology, but for physicists <em>self-adjoint</em> and <em>Hermitian</em> mean the same thing). While this may seem like a relatively arbitrary property, it is actually <em>very</em> useful, because it allows us to manipulate operators in very convenient ways. A Hermitian operator, for instance, satisfies:</p>
<p class="mathcell">
$$
\langle \beta |\hat A|\alpha\rangle = \langle \beta |\hat A^\dagger|\alpha\rangle = \langle \alpha |\hat A|\beta\rangle^*
$$
</p>
<p>The fact that we can just "flip" bras and kets around, such that $\langle \beta |\hat A|\alpha\rangle = \langle \alpha|\hat A|\beta\rangle^*$ <em>only</em> works because $\hat A$ is a Hermitian operator! In addition, a Hermitian operator satisfies the <strong>self-adjoint</strong> property:</p>
<p class="mathcell">
$$
\langle \varphi | \cdot \hat A|\psi\rangle =\langle \varphi | \hat A \cdot  |\psi\rangle
$$
</p>
<p>(Here $\cdot$ is the <em>inner product</em> but we write it in dot product notation for clarity). This all-important property is the bedrock for a lot of quantum mechanics, and is (one of the) reasons we demand our operators to be Hermitian!</p>
<blockquote>
<p><strong>Note:</strong> Mathematicians typically use the alternative notation $\langle \varphi, \hat A \psi\rangle = \langle \varphi \hat A, \psi\rangle$ which means the same thing even though the notation is different.</p>
</blockquote>
<h3 id="matrix-representations-of-operators">Matrix representations of operators</h3>
<p>The idea of an operator in quantum mechanics is very abstract, and we are not always provided (indeed, there may not even exist!) the matrix form of an operator, just like we often don't know the column/row-vector form of the state-vector. However, since matrices and vectors are often much easier to work with than abstract operators and bras/kets, it is often very <em>useful</em> to find the matrix form of the operator, also known as its <strong>matrix representation</strong>.</p>
<p>First, we should note that a matrix representation of an operator can only be found when you <strong>set your basis</strong>. To find a matrix representation $A_{ij}$ of a discrete operator $\hat A$ in some given basis (let's call this basis $|\alpha\rangle$ for clarity), we use the formula:</p>
<p class="mathcell">
$$
A_{ij} = \langle \alpha_i| \hat A |\hat \alpha_j\rangle
$$
</p>
<p>Let's take the example of the <em>projection operator</em> $\hat P = |\alpha\rangle \langle \alpha|$, where $|\alpha\rangle$ is the normalized state-vector of the system, and can be written out in the basis representation as:</p>
<p class="mathcell">
$$
|\alpha\rangle = \sum_i c_i |\alpha_i\rangle = \sum_i c_j |\alpha_j\rangle
$$
</p>
<p>Plugging in the explicit form of the projection operator gives us:</p>
<p class="mathcell">
$$
\begin{align*}
P_{ij} &amp;= \langle \alpha_i |\hat P |a_j \rangle \\
&amp;= \underbrace{\langle \alpha_i| \alpha\rangle}_{c_i} \underbrace{\langle \alpha| \alpha_j \rangle}_{c_j} \\
&amp;= c_i\delta_{ij} c_j\delta_{ij} \\
&amp;= c_i c_jI
\end{align*}
$$
</p>
<p>Which is simply the identity matrix. For instance, for a 3-dimensional system, $P_{ij}$ would be:</p>
<p class="mathcell">
$$
P_{ij} = \begin{pmatrix}
c_1 c_1 &amp; 0 &amp; 0 \\
0 &amp; c_2 c_2&amp; 0 \\
0 &amp; 0 &amp; c_3 c_3
\end{pmatrix}
$$
</p>
<p>Utilizing the matrix representation of operators is often very useful for finite-dimensional and infinite-dimensional systems alike. For instance, a formula we will use very frequently later to find the matrix representation of the Hamiltonian $\hat H$ in terms of some basis $|\varphi\rangle$ is as follows:</p>
<p class="mathcell">
$$
H_{ij} = \langle \varphi_i|\hat H|\varphi_j\rangle
$$
</p>
<p>Usually, we choose $|\varphi\rangle$ to be the <em>eigenstates</em> of the Hamiltonian operator, and this method is very useful for solving many problems - but let's not get too ahead of ourselves, we'll get to there!</p>
<h3 id="representations-of-continuous-operators">Representations of continuous operators</h3>
<p>Let's now turn our attention to situations where we have a <strong>continuous basis</strong> (i.e. one where the basis has continuous eigenvalues). It is also possible to find the representation of an operator in continuous bases, although these representations are not usually written out in matrix form. For instance, consider an operator that is defined as follows (in the position basis):</p>
<p class="mathcell">
$$
F = \dfrac{\partial}{\partial x}
$$
</p>
<p>Then, applying the operator on some function $f(x)$ gives a new function $g(x)$, which is the <em>derivative</em> of the original function:</p>
<p class="mathcell">
$$
\hat F[f(x)] \to \dfrac{\partial f}{\partial x} = g(x)
$$
</p>
<p>This may not <em>look</em> like the matrix forms of operators that we saw previously. But this is a false distinction that arises due to mathematical notation; the representation of an operator is really just the same thing as an infinite-dimensional matrix. This is because functions are <a href="https://thenumb.at/Functions-are-Vectors/">really just infinite-dimensional vectors</a>, so an operator that takes some function $f(x)$ and returns a new function $g(x)$ is really the same thing as applying a matrix to a vector, giving us a new vector. The only difference is that we're now working with <em>continuous</em> basis vectors. Indeed, this is why we use a <em>Hilbert space</em> in quantum mechanics, because a Hilbert space can have an <em>arbitrary</em> number of dimensions, unlike the Euclidean space of classical mechanics.</p>
<blockquote>
<p><strong>Note:</strong> Those familiar with more in-depth linear algebra will note that the most general representation of an operator in this context is more formally termed a <em>linear map</em> between two infinite-dimensional spaces.</p>
</blockquote>
<p>The two continuous bases that we will find most commonly are the <strong>position basis</strong> $|x\rangle$ and <strong>momentum basis</strong> $|p\rangle$. Just like any other basis, they satisfy orthogonality:</p>
<p class="mathcell">
$$
\begin{align*}
\langle x|x&#x27;\rangle &amp;= \delta(x&#x27;-x) \\
\langle p|p&#x27;\rangle &amp;= \delta(p&#x27;-p)
\end{align*}
$$
</p>
<p>Likewise, they satisfy <strong>closure</strong>:</p>
<p class="mathcell">
$$
\begin{align*}
\int dx~ |x\rangle \langle x| &amp;= 1 \\
\int dp~ |p\rangle \langle p| &amp;= 1
\end{align*}
$$
</p>
<p>Another essential property is that the inner product between any two basis vectors $|x\rangle, |p\rangle$ satisfies:</p>
<p class="mathcell">
$$
\langle x|p\rangle = \dfrac{1}{\sqrt{2\pi}} e^{ipx&#x2F;\hbar}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Depending on the text and choice of normalization, there may be a prefactor of $\dfrac{1}{\sqrt{2\pi \hbar}}$ as opposed to $\dfrac{1}{\sqrt{2\pi}}$ in the inner product.</p>
</blockquote>
<p>We can prove this by solving for the eigenvectors of the position and momentum operators. The position operator is defined as $\hat x = x$, while the momentum operator is defined as $\hat p = -i\hbar \dfrac{\partial}{\partial x}$. Solving the eigenvalue equations for each gives us:</p>
<p class="mathcell">
$$
\begin{align*}
\hat x|x\rangle &amp;= x|x\rangle \quad \Rightarrow \quad |x\rangle = \delta(x - x&#x27;) \\
\hat p|p\rangle &amp;= p|p\rangle \quad \Rightarrow \quad |p\rangle = \frac{1}{\sqrt{2\pi}}e^{ipx&#x2F;\hbar}
\end{align*}
$$
</p>
<p>Since the eigenvectors in our case are functions (which are the same thing as infinite-dimensional vectors), the inner product becomes an integral:</p>
<p class="mathcell">
$$
\langle x|p\rangle = \int dx&#x27;\delta(x - x&#x27;) \frac{1}{\sqrt{2\pi}}e^{ipx&#x27;&#x2F;\hbar}= \dfrac{1}{\sqrt{2\pi}} e^{ipx&#x2F;\hbar}
$$
</p>
<p>What about taking the inner product of $|x\rangle, |p\rangle$ with the state-vector $|\Psi\rangle$? Well, since the state-vector evolves with time, that is, $|\Psi\rangle = |\Psi(t)\rangle$, let's consider the state-vector at an instant in time, say, $t = 0$. Then we define $|\psi\rangle = |\Psi(0)\rangle$ to be the <strong>time-independent state-vector</strong>. The inner product of $|\psi\rangle$ with $|x\rangle$ then tells us the components of $|\psi\rangle$ in the position basis, so we have:</p>
<p class="mathcell">
$$
\psi(x) = \langle x|\psi\rangle
$$
</p>
<p>But this is just the position-space wavefunction! Meanwhile, the inner product of $|\psi\rangle$ with $|p\rangle$ then tells us the components of $|\psi\rangle$ in the momentum basis, so:</p>
<p class="mathcell">
$$
\tilde \psi(p) = \langle p|\psi\rangle
$$
</p>
<p>We have thus uncovered the surprising fact that <strong>the wavefunction is just the state-vector's components in a particular basis</strong>. This is why we say that the state-vector is the more fundamental quantity! With the full power of Dirac notation, we can also use the closure relation to tell us that:</p>
<p class="mathcell">
$$
\begin{align*}
\psi(x) &amp;= \langle x|\psi\rangle \\ &amp;= \int d^3p \langle x|p\rangle \langle p|\psi\rangle \\ &amp;= \dfrac{1}{\sqrt{2\pi \hbar}} \int dp~\tilde \psi(p) e^{ipx&#x2F;\hbar} \\
\tilde \psi(p) &amp;= \langle p|\psi\rangle \\
&amp;= \int d^3x \langle p|x\rangle \langle x|\psi\rangle \\
&amp;= \dfrac{1}{\sqrt{2\pi \hbar}} \int dx~\psi(x) e^{-ipx&#x2F;\hbar}
\end{align*}
$$
</p>
<p>Indeed, these are just our definitions of the position and momentum-space wavefunctions in terms of Fourier transforms of each other!</p>
<p>In higher dimensions, we can define the 3D versions of the position and momentum operators:</p>
<p class="mathcell">
$$
\hat{\mathbf{x}} = \begin{pmatrix} \hat x \\ \hat y \\ \hat z \end{pmatrix}, \quad 
\mathbf{\hat p} = \begin{pmatrix} \hat p_x \\ \hat p_y \\ \hat p_z \end{pmatrix}, \quad
$$
</p>
<p>And in $N$ dimensions, we have:</p>
<p class="mathcell">
$$
\langle \mathbf{x}|\mathbf{p}\rangle = \dfrac{1}{(2\pi)^{3&#x2F;N}} e^{i\mathbf{p} \cdot \mathbf{x}&#x2F;\hbar}
$$
</p>
<blockquote>
<p><strong>Note on notation:</strong> It is sometimes the case that $\hat{\mathbf{x}}$ is written as $\hat{\mathbf{R}}$ or $\hat{\mathbf{X}}$ and $\mathbf{\hat p}$ is written as $\hat{\mathbf{P}}$ instead. We will use these notations interchangeably.</p>
</blockquote>
<h2 id="observables">Observables</h2>
<p>Having wandered far in math-land, let us return back to physics, and discuss one of the most important topics in quantum mechanics: <strong>observables</strong>. An observable is, roughly speaking, something you can <em>measure</em> about a quantum particle. The position $x$ and the momentum $p$ of a particle, for instance, are observables. We already know that observables in quantum mechanics are represented by <strong>operators</strong>, not numbers or functions. For instance, we saw the position operator $\hat x$, the momentum operator $\hat p$, and the projection operator $\hat P$.</p>
<blockquote>
<p><strong>Note on notation:</strong> We will generally denote the observable in question without the operator hat, whereas the operator <em>associated with</em> the observable is notated with the hat. For instance, if I have an observable $A$, then its associated operator is written as $\hat A$. Similarly, if I have observable $x$ (position), then its associated operator is written as $\hat x$ (which we recognize as the position operator).</p>
</blockquote>
<p>But if observables like position and momentum are associated with operators and not functions, how can we know the <em>physical values</em> of the position and momentum of a quantum particle? In other words, how can we get out a real, measurable number from complex-valued state-vectors and operators in a Hilbert space? The answer comes from <strong>eigenvalues</strong> - by finding the eigenvalues of an operator, we can get a scalar out, and this scalar is a number you can actually measure!</p>
<p>In pure mathematics, there are essentially no restrictions (other than linearity) on linear operators. But quantum mechanics stipulates that the operators with <strong>physical significance</strong> must satisfy an <strong>eigenvalue equation</strong> in the form:</p>
<p class="mathcell">
$$
\hat A|\varphi\rangle = \lambda |\varphi\rangle
$$
</p>
<p>Where here, $|\varphi\rangle$ is called the <strong>eigenvector</strong> (or <em>eigenstate</em>) of $\hat A$ and $\lambda$ is the <strong>eigenvalue</strong>. Furthermore, $\hat A$ is required to be a <strong>Hermitian operator</strong>. Why? Since an observable is something you <em>measure</em>, it's got to be a real number! This is automatically satisfied by Hermitian operators, since one may show mathematically that:</p>
<ol>
<li>The eigenvalues of a Hermitian operator are <strong>real</strong></li>
<li>The eigenvectors of an Hermitian operator are <strong>orthogonal</strong></li>
<li>The set of all eigenvectors of a Hermitian operator form an <strong>orthonormal basis</strong> in the space</li>
</ol>
<p>Remember that we said previously that <strong>basis vectors represent possible states of a quantum system</strong>. Since the eigenvectors of a Hermitian operator automatically form an orthonormal basis, combining our two statements leads to several profound conclusions, which form the <a href="https://en.wikipedia.org/wiki/Mathematical_formulation_of_quantum_mechanics">fundamental postulates of quantum mechanics</a>:</p>
<blockquote>
<p><strong>Postulate I of quantum mechanics:</strong> A quantum system is described by a <strong>state vector</strong> $|\Psi\rangle$, which exists in a complex-valued Hilbert space $\mathcal{H}$ of arbitrary dimensions.</p>
</blockquote>
<blockquote>
<p><strong>Postulate II(a) of quantum mechanics:</strong> Observables (physical quantities) are represented by <strong>Hermitian operators</strong>, whose eigenvectors are the <em>possible states</em> of a quantum system (termed its <strong>eigenstates</strong>), and whose eigenvalues are the <em>measurable values</em> of the observable. The state-vector $|\Psi\rangle$ is a <em>superposition</em> of the eigenstates of the system.</p>
</blockquote>
<blockquote>
<p><strong>Postulate II(b) of quantum mechanics:</strong> The <strong>probability amplitude</strong> of measuring some eigenstate $|u_i\rangle$ of the system is the inner product $\langle u_i|\Psi\rangle$ of the eigenstate with the state-vector, and the probability $\mathcal{P}_i$ is given by the <em>squared norm</em> $|\langle u_i|\Psi\rangle|^2$ of the probability amplitude.</p>
</blockquote>
<p>Together with the requirement of the conservation of probability, these postulates are at the heart of quantum mechanics and form the basis of the rigorous formulation of quantum mechanics from a mathematical standpoint. In other words, they're really important!</p>
<h3 id="the-born-rule">The Born rule</h3>
<p>Let's take a closer look at <em>postulate II(b)</em>. This postulate is more formally known as the <strong>Born rule</strong>:</p>
<blockquote>
<p><strong>Born rule (for continuous quantities):</strong> For any continuous observable represented by operator $\hat a$, with eigenstates $|\alpha\rangle$ and eigenvalues $\alpha$, the wavefunction $\psi(\alpha)$ represents the <em>probability amplitude</em> $c(\alpha)$ of measuring the corresponding observable's value to be $\alpha$.</p>
</blockquote>
<blockquote>
<p><strong>Born rule (for discrete quantities):</strong> For any discrete (quantized) observable represented by operator $\hat a$, with eigenstates $|\alpha\rangle$ and eigenvalues $\alpha$, the wavefunction $\psi_\alpha$ represents the <em>probability amplitude</em> $c_\alpha$ of measuring the corresponding observable's value to be $\alpha$.</p>
</blockquote>
<p>The Born rule is the origin of the probability interpration of the wavefunction. This is because, by the Born rule, we know that for the position operator $\hat{\mathbf{x}}$, its eigenstates are given by $|\mathbf{x}\rangle$. Therefore, by the Born rule, the wavefunction $\psi(\mathbf{x}) = \langle \mathbf{x}|\psi\rangle$ represents the probability amplitude of measuring a particle to be at position $x$. This is indeed the case! From the probability amplitude, we can therefore find that:</p>
<p class="mathcell">
$$
\rho = |\psi(\mathbf{x})|^2
$$
</p>
<p>Where $\rho$ is the <strong>probability density</strong>, which is the <em>measurable</em> probability of a particle (for instance, an electron) being at a particular position. Thus, by invoking the Born rule, we have made the claim that the wavefunction represents some sort of probabilistic wave fully rigorous.</p>
<blockquote>
<p><strong>Note for the advanced reader:</strong> In molecular and solid-state physics, it is more typical to call the probability density an <a href="https://en.wikipedia.org/wiki/Electron_density"><em>electron density</em></a> and represent it as $n(\mathbf{r})$. This is because in these fields, we are interested in many-body systems, typically ones with several (and sometimes very many!) electrons. Thus, we talk of a <em>particle density</em> of finding some particle (usually electron) within a region of volume.</p>
</blockquote>
<h3 id="degeneracy-and-cscos">Degeneracy and CSCOs</h3>
<p>Let's go back to postulate II(a) of quantum mechanics, which (among other things) says that (1) eigenvectors of Hermitian operators represent possible states (<em>eigenstates</em>) of a system and that (2) eigenstates have associated <em>eigenvalues</em> that are physically-measurable (real-valued). From a simple reading, you might have the idea that quantum mechanics gives a neat, simple correspondence: each eigenstate has a unique eigenvalue, and if you solve the eigenvalue equation for some observable, you get a set of eigenvalue-eigenstate pairs. Then the state-vector just becomes a superposition of these eigenstates, and once we have that, the quantum system is - ta-da - solved!</p>
<p>If only it were that simple! The issue is that when solving the eigenvalue equation, <em>different</em> eigenstates can correspond to the <em>same</em> eigenvalue. The (unfortunate and very antiquated) term to describe this phenomenon is <strong>degeneracy</strong>, although "repeated states" communicates the same information. A common occurrence of degeneracy is when two states of a system $|\varphi_1\rangle, |\varphi_2\rangle$ have the same energy eigenvalue, i.e. $E_1 = E_2$, so you can't tell them apart from just knowing the energy of the system.</p>
<p>Let's demonstrate with another example. Say we have two observables $A, B$ which are represented by operators $\hat A, \hat B$. They respectively satisfy the eigenvalue equations:</p>
<p class="mathcell">
$$
\begin{align*}
\hat A|\psi\rangle = a|\psi\rangle \\
\hat B|\psi\rangle = b|\psi\rangle
\end{align*}
$$
</p>
<p>As a reminder, if $\hat A, \hat B$ <strong>commute</strong>, then they satisfy:</p>
<p class="mathcell">
$$
[\hat A, \hat B] = \hat A \hat B - \hat B \hat A = 0
$$
</p>
<p>Where $[\hat A, \hat B]$ is the <strong>commutator</strong> of $\hat A$ and $\hat B$. The question that now matters to us is this: is the system degenerate? Well, it is certainly <em>possible</em> for it to be! The reason is that it is possible to have an eigenvalue $a$ where:</p>
<p class="mathcell">
$$
\hat A|\psi_1\rangle = a|\psi_1\rangle, \quad \hat A|\psi_2\rangle = a|\psi_2\rangle
$$
</p>
<p>This means that the two states $|\psi_1\rangle$ and $|\psi_2\rangle$ share the <strong>same eigenvalue</strong> $a$. Remember that in quantum mechanics, <em>eigenvalues</em> of observables (like energy, momentum, position, etc.) are all we can physically measure, so if we naively measure our observable $A$ to have some eigenvalue $a$, we'd have no idea what state it came from. It could've been either the $|\psi_1\rangle$ or the $|\psi_2\rangle$ state, but it would be impossible to tell!</p>
<p>To resolve this issue, we need more information, and that information comes from our other observable $B$. This is because while $|\psi_1\rangle, |\psi_2\rangle$ share the same eigenvalue $a$ for the $\hat A$ operator, we often find that they have <em>different</em> eigenvalues for the $\hat B$ operator. That is to say:</p>
<p class="mathcell">
$$
\hat B|\psi_1\rangle = b_1 |\psi_1\rangle, \quad \hat B|\psi_2\rangle = b_2 |\psi_2\rangle
$$
</p>
<p>Now, the two states $|\psi_1\rangle$ and $|\psi_2\rangle$ share <strong>different eigenvalues</strong> $b_1, b_2$, so we can now tell which state is which: if we measure $b_1$, then we <em>know</em> the system must be in state $|\psi_1\rangle$, whereas if we measure $b_2$, then we <em>know</em> the system must be in state $|\psi_2\rangle$. This tells us that while a single eigenvalue $a$ might not allow us to determine the exact state of the system, a <em>pair of eigenvalues</em> $(a, b)$ does! Thus, despite the degeneracy in the system, the ordered pairs $(a, b_1)$ and $(a, b_2)$ can be used to <em>uniquely</em> identify the states $|\psi_1\rangle$ and $|\psi_2\rangle$, solving the problem of degeneracy! This is called a <strong>complete set of commuting observables (CSCO)</strong>, which tells us that we can <strong>uniquely identify</strong> each eigenstate of a system with degeneracy, <em>as long as</em>:</p>
<ol>
<li>We have two observables $A, B$, which have associated operators $\hat A, \hat B$ with respective eigenvalues $a, b$</li>
<li>The operators $\hat A, \hat B$ <strong>commute</strong> with each other, that is, $[\hat A, \hat B] = 0$</li>
<li>An ordered pair of eigenvalues $(a, b)$ <em>always corresponds</em> to a <strong>unique eigenstate</strong></li>
</ol>
<h3 id="the-tensor-product">The tensor product</h3>
<p>Up to this point, we have assume that we are describing a <strong>single</strong> quantum system, which has a unique single state-vector $|\Psi\rangle$ (or equivalently, if we assume $t = 0$, then by a unique time-independent state-vector $|\psi\rangle$). But what if we want to describe a <strong>composite system</strong> formed by <strong>several</strong> quantum systems interacting with each other? Then one state-vector wouldn't be enough! In fact, to describe a composite system formed by $N$ quantum systems, we will need $N$ state-vectors! This is all very wonky to work with, so instead, we can describe such a system by a <em>single</em> state-vector that is the <strong>tensor product</strong> of each of the individual state-vectors.</p>
<p>For instance, consider a composite system formed by combining two separate quantum systems, with individual state-vectors $|\psi_1\rangle$ and $|\psi_2\rangle$. The state-vector of the composite system $|\psi_{12}\rangle$ can be written as the <strong>tensor product</strong> between two systems with state-vectors $|\psi_1\rangle, |\psi_2\rangle$, and is denoted as:</p>
<p class="mathcell">
$$
|\psi_{12}\rangle = |\psi_1\rangle \otimes |\psi_2\rangle
$$
</p>
<p>If state $|\psi_1\rangle = \alpha_1 |u_1\rangle + \alpha_2 |u_2\rangle$ and $|\psi_2\rangle = \beta_1|u_1\rangle + \beta_2|u_2\rangle$, then the tensor product of $|\psi_1\rangle, |\psi_2\rangle$ is given by:</p>
<p class="mathcell">
$$
\begin{align*}
|\psi_{12}\rangle &amp;= (\alpha_1 |u_1\rangle + \alpha_2 |u_2\rangle) ~\otimes ~(\beta_1|u_1\rangle + \beta_2|u_2\rangle) \\
&amp;= \alpha_1 \beta_1 |u_1\rangle \otimes |u_2\rangle + \alpha_1 \beta_2 |u_1\rangle \otimes |u_2\rangle \\
&amp;\qquad + \alpha_2 \beta_1 |u_2\rangle \otimes |u_1\rangle + \alpha_2 \beta_2 |u_2\rangle \otimes |u_2\rangle
\end{align*}
$$
</p><h3 id="mathematical-properties-of-operators">Mathematical properties of operators</h3>
<p>We will continue discussing the physics of quantum mechanics shortly, but it is also important to take some time to discuss the mathematics of the operators that are associated with physical quantities. Considering how vital operators are in quantum mechanics, it is important to know how to mathematically manipulate them.</p>
<p>One very common operation we perform with operators is to apply them repeatedly. To demonstrate, consider an operator $\hat A$ with eigenvectors $|\varphi_n\rangle$, corresponding each to a unique eigenvalue $a$. Then, $\hat A^n$ represents applying the operator $n$ times. An important identity here is that:</p>
<p class="mathcell">
$$
\hat A |\varphi_n\rangle = a |\varphi_n\rangle \quad \Rightarrow \quad \hat A^n|\varphi_n\rangle = a^n |\varphi_n\rangle
$$
</p>
<p>That is to say, the eigenvalues of $\hat A^n$ are simply $a^n$. This is incredibly helpful because when we want to find the eigenvalues of some operator that is applied several times, we don't have to solve for the eigenvalues again.</p>
<p>Another operator we might be interested in is to take an operator and map it to another operator by a particular function. This is a mathematically nuanced concept, because defining an operator-valued function is somewhat complicated to do rigorously. However, we will dispense with the rigor for now, and just state the results. For any function $f(a)$ we have:</p>
<p class="mathcell">
$$
f(\hat A)  |\varphi_n\rangle = f(a) |\varphi_a\rangle, \quad f&#x27;(A)|\varphi_a\rangle = f&#x27;(a) |\varphi_a\rangle
$$
</p>
<p>We can also differentiate and integrate operators, which, again, is mathematically nuanced to define rigorously, but straightforward to simply state. For instance, consider two linear operators $\hat F(t)$ and $\hat G(t)$, which both depend on time. Their derivatives with respect to time satisfy the product rule and sum rules, i.e.</p>
<p class="mathcell">
$$
\begin{align*}
\dfrac{d}{dt} (\hat F + \hat G) &amp;= \dfrac{d\hat F}{dt} + \dfrac{d\hat G}{dt} \\
\dfrac{d}{dt} (\hat F \hat G) &amp;= \dfrac{d\hat F}{dt} \hat G + \hat F \dfrac{d \hat G}{dt}
\end{align*}
$$
</p>
<p>Now, let us consider two arbitrary linear operators $\hat A$ and $\hat B$. If $|\psi\rangle$ is an eigenvector of $\hat A$, then $\hat B|\psi\rangle$ is also an eigenvalue of $\hat A$, because:</p>
<p class="mathcell">
$$
\begin{align*}
\hat A|\psi\rangle &amp;= a|\psi\rangle \\
\hat B \hat A|\psi\rangle &amp;= \hat B(a|\psi\rangle) \\
&amp;=a \hat B|\psi\rangle
\end{align*}
$$
</p>
<p>These identities often prove very helpful in taking complex operator algebra (and calculus) and making them much simpler, so it is helpful to keep them in mind.</p>
<h3 id="the-trace">The trace</h3>
<p>We will now discuss a mathematical operation called the <strong>trace</strong>, which will be very important coming up. The trace is an important <em>scalar</em> quantity of an operator. It has a particularly elegant form if an operator can be written as a matrix. For instance, consider an operator $\hat A$ that can be represented as a $(n \times n)$ matrix. Then the trace (denoted $\operatorname{Tr}(\hat A)$) is just the <strong>sum of diagonals</strong> of the matrix:</p>
<p class="mathcell">
$$
\operatorname{Tr}(\hat A) = \operatorname{Tr} \begin{pmatrix}
A_{11} &amp; A_{12} &amp; \dots &amp; A_{1n} \\
A_{21} &amp; A_{22} &amp; \dots &amp; A_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
A_{n1} &amp; A_{n2} &amp; \dots &amp; A_{nn}
\end{pmatrix}
=
A_{11} + A_{22} + A_{33} + \dots + A_{nn}
$$
</p>
<p>In general, as long as a matrix $A_{ij}$ is a $(n \times n)$ square matrix, we can find its trace by just adding up its diagonals:</p>
<p class="mathcell">
$$
\operatorname{Tr}(A_{ij}) = \sum_{i=1}^n A_{ii}
$$
</p>
<p>Whereas for a generalized operator $\hat A$ (which may or may not have a matrix representation), the trace is given by:</p>
<p class="mathcell">
$$
\operatorname{Tr}(\hat A) = \sum_i \langle \varphi_i|\hat A |\varphi_i\rangle
$$
</p>
<p>Where $|\varphi_k\rangle$ is an eigenstate of the operator $\hat A$. One can show that these two definitions are equivalent when we substitute $A_{ii} = \langle \varphi_i|\hat A|\varphi_i\rangle$ (which is the matrix representation of $\hat A$ in the $|\varphi_i\rangle$ basis), giving us:</p>
<p class="mathcell">
$$
\sum_{i=1}^n A_{ii} = \sum_{i=1}^n \langle \varphi_i|\hat A |\varphi_i\rangle = \operatorname{Tr}(\hat A)
$$
</p>
<p>The trace is a <strong>linear operation</strong>, so (among others) it satisfies all identities of a linear operator. In particular, some key identities of the trace are:</p>
<p class="mathcell">
$$
\begin{gather*}
\operatorname{Tr}(\hat A \hat B) = \operatorname{Tr}(\hat B \hat A) \\
\operatorname{Tr}(\hat A \hat B \hat C) = \operatorname{Tr}(\hat C \hat A \hat B) = \operatorname{Tr}(\hat B \hat C \hat A) \\
\operatorname{Tr}(\hat A \pm \hat B) = \operatorname{Tr}(\hat A) \pm \operatorname{Tr}(\hat B) \\
\operatorname{Tr}(c \hat A) = c \operatorname{Tr}(\hat A)
\end{gather*}
$$
</p>
<p>But why do we care about the trace? The answer is that for <em>any</em> matrix, the trace is equal to the <strong>sum of its eigenvalues</strong>. Crucially, this is a key <strong>invariant</strong> of a matrix that is independent of the basis chosen. That means that the usual but tedious way to find the sum of a matrix's eigenvalues - by diagonalization (that is, making a matrix have its eigenvalues along its diagonal, and zero everywhere else) - is not needed! Therefore, the trace is a powerful operation that has tremendous significance in the mathematical framework of quantum mechanics</p>
<h3 id="commutators-and-commutation-relations">Commutators and commutation relations</h3>
<p>Another key mathematical structure used in quantum mechanics is the <strong>commutator</strong>. We have already seen what a commutator is: for two given operators $\hat A, \hat B$, their commutator is written as $[\hat A, \hat B]$ and is given by $[\hat A, \hat B] = \hat A \hat B - \hat B \hat A$. If the two operators satisfy $[\hat A, \hat B] = 0$, then we say that they <strong>commute</strong>. However, if we find that $[\hat A, \hat B] \neq 0$, then we say they <em>do not commute</em> or (equivalently) that they are <strong>non-commuting</strong>.</p>
<p>The commutator, in essence, measures the <em>extent to which two operators are incompatible</em>. Physically, this corresponds to the inherent <strong>uncertainty in measurement</strong> in quantum mechanics, which is what makes quantum mechanics so distinct from classical mechanics. This idea of uncertainty can be mathematically formalized as follows. Consider two <strong>non-commuting</strong> operators $\hat A, \hat B$, which represent two observables $A, B$. Then, the <strong>generalized uncertainty principle</strong> in quantum mechanics tells us that:</p>
<p class="mathcell">
$$
\Delta A \Delta B \geq \left|\dfrac{\langle [\hat A, \hat B]\rangle}{2}\right|
$$
</p>
<p>Where $\Delta A$ is the uncertainty in measuring observable $A$, $\Delta B$ is the uncertainty in measuring observable $B$, and $|\dots |$ denotes the <strong>complex norm</strong> (absolute value of a complex number). Let's take some time to absorb what this means.</p>
<p>We know that all real-world measurements have <em>some</em> amount of inaccuracy just because our measurement instruments aren't perfect. For instance, you might measure a paper clip with a ruler and say that its length is, say, $\pu{2 cm}$. But it would be almost impossible for a paper clip to be <em>exactly</em> $\pu{2cm}$ in length! It is far more likely that the paper clip is within a range of $\pu{2 \pm 0.5 cm}$, because a typical (metric-based) ruler has markings per every centimeter, so it cannot measure anything to more precise than $\pu{1 cm}$. Therefore, in making a measurement with the ruler, the result can be off by $\pm \pu{0.5 cm}$ on either direction and it would be impossible to know! This means that the ruler has a total uncertainty range of $\pu{1 cm}$, and therefore it is important to conduct any measurement with its uncertainty also recorded.</p>
<p>This is all well and good, but in theory, there is no limit to how <em>arbitrarily good</em> we can make a measurement instrument - at least, in <strong>classical mechanics</strong>. As an example, we can imagine making a super-accurate ruler that measures distances with an uncertainty of only $\pm\pu{0.5 nm}$ (how you would make such a ruler is an entirely different question altogether, but let's assume you have some superhuman ruler engineering skills and manage to build one). In classical mechanics, there is nothing stopping you from building this ruler and making a measurement as precisely as you want. But this is <strong>no longer true</strong> in <strong>quantum mechanics</strong>! Quantum mechanics says that <em>if</em> you measure the momentum and position of some object (let's say, our paperclip) at the <strong>same time</strong>, there is a <strong>theoretical limit</strong> on how accurate you can measure its position. In particular, the uncertainty $\Delta x$ in the position (and therefore the length) that our super-accurate ruler could measure is given by:</p>
<p class="mathcell">
$$
\Delta x = \frac{\hbar}{2\Delta p}
$$
</p>
<p>Where $\Delta p$ is the uncertainty in the momentum that is measured. For instance, if we assume that our paperclip has a measured uncertainty in momentum of $\pu{3E-28 kg*ms^{-1}}$, then its uncertainty in position is given by:</p>
<p class="mathcell">
$$
\Delta x = \frac{\hbar}{2\times(\pu{3E-28 kg*ms^{-1}})} \approx \pu{175 nm}
$$
</p>
<p>This means that even though the super-accurate ruler is designed to measure with an uncertainty of only $\pm\pu{0.5 nm}$, its <em>actual</em> uncertainty is much higher, due to the uncertainty principle! Note that since the uncertainty in position is inversely proportional to the momentum, the effects of the uncertainty principle only become evident on atomic and subatomic scales, but it most certainly does exist, and it means that our Universe is <strong>inherently uncertain</strong>. We don't precisely know where anything really is, or how fast anything is going, or even the amount of energy or momentum something has. The radical nature of this idea was a complete break from any classical intuition, and even today, it is still a very hard fact for many to accept.</p>
<p>Now, let's derive the important relation $\Delta x = \frac{\hbar}{2\Delta p}$ that we just used to demonstrate the existence of quantum uncertainty in measuring position and momentum. First, let's compute the commutator $[\hat x, \hat p]$. This gives us:</p>
<p class="mathcell">
$$
\begin{align*}
[\hat x, \hat p]\psi(x) &amp;= x \hat p \psi(x) - \hat p \hat x \psi(x) \\
&amp;= x(-i\hbar \nabla) \psi(x) - (-i\hbar \nabla) x \psi \\
&amp;= -i\hbar x \nabla\psi(x) + i\hbar \underbrace{\nabla (x \psi)}_\text{product rule} \\
&amp;= -i\hbar x \nabla\psi + i\hbar (\nabla x) \psi + i\hbar  (x \nabla) \psi \\
&amp;= -i\hbar x \nabla\psi + i\hbar \psi(x) + i\hbar x \nabla \psi \\
&amp;= i\hbar \psi(x)
\end{align*}
$$
</p>
<p>Thus, we have found that $[\hat x, \hat p]\psi = i\hbar \psi$, or in other terms:</p>
<p class="mathcell">
$$
[\hat x, \hat p] = i\hbar
$$
</p>
<p>Which is often called the <strong>canonical commutator</strong>. Now, if we substitute this result into the generalized uncertainty relation, we have:</p>
<p class="mathcell">
$$
\Delta x \Delta p \geq \left|\dfrac{\langle [\hat x, \hat p]\rangle}{2}\right| 
\geq \left|\dfrac{i\hbar}{2}\right| \geq \frac{\hbar}{2}
$$
</p>
<p>Thus we now arrive at the infamous <strong>Heisenberg uncertainty principle</strong>, which is described by the equation:</p>
<p class="mathcell">
$$
\Delta x \Delta p \geq \dfrac{\hbar}{2}
$$
</p>
<p>The <em>minimum uncertainty</em> - which corresponds to the <em>highest accuracy</em> that we can make - is given by:</p>
<p class="mathcell">
$$
\Delta x \Delta p = \dfrac{\hbar}{2}
$$
</p>
<p>Rearranging gives us the equation we started with:</p>
<p class="mathcell">
$$
\Delta x = \dfrac{\hbar}{2\Delta p}
$$
</p>
<p>This is a powerful result that came from using commutators, and is a demonstration of how important commutators are in quantum mechanics. Not surprisingly, it is important to be familiar with several properties of commutators, including the following:</p>
<ul>
<li>$[\hat A, \hat A] = [\hat B, \hat B] = 0$</li>
<li>$[\hat A, \hat B] = -[\hat B, \hat A]$</li>
<li>$[\hat A, \hat B + \hat C] = [\hat A, \hat B] + [\hat A, \hat C]$</li>
<li>$[\hat A + \hat B, \hat C] = [\hat A, \hat C] + [\hat B, \hat C]$</li>
<li>$[\hat A, \hat B \hat C] = [\hat A, \hat B]\hat C + \hat B[\hat A, \hat C]$</li>
<li>$[\hat A \hat B, \hat C] = \hat A[\hat B, \hat C] + [\hat A, \hat C] \hat B$</li>
<li>$[c \hat A, \hat B] = [\hat A, c\hat B] = c[\hat A, \hat B]$ where $c$ is some constant</li>
<li>$[\hat A, [\hat B, \hat C]] + [\hat B, [\hat C, \hat A]] + [\hat C, [\hat A, \hat B]] = 0$, which is also known as the <strong>Jacobi identity</strong></li>
<li>$[\hat A, f(\hat A)] = 0$</li>
<li>If $[\hat A, \hat B] = 0$ then $[\hat A, f(\hat B)] = 0$</li>
</ul>
<p>In addition, for vector-valued operators $\mathbf{\hat A}$ and $\mathbf{\hat B}$, where $\mathbf{\hat A} = (\hat A_1, \hat A_2, \dots, \hat A_n)$ is a vector of $n$ operators and likewise $\mathbf{\hat B} = (\hat B_1, \hat B_2, \dots, \hat B_n)$ is also a vector of $n$ operators, we have the following identities:</p>
<ul>
<li>$[\hat A_i, \hat A_i] = [\hat B_i, \hat B_i] = 0$</li>
<li>$[\hat A_i, \hat B_j] = -[\hat A_j, \hat B_i]$</li>
</ul>
<p>Here, $\hat A_i$ denotes the $i$-th component of $\mathbf{\hat A}$ and $\hat B_i$ denotes the $j$-th component of $\mathbf{\hat B}$. For instance, consider the position operator $\mathbf{\hat p} = (\hat p_x, \hat p_y, \hat p_z)$. By the above identities, we know that it satisfies $[\hat p_i, \hat p_i] = 0$, where $i \in (x, y, z)$. We can expand this to component form, giving us $[\hat p_x, \hat p_x] = [\hat p_y, \hat p_y] = [\hat p_z, \hat p_z] = 0$. Using this index notation can be a bit complicated upon first seeing it, but it becomes a powerful notation once you get used to it, and allows us to express complex relationships between operators in a concise way.</p>
<blockquote>
<p><strong>Note:</strong> More of these identities can be found on the <a href="https://en.wikipedia.org/wiki/Commutator#Identities_(ring_theory)">Wikipedia page of commutator identities</a></p>
</blockquote>
<p>Lastly, let's take a look at the <strong>canonical commutator</strong> $[\hat x, \hat p] = i\hbar$, perhaps the most important commutator in quantum mechanics. We can generalize the canonical commutator to the following identities:</p>
<p class="mathcell">
$$
\begin{gather*}
[\hat x, \hat p^n] = (i\hbar n)\hat p^{n - 1} \\
[\hat x^n, \hat p] = (i\hbar n) \hat
x^{n-1}
\end{gather*}
$$
</p>
<p>In higher dimensions (2D and 3D), we write the position operator as $\mathbf{\hat r}$ and momentum operator as $\mathbf{\hat p}$, which satisfy:</p>
<p class="mathcell">
$$
\begin{align*}
[\mathbf{\hat r}_i, \mathbf{\hat r}_j] &amp;= 0 \\
[\mathbf{\hat p}_i, \mathbf{\hat p}_j] &amp;= 0 \\
[\mathbf{\hat r}_i, \mathbf{\hat p}_j] &amp;= i\hbar \delta_{ij}, \quad
\end{align*}
$$
</p>
<p>Where $\delta_{ij}$ is the <strong>Kronecker delta</strong> and is given by:</p>
<p class="mathcell">
$$
\delta_{ij} = \begin{cases}
1 &amp; i = j \\
0 &amp; i \neq j
\end{cases}
$$
</p>
<p>In addition, $\mathbf{\hat r}_i$ is the $i$-th component of the position operator, and $\mathbf{\hat p}_j$ is the $j$-th component of the momentum operator. Using these commutation relations tells us, for instance, that $[\hat y, \hat p_y] = i\hbar$ (since we have $i = j = y$) but that $[\hat y, \hat p_x] = 0$ (since we have $i = y$ and $j = x$, so $i \neq j$).</p>
<h3 id="a-summary-of-the-state-vector-formalism">A summary of the state-vector formalism</h3>
<p>Let's recap what we've covered so far. We have learned that the quantum state is represented as a vector, called a <strong>state-vector</strong>, and written using Dirac (bra-ket) notation as $|\Psi(t)\rangle$. The quantum state "lives" in a <strong>Hilbert space</strong> $\mathcal{H}$, which is complex-valued and can be finite or infinite-dimensional.</p>
<p>In general, the state-vector is a function of time, that is. But if we consider the state-vector at a particular moment in time, for instance, $t = 0$, we can define $|\Psi\rangle = |\Psi(0)\rangle$ to be the <em>time-independent state-vector</em>. Depending on the type of system, we can decompose $|\Psi\rangle$ as a superposition of basis vectors $|\alpha\rangle$ as either a sum:</p>
<p class="mathcell">
$$
|\Psi\rangle = \sum_i c_i |\alpha_i\rangle
$$
</p>
<p>Or as an integral:</p>
<p class="mathcell">
$$
|\Psi\rangle = \int c(\alpha)|\alpha\rangle~ d\alpha
$$
</p>
<p>Such basis vectors must be the <strong>eigenstates</strong> of quantum-mechanical operators that represent <strong>physical quantities</strong> like position, momentum, energy, and spin, which are called <strong>observables</strong>. Their <strong>eigenvalues</strong> are the possible measurable values of the operator (such as possible energies or momenta of a particle); meanwhile, the coefficients $c_i$ (in the discrete case) and $c(\alpha)$ in the continuous case are interpreted as <strong>probability amplitudes</strong>. One can then find the <strong>probability</strong> of measuring the $i$-th eigenvalue of a discrete operator with:</p>
<p class="mathcell">
$$
\mathcal{P}_i = |c_i|^2, \quad c_i = \langle \alpha_i|\psi\rangle
$$
</p>
<p>In the continuous case, the probability amplitude $c(\alpha) = \langle \alpha|\psi\rangle$ becomes a continuous function (equivalently, an <em>infinite-dimensional vector</em>), which is called the <strong>wavefunction</strong>, and denoted $\psi(\alpha)$. One may obtain the <strong>probability density</strong> $\rho$ of measuring eigenvalue $\alpha$ with:</p>
<p class="mathcell">
$$
\rho = |\psi(\alpha)|^2 = \psi(\alpha)\psi^*(\alpha)
$$
</p>
<p>Collectively, these two formulas comprise the <strong>Born rule</strong>. In the case of the special case of the position operator $\hat x$, the eigenvalues of the position $\hat x$ are possible positions $x$, and eigenstates are position eigenstates $|x\rangle$. Thus, the wavefunction takes the form $\psi(x)$, and by the Born rule, taking its squared norm $|\psi|^2$ gives the <strong>probability per unit volume</strong> of measuring a particle at position $x$. In the more general case, we can take the inner product of the state-vector with an arbitrary basis $|\alpha\rangle$ to find the probabilities of measuring its eigenstates.</p>
<h2 id="the-density-operator-and-density-matrix">The density operator and density matrix</h2>
<p>In doing calculations of quantum systems, we've mostly restricted our attention to using the <em>wavefunction representation</em> for all but the simplest systems. It is an idea that we've seen fits neatly into the state-vector picture: the wavefunction is simply the <em>components</em> of the state-vector in a continuous basis. In particular, if we express the state-vector in the position and momentum bases, we get the position-space and momentum space wavefunctions:</p>
<p class="mathcell">
$$
\begin{align*}
\psi(x) &amp;= \langle x|\Psi\rangle \\
\psi(p) &amp;= \langle p|\Psi\rangle
\end{align*}
$$
</p>
<p>But wavefunctions, however convenient they may be, are <em>not</em> ideally suited to analyzing many quantum systems. We often find systems that <em>cannot</em> be represented by a wavefunction, and we'll soon see some examples. The good news is that there <em>is</em> a way to analyze quantum systems that doesn't need to use wavefunctions, or even need precise knowledge of the state-vector in a particular basis. This method involves using a special operator, known as the <strong>density operator</strong>.</p>
<p>The density operator starts by assuming that we know a system <em>can be</em> in a mix of some states, which we'll denote as $|u_1\rangle, |u_2\rangle, \dots, |u_n\rangle$. We also know that these states have some associated probabilities, which we'll denote as $P_1, P_2, \dots, P_n$. From here, we <em>define</em> the density operator to be given by:</p>
<p class="mathcell">
$$
\hat \rho = \sum_i P_i |u_i\rangle \langle u_i|
$$
</p>
<p>The density operator has several special properties; among which include:</p>
<ul>
<li>$\hat \rho$ is <strong>Hermitian</strong>, that is, $\hat \rho^\dagger = \hat \rho$</li>
<li>$\hat \rho$ is <strong>idempotent</strong>, that is, $\hat \rho^2 = \hat \rho$ (so long as all of the $|u_i\rangle$'s are normalized)</li>
<li>$\hat \rho$ satisfies $\operatorname{Tr}(\hat \rho) = 1$ and $\operatorname{Tr}(\hat \rho A) = \operatorname{Tr}(\hat A \hat \rho)$</li>
</ul>
<p>"Alright", you might say, "but how could this <em>ever</em> be useful?" This is indeed a good question to answer, so let's provide some motivation for using the density matrix. The first advantage of using the density matrix is that it <em>doesn't require</em> an orthonormal basis! Indeed, our $|u_i\rangle$'s can be essentially any state a system can be in. For instance, suppose we wanted to calculate a very basic quantum system with two known states, which are given by:</p>
<p class="mathcell">
$$
|u_1\rangle = \begin{pmatrix} 1&#x2F;2 \\ \sqrt{3}&#x2F;2 \end{pmatrix}, \quad |u_2\rangle = \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$
</p>
<p>We know that there is a 50% probability for the system to be in state $|u_1\rangle$, and 50% probability for the system to be in state $|u_2\rangle$. That is to say, $P_1 = P_2 = 1/2$. And this is all the information we have of the system! However, even with this limited information, we can still write down the density operator in its <em>matrix representation</em>, which we usually just call the <strong>density matrix</strong>. In our case, we can find the matrix entries (also called <em>matrix elements</em>) $\rho_{mn}$ as follows:</p>
<p class="mathcell">
$$
\begin{align*}
\rho_{mn} &amp;= \langle u_m| \hat \rho |u_n\rangle \\
&amp;= \langle u_m |\bigg(\sum_i P_i |u_i\rangle \langle u_i|\bigg)|u_n\rangle \\
&amp;= \sum_i \langle u_m| P_i|u_i\rangle \langle u_i|u_n\rangle
\end{align*}
$$
</p>
<p>This definition <em>can</em> be used (and often is useful for complex systems) to find the matrix elements, although in our case a much easier method suffices: just do it by hand! Substituting our known values for our states $|u_1\rangle, |u_2\rangle$ and their respective probabilities gives us:</p>
<p class="mathcell">
$$
\begin{align*}
\hat \rho &amp;= \sum_i P_i |u_i\rangle \langle u_i| \\
&amp;= P_1 |u_1\rangle \langle u_1| + P_2 |u_2\rangle \langle u_2| \\
&amp;= \frac{1}{2}
\begin{pmatrix} 1&#x2F;2 \\ \sqrt{3}&#x2F;2 \end{pmatrix}
\begin{pmatrix} 1&#x2F;2 \\ \sqrt{3}&#x2F;2 \end{pmatrix}^T 
+ \frac{1}{2}
\begin{pmatrix} 0 \\ 1 \end{pmatrix}
\begin{pmatrix} 0 \\ 1 \end{pmatrix}^T \\
&amp;= \frac{1}{2} \begin{pmatrix} 1&#x2F;4 &amp; \sqrt{3}&#x2F;4 \\
\sqrt{3}&#x2F;4 &amp; 3&#x2F;4 \end{pmatrix} +
\frac{1}{2}\begin{pmatrix}
0 &amp; 0  \\
0 &amp; 1
\end{pmatrix} \\
&amp;= \frac{1}{2} \begin{pmatrix}
1&#x2F;4 &amp; \sqrt{3}&#x2F;4 \\
\sqrt{3}&#x2F;4 &amp; 7&#x2F;4 
\end{pmatrix} \\
&amp;= \frac{1}{8} \begin{pmatrix}
1 &amp; \sqrt{3} \\
\sqrt{3} &amp; 7
\end{pmatrix}
\end{align*}
$$
</p>
<p>Here, we can indeed see that the density matrix is Hermitian ($\hat \rho = \hat \rho^\dagger$) and if we calculate the sum of its diagonals, we can verify that it has a trace of one, which we would expect of a density matrix. For a simple two-state system, this calculation is straightforward. But what about more general systems? In a typical situation where we'll need to use a density matrix, we find ourselves <strong>lacking information</strong> that we would otherwise need to understand a quantum system:</p>
<ul>
<li>We know that the system can be in one of some number of states (e.g. $|u_1\rangle$ and $|u_2\rangle$ for the previous example), and..</li>
<li>$|u_1\rangle$ and $|u_2\rangle$ are <strong>normalized</strong>, but..</li>
<li>$|u_1\rangle$ and $|u_2\rangle$ are <strong>not necessarily orthogonal</strong>!</li>
</ul>
<p>This means that writing out a state-vector $|\Psi\rangle$ as a superposition of eigenstates is <em>not possible</em>, because that would require an orthonormal basis of eigenstates to span the state space of a system. However, the density matrix gives us options, because it still contains critical information about a quantum system. For instance, it allows us to calculate the <strong>expectation values</strong> of operators. This is because an expectation value of an operator $\hat A$, which has eigenvalues $\lambda_i$, each of which has a probability $P_i$ of being measured, is given by:</p>
<p class="mathcell">
$$
\langle \psi|\hat A|\psi\rangle = \sum_i P_i \lambda_i = \sum_i P_i \langle u_i |\hat A|u_i\rangle
$$
</p>
<blockquote>
<p><strong>Note:</strong> The reason why $\lambda_i =\langle u_i |\hat A|u_i\rangle$ is because the term $\langle u_i |\hat A|u_i\rangle$ tells us that we have <em>diagonalized</em> the matrix that represents $\hat A$. A diagonal matrix always has its eigenvalues along the diagonal, and therefore $\langle u_i |\hat A|u_i\rangle$ simply returns the diagonal values, giving the eigenvalues of $\hat A$.</p>
</blockquote>
<p>Now, with some careful rewriting, we note that we can also write the equation for an expectation value in a different form. The key is to insert a closure relation $\hat I = \sum_i |u_i\rangle \langle u_i|$ cleverly, which allows us to simplify things down greatly. The steps for the calculation are shown below:</p>
<p class="mathcell">
$$
\begin{align*}
\sum_i P_i \langle u_i |\hat A|u_i\rangle &amp;= \sum_i P_i \langle u_i |\hat I \hat A|u_i\rangle  \\
&amp;= \sum_i P_i \langle u_i |\left(\sum_i |u_i\rangle \langle u_i|\right) \hat A |u_i\rangle \\
&amp;= \sum_i P_i\langle u_i| \sum_i |u_i\rangle \langle u_i|\hat A|u_i\rangle \\
&amp;= \sum_i \langle u_i|\left(\sum_i P_i|u_i\rangle\langle u_i|\hat A\right)|u_i\rangle \\
&amp;= \sum_i \langle u_i |\hat \rho \hat A|u_i\rangle \\
&amp;= \operatorname{tr}(\hat \rho \hat A) \\
&amp; \Rightarrow \langle A\rangle = \operatorname{tr}(\hat \rho \hat A)
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Notice we used the familiar trick of inserting the closure relation $\hat I = \displaystyle\sum_i |u_i\rangle \langle u_i|$ to derive this result.</p>
</blockquote>
<p>Thus, we find that to calculate the expectation value $\langle A\rangle = \langle \psi|\hat A|\psi\rangle$ of an arbitrary operator $\hat A$, we simply need to take the trace of $\hat \rho \hat A$! This approach - using the density operator - allows us to often <em>completely circumvent</em> the need for manually finding expectation values. In addition, one of the key properties of the trace is that it is <em>independent of the basis chosen</em>, so the density matrix approach is often much faster, as well!</p>
<p>The power of the density matrix is that it contains the <strong>equivalent information</strong> as the wavefunction. It allows us to calculate the expectation values $\langle A\rangle$ of <em>any observables</em> of a system by simply taking the trace $\langle A\rangle = \operatorname{Tr}(\hat \rho \hat A)$. And even better, it <em>guarantees</em> conservation of probability. That is to say, the projection operator automatically satisfies:</p>
<p class="mathcell">
$$
\sum_n P_n = 1
$$
</p>
<p>For all of these reasons, and more, the density matrix method is an incredibly helpful alternative formalism for solving problems in quantum mechanics. We'll now discuss one of its main applications - in the treatment of <strong>mixed states</strong>.</p>
<h3 id="mixed-states">Mixed states</h3>
<p>Before we discussed density matrices, we've always assumed that we have a quantum system whose <em>initial state</em> is known. In this case, we call the state of a system a <strong>pure state</strong>. In practice, this is not always the case; due to imperfections (or quantum entanglement, which we'll discuss more later), it is <em>not possible</em> in many cases to prepare a quantum system such that its initial state is known.</p>
<p>When we have limited information about the state of a quantum system, we have a <strong>mixed state</strong>. A mixed state is a <strong>statistical mixture</strong> of the different states a system can be in (it is also common to speak of a <em>statistical ensemble</em>, which means the same thing). In other words, we know certain states the system can be in, and how likely it is to be in any given state, but we <em>don't know</em> the system's actual state!</p>
<blockquote>
<p><strong>Note:</strong> It is important to remember that a mixed state is different from a superposition of eigenstates. A superposition of eigenstates is still a <strong>pure state</strong> because it fundamentally represents a <strong>single</strong> quantum state that can be written as a sum of other states. A mixed state, by contrast, is a <em>statistical concept</em> and fundamentally <strong>cannot</strong> be written as a superposition of states, because it doesn't represent a single state, but rather some statistical mix of <strong>several states</strong>. While there is <em>always</em> uncertainty in measurement outcomes (just because quantum mechanics is probabilistic in nature), a superposition state is a pure state and does <em>not</em> behave the same way as a mixed state.</p>
</blockquote>
<p>For a mixed state, then the density operator is still Hermitian and has a trace of one, but it <strong>no longer satisfies</strong> $\hat \rho^2 = \hat \rho$. Indeed, we find that $\operatorname{Tr}(\hat \rho^2) &lt; \operatorname{Tr}(\hat \rho)$ and that it (in general) has <em>off-diagonal entries</em>. When we have a mixed state, we almost <em>always</em> resort to using the density operator, since projecting the density operator along a particular <em>pure state</em> (such as along a specific basis) allows us to calculate the information about a mixed-state system even though it cannot be written as a superposition of pure states.</p>
<h3 id="the-von-neumann-equation">The Von Neumann equation</h3>
<p>In addition, there is another major advantage to the density matrix. To start, we'll first note that the density matrix can be written in a slightly different form, when we are analyzing systems for which the state-vector <em>can</em> be expressed as a superposition of orthonormal eigenstates. In such a case, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\hat \rho &amp;= \sum_i P_i |u_i\rangle \langle u_i | \\
&amp;= \sum_i |c_i|^2 |u_i\rangle \langle u_i| \\
&amp;= \sum_i c_i^* c_i |u_i\rangle\langle u_i| \\
&amp;= \sum_i c_i |u_i\rangle \langle u_i|c_i^* \\
&amp;= |\Psi\rangle \langle \Psi|
\end{align*}
$$
</p>
<p>Thus, the density matrix reduces to the projector $|\Psi\rangle \langle \Psi|$ in such cases. If we take its derivative, by the product rule, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\dfrac{d}{dt}\hat \rho &amp;= \dfrac{d}{dt} |\Psi(t)\rangle \langle \Psi(t) \\
&amp;= \left(\dfrac{d}{dt} |\Psi(t)\rangle\right) \langle \Psi| + |\Psi(t)\rangle \left(\dfrac{d}{dt} \langle \Psi(t)|\right) \\
&amp;= \dfrac{1}{i\hbar} \hat H |\Psi(t)\rangle\langle \Psi(t) | - \dfrac{1}{i\hbar} \langle \Psi(t)|\langle \Psi(t)| \hat H \\
&amp;= \dfrac{1}{i\hbar} [\hat H, \hat \rho]
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Here we substitute in $i\hbar\dfrac{d}{dt}|\Psi(t)\rangle = \hat H |\Psi(t)\rangle$ (the Schrödinger equation) to obtain our result.</p>
</blockquote>
<p>Thus, we find that the equation of motion for the density matrix $\hat \rho$ is:</p>
<p class="mathcell">
$$
i\hbar\dfrac{d\hat \rho(t)}{dt} = [\hat H, \hat \rho]
$$
</p>
<p>This is the <strong>von Neumann equation</strong> that describes the evolution of the density matrix throughout time, and it provides all the same information as the Schrödinger equation without us needing to explicitly solve for systems that, by nature, have inherent uncertainty. For quantum physicists studying complicated systems, it is often solved by a computer. But its results yield the same density matrix as the one we've been discussing all along, and contains all of its same advantages.</p>
<h2 id="introduction-to-intrinsic-spins">Introduction to intrinsic spins</h2>
<p>In our discussion of the density matrix, we mentioned one application: the study of <strong>spin</strong> (more accurately termed <em>intrinsic spin</em>, although it is common to just call it "spin" for short).</p>
<p>Intrinsic spin is one of the most important and most fundamentally <em>quantum</em> phenomena, which cannot be explained in classical terms. It refers to the fact that there is a mysterious form of angular momentum that is a fundamental property of subatomic particles, like protons and electrons. This means that certain quantum particles <em>behave</em> like tiny spinning magnets, just like classical rotating charged spheres. A full explanation of what spin <em>is</em> in a physical sense, however, is very difficult, since it is so far from any sort of everyday intuition that trying to explain it in terms of concepts of rotation that are familiar to us would be a gross oversimplification.</p>
<p>Historically, spin was accidentally discovered by the <strong>Stern-Gerlach experiment</strong>, first proposed by German physicist <a href="https://en.wikipedia.org/wiki/Otto_Stern">Otto Stern</a> and then experimentally conducted by <a href="https://en.wikipedia.org/wiki/Walther_Gerlach">Walther Gerlach</a>.</p>
<blockquote>
<p><strong>Note:</strong> Despite its name, spin <em>does not correspond</em> to the concept of "spinning" particles. The quantum notion of particles - which have no well-defined volume and are essentially zero-dimensional points - means that the very idea of "spinning" quite nonsensical. Even if quantum particles could spin, theoretical calculations quickly show that they would spin faster than the speed of light, which of course is unphysical. In essence name "spin" was coined as a historical accident and unfortunately has stuck around to confuse every generation of physicists afterwards.</p>
</blockquote>
<p>For particles like electrons (which are called <em>spin-1/2</em> particles for complicated reasons), there are <em>precisely</em> two basis states of the spin operators, which are usually called "spin-up" and "spin down" and notated with $|\uparrow \rangle$ and $\langle \downarrow|$ respectively. Thus, we can write out the state-vector of a <strong>spin-1/2 system</strong> as:</p>
<p class="mathcell">
$$
|\Psi\rangle = \alpha |\uparrow\rangle + \beta |\downarrow\rangle
$$
</p>
<p>Where $\alpha, \beta$ are the probability amplitudes of measuring the spin-up and spin-down states. Note that the two spin states are orthonormal (that is, $\langle \uparrow|\downarrow\rangle = 0$ and $\langle \uparrow|\uparrow\rangle = \langle \downarrow|\downarrow\rangle = 1$. The <strong>spin operator</strong> $\mathbf{\hat S} = (\hat S_x, \hat S_y, \hat S_z)^T$, which we have already seen, have a very important <em>physical</em> meaning: they give the <strong>spin angular momentum</strong> of a spin-1/2 particle. Specifically, they predict that all spin-1/2 particles have an additional angular momentum associated with their intrinsic spin, called the <em>spin angular momentum</em>. This is usually represented by $\mathbf{S} = (S_x, S_y, S_z)$, which is a vector of the spin angular momentum of the particle in each coordinate direction.</p>
<p>To accommodate this decidedly non-classical behavior, physicists invented a new operator $\mathbf{\hat S}$, the <strong>spin operator</strong>. The spin operator also has components, which are given by $\mathbf{\hat S} = (\hat S_x, \hat S_y, \hat S_z)$. This gives us three eigenvalue equations, one each for each component of the spin operator:</p>
<p class="mathcell">
$$
\begin{align*}
\hat S_x|\psi\rangle = S_x|\psi\rangle \\
\hat S_y|\psi\rangle = S_y|\psi\rangle \\
\hat S_z|\psi\rangle = S_z|\psi\rangle
\end{align*}
$$
</p>
<p>This tells us that $S_x, S_y, S_z$ are the <strong>eigenvalues</strong> of the spin operators along the $x$, $y$, and $z$ axes (respectively). A remarkable result that defies all classical intuition is that these eigenvalues are constrained to only one of two values: $\hbar/2$ or $-\hbar/2$. That is to say:</p>
<p class="mathcell">
$$
S_x = \pm \dfrac{\hbar}{2}, \quad S_y = \pm \dfrac{\hbar}{2}, \quad S_z = \pm \dfrac{\hbar}{2}
$$
</p>
<blockquote>
<p><strong>Note:</strong> We will rarely refer to the spin operator $\mathbf{\hat S}$ and usually just discuss its components $\hat S_x, \hat S_y, \hat S_z$. Thus, if we say "spin operator along $x$" we mean $\hat S_x$, not $\mathbf{\hat S}$.</p>
</blockquote>
<p>Meanwhile, the <em>direction</em> of the spin angular momentum depends on the specific eigenstate. For instance, the <strong>spin-up eigenstate</strong> of the $\hat S_z$ operator tells us that the particle's spin angular momentum points along $+z$. Similarly, the <strong>spin-down eigenstate</strong> of the $\hat S_z$ operator tells us that the particle's spin angular momentum points along $-z$. It is important to note that the direction of the spin angular momentum vector has <em>no correspondence</em> with a particle's actual orientation. It <em>only</em> tells us information about where the particle's <em>spin angular momentum</em> points and is (usually) only relevant for understanding a particle's interaction with magnetic fields.</p>
<blockquote>
<p><strong>Note:</strong> It is often the case that we just say "spin" as opposed to "spin angular momentum", although technically the latter is the correct terminology. However, in the interest of simplicity, we will call both "spin" wherever convenient.</p>
</blockquote>
<p>To make things more concrete, the spin operators in the matrix representation are given by:</p>
<p class="mathcell">
$$
\begin{align*}
\hat S_x = \frac{\hbar}{2} \sigma_x, \quad \sigma_x &amp;= \begin{pmatrix} 0 &amp; 1 \\ 1 &amp; 0 \end{pmatrix} \\
\hat S_y = \frac{\hbar}{2} \sigma_y, \quad \sigma_y &amp;= \begin{pmatrix} 0 &amp; -i \\ i &amp; 0 \end{pmatrix} \\
\hat S_z = \frac{\hbar}{2} \sigma_z, \quad \sigma_z &amp;= \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix}
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> $\sigma_x, \sigma_y, \sigma_z$ are the <strong>Pauli matrices</strong>, which have eigenvalues $\pm 1$. Since the spin operators are just the Pauli matrices multiplied by a factor of $\hbar/2$, the eigenvalues of the all three spin operators are just a factor of $\hbar/2$ multiplied by the eigenvalues of the Pauli matrices (which are $\pm 1$). It is also useful to note that all three have <strong>determinant</strong> of $-1$ and <strong>zero trace</strong>. Other information can be found on its <a href="https://en.wikipedia.org/wiki/Pauli_matrices">wikipedia page</a></p>
</blockquote>
<p>Meanwhile, the eigenstates of the spin operators are called <strong>spinors</strong> (or <a href="https://en.wikipedia.org/wiki/Eigenspinor">eigenspinors</a>). They are often written as $|\uparrow_x\rangle$ and $|\downarrow_x \rangle$ to indicate whether they are spin-up or spin down eigenstates (indicated by the direction of the arrows), which corresponds directly to what direction the spin angular momentum points towards ($x$, $y$, or $z$).</p>
<blockquote>
<p><strong>Note:</strong> Another common notation is to use $|+\rangle_x, |-\rangle_x$ for spin-up and spin-down states respectively, though this can clutter things up so we'll avoid it.</p>
</blockquote>
<p>The explicit forms of the spinors associated with the spin operators $\hat S_x, \hat S_y, \hat S_z$ are given by:</p>
<p class="mathcell">
$$
\begin{align*}
|\uparrow_x\rangle &amp;= \dfrac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix}, \quad
|\downarrow_x\rangle = \dfrac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix} \\
|\uparrow_y\rangle &amp;= \dfrac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ i \end{pmatrix}, \quad
|\downarrow_y\rangle = \dfrac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -i \end{pmatrix} \\
|\uparrow_z\rangle &amp;= \begin{pmatrix} 1 \\ 0 \end{pmatrix},\quad\qquad 
|\downarrow_z\rangle = \begin{pmatrix} 0 \\ 1 \end{pmatrix} \\
\end{align*}
$$
</p>
<p>For calculations, it is frequently useful to reference certain identities of the Pauli matrices and spin operators. For instance, a very useful one is that:</p>
<p class="mathcell">
$$
\sigma_x^2 = \sigma_y^2 = \sigma_z^2 = \hat I
$$
</p>
<p>Where $\hat I$ is the identity matrix. The spin operators and Pauli matrices also satisfy the below commutation relations:</p>
<p class="mathcell">
$$
\begin{align*}
[\sigma_x, \sigma_y] &amp;= 2i\sigma_z \\
[\sigma_j, \sigma_k] &amp;= 2i\varepsilon_{jkl}\sigma_l \\
[\hat S_x, \hat S_y] &amp;= i\hbar \hat S_z \\
[\hat S_y, \hat S_z] &amp;= i\hbar S_x \\
[\hat S_z, \hat S_x] &amp;= i\hbar \hat S_y
\end{align*}
$$
</p>
<p>In addition, some other useful identities are:</p>
<p class="mathcell">
$$
\begin{gather*}
\sigma_j \sigma_k + \sigma_k \sigma_j = 2\delta_{jk} \\
[\hat S^2, \hat S_x] = [\hat S^2, \hat S_y] = [\hat S^2, \hat S_z] = 0
\end{gather*}
$$
</p>
<p>Where $\hat S^2 = \mathbf{\hat S}\cdot\mathbf{\hat S}$ is the squared spin operator, which will be <em>very</em> important when we discuss more types of angular momentum in quantum mechanics. Finally, we have the all-important <strong>Pauli identity</strong>:</p>
<p class="mathcell">
$$
\sigma_i \sigma_j = \delta_{ij} + i\varepsilon_{ijk}\sigma_k, \quad i,j \in (x, y, z)
$$
</p>
<p>Where $\delta_{ij}$ (as we've seen before) is the Kronecker delta, defined as:</p>
<p class="mathcell">
$$
\delta_{ij} = \begin{cases}
1 &amp; i = j \\
0 &amp; i \neq j
\end{cases}
$$
</p>
<p>And $\varepsilon_{ijk}$ is called the <a href="https://jackysci.com/quantum-mechanics/en.wikipedia.org/wiki/Levi-Civita_symbol">Levi-Civita symbol</a> (or <em>Levi-Civita tensor</em>), and is given by:</p>
<p class="mathcell">
$$
\varepsilon_{ijk}=
\begin{cases}+1&amp;{\text{if }}(i,j,k){\text{ is }}(1,2,3),(2,3,1),{\text{ or }}(3,1,2),\\-1&amp;{\text{if }}(i,j,k){\text{ is }}(3,2,1),(1,3,2),{\text{ or }}(2,1,3),\\\;\;\,0&amp;{\text{if }}i=j,{\text{ or }}j=k,{\text{ or }}k=i
\end{cases}
$$
</p>
<blockquote>
<p><strong>Note:</strong> A very useful property of the Levi-Civita symbol is that it can be used to define the cross product of two vectors $\mathbf{A} \times \mathbf{B}$ via $(\mathbf{A} \times \mathbf{B})<em>i = \varepsilon</em>{ijk} A_i B_j$. There is also the so-called <a href="https://en.wikipedia.org/wiki/Levi-Civita_symbol#Three_dimensions">permutation definition</a> of the Levi-Civita symbol, although we will not cover that here.</p>
</blockquote>
<p>While the eigenstates of the $\hat S_x, \hat S_y, \hat S_z$ operators are distinct, it is possible to express eigenstates in one particular spin basis as a superposition of eigenstates in another spin basis. For instance, the eigenstates of the $\hat S_x$ operator can be written in terms of the eigenstates of the $\hat S_z$ operator:</p>
<p class="mathcell">
$$
\begin{align*}
|\uparrow_x\rangle &amp;= \frac{1}{\sqrt{2}} \left(|\uparrow_z\rangle + |\downarrow_z\rangle\right) \\ 
|\downarrow_x\rangle &amp;= \frac{1}{\sqrt{2}} \left(|\uparrow_z\rangle - |\downarrow_z\rangle \right)
\end{align*}
$$
</p>
<p>Likewise, the eigenstates of the $\hat S_y$ operator can be written in terms of the eigenstates of the $\hat S_z$ operator:</p>
<p class="mathcell">
$$
\begin{align*}
|\uparrow_y\rangle &amp;= \frac{1}{\sqrt{2}} \left(|\uparrow_z\rangle + i |\downarrow_z\rangle\right) \\ |\downarrow_y\rangle &amp;= \frac{1}{\sqrt{2}} \left(|\uparrow_z\rangle - i |\downarrow_z\rangle\right)
\end{align*}
$$
</p>
<blockquote>
<p><strong>Historical note:</strong> Interestingly, Wolfgang Pauli, for which the Pauli matrices are named, did not initially even like matrices (or linear algebra for that matter) being there in quantum mechanics! He once said of Schrödinger (to Max Born), <em>"Yes, I know you are fond of tedious and complicated formalism. You are only going to spoil Heisenberg's physical idea by your futile mathematics"</em>. Funnily enough, he would eventually be most remembered for the his contribution to matrix mechanics and in describing spin, something he had once furiously railed against! (See the <a href="https://hsm.stackexchange.com/a/17997">first comment on this Physics SE answer</a>).</p>
</blockquote>
<h3 id="spin-and-the-stern-gerlach-experiment">Spin and the Stern-Gerlach experiment</h3>
<p>The fact that the spin operator tells us that spin-1/2 particles have <em>additional</em> angular momentum not predicted by classical physics has a profound implication: it means that any two otherwise identical spin-1/2 particles (for instance, electrons) will behave <em>differently</em> in a magnetic field. The magnetic force exerted on a classical charged particle with <a href="https://en.wikipedia.org/wiki/Magnetic_moment">magnetic moment</a> $\vec{\boldsymbol{\mu}}$ is given by $\mathbf{F}_B = -\nabla(\vec{\boldsymbol{\mu}} \cdot \mathbf{B})$, where $\mathbf{B}$ is the magnetic field, and $\vec{\boldsymbol{\mu}}$ is given by:</p>
<p class="mathcell">
$$
\vec{\boldsymbol{\mu}} = g\dfrac{q}{2m} \mathbf{S}, \quad g \approx 2
$$
</p>
<blockquote>
<p><strong>Note:</strong> The <strong>magnetic moment</strong> $\vec{\boldsymbol{\mu}}$ is the vector that measures the orientation of the magnetic field associated with an electric charge. A <em>nonzero</em> magnetic moment causes a charge to <em>align</em> with (or against) an external magnetic field, an effect that can be measured very precisely and is used in a variety of applications.</p>
</blockquote>
<p>Since two randomly-chosen electrons would most likely have (and in some cases, <em>must have</em>) different spins, they would have opposite magnetic moments, and thus be deflected in different ways due to the magnetic force. Unknowingly taking advantage of the phenomenon, Stern and Gerlach conducted an experiment (shown in the diagram below) that showed a beam of silver atoms would split into two in a magnetic field, experimentally confirming the prediction of spin from quantum theory. This, again, is because electrons with different spins are deflected in <em>opposite directions</em> by the magnetic field, leading to the beam splitting into two (one of spin-up electrons and one of spin-down electrons).</p>
<p><img src="https://www.informationphilosopher.com/solutions/experiments/stern_gerlach/Stern-Gerlach.png" alt="A diagram of the Stern-Gerlach experiment. A beam of silver atoms is passed through a magnetic field, leading to the beam diverging due to the opposite angular momenta of spin-up and spin-down particles" /></p>
<p><em>A diagram of the Stern-Gerlach experiment. Source: <a href="https://www.informationphilosopher.com/solutions/experiments/stern_gerlach/">The Information Philosopher</a></em></p>
<h3 id="repeated-measurements-of-spin">Repeated measurements of spin</h3>
<p>We saw previously that all three spin operators <strong>don't commute</strong> with each other; for instance, $[\hat S_x, \hat S_y] = i\hbar \hat S_z$. This is not just a mathematical peculiarity! In quantum mechanics, remember that any two operators that <em>do not commute</em> represent observables that <strong>cannot be simultaneously measured to arbitrary precision</strong>. In formal terms, the uncertainty principle tells us that for two non-commuting operators $\hat A, \hat B$, where $[\hat A, \hat B] = i\hat C$, then:</p>
<p class="mathcell">
$$
\Delta A \Delta B \geq \dfrac{|\langle \hat C\rangle|}{2}
$$
</p>
<p>For instance, in the case of the position and momentum operators (where $[\hat x, \hat p] = i\hbar$), this reduces to the famous <strong>Heisenberg uncertainty principle</strong>:</p>
<p class="mathcell">
$$
\Delta x \Delta p \geq \frac{\hbar}{2}
$$
</p>
<p>The consequence of the uncertainty principle is that if you measure one observable $A$ and then measure another observable $B$, if $\hat A, \hat B$ <em>do not commute</em>, then:</p>
<ol>
<li>They cannot <em>both be measured simultaneously</em> to perfect accuracy, and</li>
<li>A measurement of one observable <em>tells you nothing</em> about the other observable</li>
</ol>
<p>Let's break down what this means. Suppose you had an electron which initially is know to be spin-up along $z$. Mathematically, you know the precise value of the $\hat S_z$ operator (spin operator in the z direction): there is <strong>100% probability</strong> that the electron is in the spin-up state along the $z$ axis. What happens if you measure the electron's spin along $x$? Well, the electron is <strong>equally likely</strong> to be spin-up or spin-down along the $x$ axis because of rule (2). We already saw that $[\hat S_z, \hat S_x] \neq 0$ (that is, the spin operators along $z$ and $x$ <em>do not</em> commute), so knowing $S_z$ (the electron's spin along $z$) tells you nothing about $S_x$.</p>
<p>Now, what happens if you measure the electron's spin along $z$ again? "This is a pointless question!", you may say, "since I already measured the electron to be in the spin-up state along $z$, so it must certainly be in that same state!" However, things are not <em>quite</em> so simple. Remember that since the spin operators along $z$ and $x$ do not commute, measuring $S_x$ tells you nothing about $S_z$ (and vice-versa). This means that if you know $S_x$, you would not know <em>anything</em> about $S_z$ prior to measuring it. Since you had measured $S_x$, if you then choose to measure $S_z$, it would be a contradiction if the electron's spin along $z$ could also be precisely known to be in one state. Thus we are left with the profound and puzzling conclusion that upon measuring $S_z$ after $S_x$, the electron is again <strong>equally likely</strong> to be spin-up or spin-down along the $z$ axis. The previous information you had about the electron - namely, that it was in the spin-up state along $z$ - has now been erased!</p>
<h3 id="larmor-precession">Larmor precession</h3>
<p>If we think about a spinning top rotating on a table, it will rotate round and round in a circle tilted at an angle (until it inevitably falls). This phenomenon is known as <strong>precession</strong>, and is shown in the animation below:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/8/82/Gyroscope_precession.gif" alt="A gyroscope rotating at an angle, showcasing precession" /></p>
<p><em>Source: <a href="https://commons.wikimedia.org/wiki/File:Gyroscope_precession.gif">Wikimedia Commons</a></em></p>
<p>The reason why everyday precession happens is that the Earth's gravity produces a torque on the spinning top. But since it is also spinning, the top has <em>angular momentum</em> which resists the torque, leading to the gyroscope rotating at an angle. In physics, the gyroscope is said to <em>precess</em>. But angular momentum is not just found in the classical world: it is <em>also</em> found in the quantum world, and it leads to an effect called <strong>Larmor precession</strong>.</p>
<p>To understand Larmor precession, recall that we earlier noted that spin-1/2 particles have an <em>intrinsic angular momentum</em> that comes from their spin. We have notated this angular momentum as $\mathbf{S}$, as it is the <strong>spin angular momentum</strong>, and we know it must have a magnitude of $\pm \hbar/2$ along any axis (so long as it is a spin-1/2 particle). This angular momentum creates a magnetic moment $\vec{\boldsymbol{\mu}}$ associated with the particle, given by:</p>
<p class="mathcell">
$$
\vec{\boldsymbol{\mu}} = \gamma \mathbf{S}
$$
</p>
<p>Here, $\gamma$ is known as the <strong>gyromagnetic ratio</strong>, and it is a constant that can be calculated from the mass, charge, and other characteristics of the particle in question. From the magnetic moment, we can construct a Hamiltonian for a spin-1/2 particle placed in an applied magnetic field $\mathbf{B}$ with the spin operator $\mathbf{\hat S}$, given by:</p>
<p class="mathcell">
$$
\hat H = -\vec{\boldsymbol{\mu}} \cdot \mathbf{B} = -\gamma(\mathbf{\hat S} \cdot \mathbf{B})
$$
</p>
<p>It is usually easiest to consider a magnetic field aligned along the $z$-direction, and thus we have:</p>
<p class="mathcell">
$$
\hat H = -\gamma \hat S_z B_z
$$
</p>
<p>By solving for the eigenvalues and eigenstates of the Hamiltonian, we get two spin eigenstates, which are respectively given by:</p>
<p class="mathcell">
$$
|\uparrow_z\rangle = \begin{pmatrix} 1 \\ 0 \end{pmatrix},\quad 
|\downarrow_z\rangle = \begin{pmatrix} 0 \\ 1 \end{pmatrix}
$$
</p>
<p>Meanwhile, the energy eigenvalues are given by:</p>
<p class="mathcell">
$$
E_{\pm z} = \pm \frac{1}{2}\hbar \gamma B_z
$$
</p>
<p>This tells us that we have two distinct energy eigenstates: the first one with higher energy, and the second one with lower energy. The energy difference between the two energy levels can be written in terms of the <strong>Larmor frequency</strong> $\omega_0$ via:</p>
<p class="mathcell">
$$
\begin{align*}
\Delta E &amp;= E_+ - E_- \\
&amp;= \hbar \gamma B_z \\
&amp;= \hbar \omega_0, \quad \omega_0 \equiv |\gamma B_z|
\end{align*}
$$
</p>
<p>Physically, when a magnetic field is applied, we say that the magnetic moments of a spin-1/2 particle <strong>precess</strong>, since they rotate (just like a gyroscope) to line up along or against the magnetic field. This type of precession is called <strong>Larmor precession</strong>, and it is very similar to the classical precession of a gyroscope, with some notable differences being that there is nothing physically "rotating" at an angle; rather, it is the <em>magnetic moment vector</em> that becomes tilted off-axis, which is why we term it as <em>precession</em>.</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Precession_in_magnetic_field.svg/120px-Precession_in_magnetic_field.svg.png" alt="A diagram showing Larmor precession, where the magnetic moment vector rotates in a circle at an angle due to spin-magnetic-field interactions" /></p>
<p><em>A diagram of Larmor precession. Source: <a href="https://commons.wikimedia.org/wiki/File:Precession_in_magnetic_field.svg">Wikimedia Commons</a></em></p>
<p>Precise measurements of Larmor precession are essential in many areas of science, especially in <strong>nuclear magnetic resonance (NMR) spectroscopy</strong>, which is used in the medical sciences, as well as biotechnology and biochemistry. Additionally, many electronic devices (like hard disk storage) relies on exploiting the effects of spin, with an entire field of [spintronics](<a href="https://en.wikipedia.org/wiki/Spintronics">https://en.wikipedia.org/wiki/Spintronics</a> devoted to research in this area. With so many observations proving the existence of spin, we know for certain that even if spin runs counter to our intuitions (pun intended!), it is nevertheless a very real aspect of the quantum world.</p>
<h3 id="generalized-two-level-systems">Generalized two-level systems</h3>
<p>We will close our introductory discussion of spin and the ways to analyze spin by quickly discussing the generalization of a spin-1/2 system: <strong>two-level systems</strong>. Two-level systems (that is, a quantum system with two degrees of freedom) are found throughout quantum mechanics. Since spin-1/2 particles can either be spin-up or spin-down, which are the only two possible states in a spin-1/2 system, it is the <strong>prototypical two-level system</strong>. However, it is by no means unique: there are other two-level systems out there, and they can often be modelled using the same tools. For instance, it is very common to see Hamiltonians of the form:</p>
<p class="mathcell">
$$
\hat H \propto \sigma_i
$$
</p>
<p>Where $\sigma_i \in [\sigma_x, \sigma_y, \sigma_z]$ is a Pauli matrix, and they obey:</p>
<p class="mathcell">
$$
\begin{align*}
[\sigma_x, \sigma_y] =  \sigma_z \\
[\sigma_y, \sigma_z] =  \sigma_x \\
[\sigma_z, \sigma_x] =  \sigma_y
\end{align*}
$$
</p>
<p>Two-level systems are the fundamental model behind a vast variety of quantum systems, including <a href="https://qubit.guide">qubits</a> in quantum computing, <a href="https://en.wikipedia.org/wiki/Rabi_cycle">optically-pumped lasers</a>, and <a href="https://web1.eng.famu.fsu.edu/~dommelen/quantum/style_a/hion.html">molecular ions</a>, as well as playing an important role in understanding the emission and absorption of radiation at the quantum level. We will discuss more examples of them throughout this guide - stay tuned!</p>
<h2 id="the-quantum-harmonic-oscillator">The quantum harmonic oscillator</h2>
<p>One of the simplest, but most important quantum systems is the <strong>quantum harmonic oscillator</strong>. At face value, it describes a particle in a harmonic potential well. To start, let us recall that the classical harmonic potential is given by:</p>
<p class="mathcell">
$$
V(x) = \dfrac{1}{2} kx^2 = \dfrac{1}{2} m\omega^2 x^2, \quad \omega \equiv \sqrt{k&#x2F;m}
$$
</p>
<p>The classical solutions to the (classical) harmonic oscillator are in terms of sinusoidal functions, which is why it is indeed called the <em>harmonic</em> oscillator. However, on a basic level, a harmonic potential is nothing more than a basic quadratic potential, one that we show in the below diagram:</p>
<img class="diagram"
	
		src="/quantum-mechanics/harmonic-potential.excalidraw.svg"
	
	alt="A plot of the harmonic potential, showing that it has an energy that is quadratic with the potential and symmetric about the x-axis" />
<p>In quantum mechanics, we retain the same form of the harmonic potential, except we perform the substitution $x \to \hat x$, where $\hat x$ is the position operator. Thus the Hamiltonian is given by:</p>
<p class="mathcell">
$$
\hat H = \dfrac{\hat p^2}{2m} + V(x) = \dfrac{\hat p^2}{2m} + \dfrac{1}{2} m\omega^2 \hat x^2
$$
</p>
<p>Note that we can also write this in the position representation (where $\hat x = x$ and $\hat p = -i\hbar \frac{d}{dx}$ in 1D) as:</p>
<p class="mathcell">
$$
\hat H = -\frac{\hbar^2}{2m} \dfrac{d^2}{dx^2} + \frac{1}{2} m \omega^2 x^2
$$
</p>
<p>The quantum harmonic oscillator is a very useful model in quantum mechanics, since it is one of the few problems that can be solved exactly. This <em>does not</em> mean it is trivial - the quantum harmonic oscillator finds numerous applications in molecular and atomic physics. The quantum harmonic oscillator can first be used as an approximation for a complicated potential. This is because the Taylor expansion of an <em>arbitrary</em> potential centered at $x = x_0$ is given by:</p>
<p class="mathcell">
$$
V(x) = V_0 + V&#x27;(x-x_0) + \dfrac{1}{2} V&#x27;&#x27;(x-x_0)^2 + \dfrac{1}{6} V&#x27;&#x27;&#x27;(x-x_0)^3 + \dots
$$
</p>
<p>Another application is to describe the interaction of a charged (quantum) particle with an <em>standing electromagnetic waves</em>, something we will discuss later. When an electromagnetic field is trapped in some cavity, it decomposes into a series of modes, whose wavelength can only come in quantized values:</p>
<p class="mathcell">
$$
\omega = \dfrac{2\pi c}{\lambda}, \quad \lambda = \dfrac{2L}{n}, \quad n = 1,2,3, \dots
$$
</p>
<p>Thus, any charged particle within such a cavity will interact with the standing waves of the electromagnetic field, leading to its energy levels being quantized. This is the origin of uniquely quantum phenomena such as the <a href="https://en.wikipedia.org/wiki/Stark_effect">Stark effect</a>, but we'll explain this in more detail later.</p>
<p>The third main application of the quantum harmonic oscillator is to describe the interaction of a particle with a quantized electromagnetic field, which is the realm of <strong>quantum electrodynamics</strong> and <em>second quantization</em>. While this can get complicated very quickly, the essence is to describe the quantized electromagnetic field as a series of coupled harmonic oscillators. We will cover this more at the very end of this guide.</p>
<p>To solve the quantum harmonic oscillator we begin with the same general methods as for essentially any quantum system - to write out the eigenvalue equation for the Hamiltonian:</p>
<p class="mathcell">
$$
\hat H|\psi\rangle = E|\psi\rangle
$$
</p>
<p>This is the starting point, and there are several different ways to proceed from here. For instance, we can solve the eigenvalue equation in the position basis by taking the inner product with a position basis ket:</p>
<p class="mathcell">
$$
\begin{gather*}
\langle x|\hat H|\psi\rangle = \langle x|E|\psi\rangle \\
\langle x|\hat H|\psi\rangle = E\langle x|\psi\rangle \\
\left\langle x \left|-\dfrac{\hbar^2}{2m}\dfrac{d^2}{dx^2} + \dfrac{1}{2} m\omega^2 \hat x^2\right|\psi\right\rangle = E\langle x|\psi\rangle \\
\Rightarrow -\dfrac{\hbar^2}{2m}\dfrac{d^2 \psi}{dx^2} + \dfrac{1}{2} m\omega^2 x^2 \psi(x) = E \psi(x)
\end{gather*}
$$
</p>
<p>This is a differential equation that can indeed be solved, although it is not very easy to solve. Indeed, a much better approach is to use the so-called <em>algebraic approach</em>, which originated with the physicist Paul Dirac, which we'll now discuss.</p>
<h3 id="the-ladder-operator-approach">The ladder operator approach</h3>
<p>Dirac's key insight in solving the quantum harmonic oscillator is to "factor" the Hamiltonian by defining two new operators $\hat a$ and $\hat a^\dagger$, given by:</p>
<p class="mathcell">
$$
\begin{align*}
\hat a &amp;= \dfrac{1}{\sqrt{2}}(\hat x&#x27;+ i\hat p&#x27;) \\
\hat a^\dagger &amp;= \dfrac{1}{\sqrt{2}}(\hat x&#x27; - i\hat p&#x27;)
\end{align*}
$$
</p>
<p>Where $\hat x', \hat p'$ are related to the position and momentum operators $\hat x, \hat p$ as follows:</p>
<p class="mathcell">
$$
\hat x&#x27; = \sqrt{\dfrac{m\omega}{\hbar}} \hat x, \quad \hat p&#x27; = \dfrac{1}{\sqrt{m\hbar \omega}} \hat p
$$
</p>
<p>It is also useful to note that:</p>
<p class="mathcell">
$$
\begin{align*}
\hat x&#x27; &amp;= \frac{1}{\sqrt{2}}(\hat a^\dagger + \hat a) \\ 
\hat p&#x27; &amp;= \dfrac{i}{\sqrt{2}}(\hat a^\dagger - \hat a) \\
\end{align*}
$$
</p>
<p>$\hat a$ and $\hat a^\dagger$ are conventionally called the <strong>ladder operators</strong>. These two operators satisfy $[\hat a, \hat a^\dagger] = 1$, an important identity to keep in mind for later. While many steps avoid using this approach and write $\hat a$ and $\hat a^\dagger$ <em>purely</em> in terms of $\hat x$ and $\hat p$, by defining our new operators $\hat x', \hat p'$ we can <em>non-dimensionalize</em> the problem, making it easier to solve. This is essentially the same thing as a change of variables in a classical mechanics problem, only here we're using operators, not classical functions.</p>
<blockquote>
<p><strong>Note:</strong> While $\hat a$ and $\hat a^\dagger$ are indeed adjoints of each other, it is common to consider them essentially separate operators (for reasons we'll soon see). It is also common to use the notation $(\hat a_-, \hat a_+)$ instead of $(\hat a, \hat a^\dagger)$ (where $\hat a_- = \hat a$ and $\hat a_+ = \hat a^\dagger$) which is a completely equivalent notation.</p>
</blockquote>
<p>Thus, with our operators $\hat x'$ and $\hat p'$, the Hamiltonian can be written as:</p>
<p class="mathcell">
$$
\hat H = \hbar \omega \hat H&#x27;, \quad \hat H&#x27; = \dfrac{1}{2}(\hat x&#x27;^2 + \hat p&#x27;^2)
$$
</p>
<p>These operators allow us to simplify the Hamiltonian down greatly, since we find that:</p>
<p class="mathcell">
$$
\begin{align*}
\hbar \omega\left(\hat a^\dagger \hat a + \frac{1}{2}\right) &amp;= \hbar \omega\left(\frac{1}{2}(\hat x&#x27; + i\hat p&#x27;)(\hat x&#x27; - i\hat p&#x27;) + \frac{1}{2}\right)\\
&amp;= \dfrac{1}{2}\hbar \omega \left(\hat x&#x27;^2 - i\hat x&#x27; \hat p&#x27; + i\hat x&#x27; \hat p&#x27; + \hat p&#x27;^2 + 1\right) \\
&amp;= \dfrac{1}{2}\hbar \omega(\hat x&#x27;^2 + \hat p&#x27;^2 + i[\hat x&#x27;, \hat p&#x27;] + 1) \\
&amp;= \dfrac{1}{2}\hbar \omega(\hat x&#x27;^2 + \hat p&#x27;^2 -1 + 1) \\
&amp;= \hbar \omega H&#x27; \\
&amp;= \hat H
\end{align*}
$$
</p>
<p>If one defines another new operator $\hat N$ (we will discuss what this means later), given by $\hat N = \hat a^\dagger \hat a$, then the Hamiltonian takes the form:</p>
<p class="mathcell">
$$
\hat H = \hbar \omega \left(\hat N + \dfrac{1}{2}\right)
$$
</p>
<blockquote>
<p><strong>Note:</strong> Be careful of the order of the $\hat a$ and $\hat a^\dagger$ operators! This is because $\hat N = \hat a^\dagger \hat a$, but $\hat a^\dagger \hat a \neq \hat a \hat a^\dagger$ so $\hat N \neq \hat a \hat a^\dagger$! Indeed we find that $\hat a \hat a^\dagger = 1 + \hat N$, which can be derived from the commutation relation $[\hat a, \hat a^\dagger] = 1$.</p>
</blockquote>
<p>The genius of using this operator-based "algebraic" approach to solving the quantum harmonic oscillator is that the $\hat a, \hat a^\dagger$ operators satisfy:</p>
<p class="mathcell">
$$
\hat a|\psi_0\rangle = 0, \quad \hat a^\dagger |\psi_0\rangle = |\psi_1\rangle
$$
</p>
<p>In fact, in the general case, we find that for the $n$-th eigenstate $|\psi_n\rangle$ we have:</p>
<p class="mathcell">
$$
\hat a|\psi_n\rangle = \sqrt{n}|\psi_{n - 1}\rangle, \quad \hat a^\dagger|\psi_n\rangle = \sqrt{n + 1}~|\psi_{n + 1}\rangle
$$
</p>
<p>It is common convention to indicate the $n$-th eigenstate of the quantum harmonic oscillator with $|n\rangle = |\psi_n\rangle$, in which case one may write the more elegant expression:</p>
<p class="mathcell">
$$
\hat a|n\rangle = \sqrt{n}~|n-1\rangle, \quad \hat a^\dagger|n\rangle = \sqrt{n+1}~|n + 1\rangle
$$
</p>
<p>Thus we again find that $\hat a|0\rangle = \hat a|\psi_0\rangle = 0$ and $\hat a^\dagger |0\rangle = \hat a|\psi_0\rangle = |\psi_1\rangle$. These are indeed the chief identities of the quantum harmonic oscillator, because if we substitute in the definitions of the $\hat a, \hat a^\dagger$ operators, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\hat a^\dagger \hat a|n\rangle &amp;= \hat a^\dagger(\sqrt{n}~|n+1\rangle) \\
&amp;= \sqrt{n^2} |(n + 1) - 1\rangle \\
&amp;= n|n\rangle
\end{align*}
$$
</p>
<p>But we already know that $\hat a^\dagger \hat a$ is just the $\hat N$ operator, so we therefore have:</p>
<p class="mathcell">
$$
\hat
N|n\rangle = n|n\rangle
$$
</p>
<p>In addition, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\hat a^\dagger\hat a|n\rangle &amp;= \hat N|n\rangle 
= n|n\rangle \\
\hat a \hat a^\dagger|n\rangle &amp;= (\hat N + \hat I)|n\rangle 
= (n + 1)|n\rangle
\end{align*}
$$
</p>
<p>The $\hat N$ operator is often called the <strong>number operator</strong> since it returns the index $n$ corresponding to the <em>nth</em> eigenstate. For instance, for the first eigenstate $|0\rangle$ (in more traditional notation, this can be written as $|\psi_0\rangle$, which is equivalent), $\hat N|0\rangle = 0$, which tells us that (as we expect) the first eigenstate is labelled with index $n = 0$. Likewise, for the eigenstate $|3\rangle$ then $\hat N|3\rangle = 3 |n\rangle$, which indeed returns its index $n = 3$. What makes it <em>particularly</em> special is how the number operator can be defined solely in terms of the $\hat a$ and $\hat a^\dagger$ operators, which is a very non-trivial result. Thus, if we substitute $\hat N|n\rangle = n|n\rangle$ into our Hamiltonian, we have:</p>
<p class="mathcell">
$$
\hat H|n\rangle = \hbar \omega \left(\hat N + \dfrac{1}{2}\right)|n\rangle = \underbrace{\hbar \omega \left(n + \dfrac{1}{2}\right)}_{E_n}|n\rangle
$$
</p>
<p>Where by comparison our expression with the Hamiltonian's eigenvalue equation $\hat H|n\rangle = E_n|n\rangle$, we find that:</p>
<p class="mathcell">
$$
E_n = \hbar \omega\left(n + \dfrac{1}{2}\right)
$$
</p>
<p>Thus, we have found the energy eigenvalues without needing to solve any differential equation, which is quite an enormous feat! Additionally, since the energies are only dependent on $n$, the energies are <strong>non-degenerate</strong>, meaning that each energy eigenvalue is associated with a <em>distinct</em> eigenstate. This is incredibly important because it's uncommon to encounter fully non-degenerate systems in quantum mechanics, where each eigenstate can be labelled by a single eigenvalue (think about our previous discussion of CSCOs).</p>
<p>In addition, we can also calculate the eigenstates in the position (or momentum) bases, so the algebraic formalism helps us get the wavefunctions too. To do so, let us recognize that if we take the $n = 0$ state (often called the <em>ground state</em>, and notated as $|0\rangle$), we have $\hat a |0\rangle = 0$. Keep this in mind! Now, recall that we <em>defined</em> our relevant operators to take the following forms:</p>
<p class="mathcell">
$$
\begin{align*}
\hat a &amp;= \dfrac{1}{\sqrt{2}}(\hat x&#x27;+ i\hat p&#x27;) \\
&amp;= \sqrt{\dfrac{m\omega}{2\hbar}}\hat x + \frac{i}{\sqrt{2m\omega \hbar}}\hat p  \\
\hat a^\dagger &amp;= \dfrac{1}{\sqrt{2}}(\hat x&#x27; - i\hat p&#x27;) \\
&amp;= \sqrt{\dfrac{m\omega}{2\hbar}}\hat x + \frac{i}{\sqrt{2m\omega \hbar}}\hat p
\end{align*}
$$
</p>
<p>Thus, the explicit forms of $\hat a$ and $\hat a^\dagger$ can be found in the position basis by substituting in $\hat x = x$ and $\hat p = -i\hbar \dfrac{d}{dx}$, giving us:</p>
<p class="mathcell">
$$
\begin{align*}
\hat a &amp;= \sqrt{\frac{m\omega}{2\hbar}} \left(x + \frac{\hbar}{m\omega} \dfrac{d}{dx}\right) \\
\hat a^\dagger &amp;= \sqrt{\frac{m\omega}{2\hbar}} \left(x - \frac{\hbar}{m\omega} \dfrac{d}{dx}\right)
\end{align*}
$$
</p>
<p>Now substituting these the explicit form of $\hat a$ in the position basis, we have:</p>
<p class="mathcell">
$$
\begin{gather*}
\hat a|0\rangle = 0 \\
\Rightarrow ~ \hat a\langle x|0\rangle = \hat a\psi_0(x)  = 0 \\
\Rightarrow ~ \left(x + \dfrac{\hbar}{m\omega}  \dfrac{d}{dx}\right)\psi_0(x) = 0
\end{gather*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> It is also possible to do this in the <em>momentum basis</em> but the calculations become much more hairy. See <a href="https://physics.stackexchange.com/questions/632095/eigenstates-of-qm-harmonic-oscillator-in-momentum-space">this Physics StackExchange answer</a> if interested.</p>
</blockquote>
<p>This gives us a differential equation to solve, albeit a much easier one that can be solved explicitly by the standard methods of solving 1st-order differential equations. The solution is a Gaussian function, and in particular:</p>
<p class="mathcell">
$$
\psi_0(x) = C e^{-m\omega x^2 &#x2F; (2\hbar)}
$$
</p>
<p>And applying the normalization condition, the undetermined constant $C$ can be found to be $C = \left(\frac{m\omega}{\pi \hbar}\right)^{1/4}$, and thus we have:</p>
<p class="mathcell">
$$
\psi_0(x) = \left(\dfrac{m\omega}{\pi \hbar}\right)^{1&#x2F;4} e^{-m\omega x^2 &#x2F; (2\hbar)}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Since this is a Gaussian function, it is thus symmetric about $x = 0$, and thus we can infer that the expectation value of $\psi_0(x)$ (the quantum harmonic oscillator in its ground state) is $\langle x\rangle = 0$, which is <em>also</em> true for the classical harmonic oscillator.</p>
</blockquote>
<p>We can then use the definition of the $\hat a^\dagger$ operator to find all of the wavefunctions for the higher-energy states, since:</p>
<p class="mathcell">
$$
\begin{gather*}
\hat a^\dagger|n\rangle = \sqrt{n + 1}~|n+1\rangle \\
\Rightarrow ~ \hat a^\dagger\langle x|n\rangle = \sqrt{n + 1}\langle x|n+1\rangle \\
\Rightarrow ~ \hat a^\dagger \psi_n(x) = (\sqrt{n + 1} )~\psi_{n + 1}(x)
\end{gather*}
$$
</p>
<p>Thus, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\psi_{n + 1}(x) &amp;= \dfrac{1}{\sqrt{n + 1}} \hat a^\dagger \psi_n(x) \\
&amp;= \psi_{n + 1}(x)  \\ &amp;= \ \sqrt{\frac{m\omega}{2(n+1)\hbar}} \left(x - \frac{\hbar}{m\omega} \dfrac{d}{dx}\right) \psi_n(x)
\end{align*}
$$
</p>
<p>This allows us to recursively construct all the eigenstates of the system. For instance, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\psi_1(x) &amp;= \dfrac{1}{\sqrt{0 + 1}}\hat a^\dagger \psi_0(x) \\
&amp;= \ \sqrt{\frac{m\omega}{2\hbar}} \left(x - \frac{\hbar}{m\omega} \dfrac{d}{dx}\right) \left[\left(\frac{m\omega}{\pi \hbar}\right)^{1&#x2F;4} e^{-m\omega x^2 &#x2F; (2\hbar)}\right] \\
&amp;= \left(\frac{m\omega}{\pi \hbar}\right)^{1&#x2F;4} \sqrt{\frac{2m\omega}{\hbar}} \, x \, e^{-\frac{m\omega}{2\hbar}x^2} \\
&amp;= \left(\dfrac{4m^3\omega^3}{\pi \hbar^3}\right)^{1&#x2F;4} x e^{-m\omega x^2 &#x2F; (2\hbar)}
\end{align*}
$$
</p>
<p>In general, we have:</p>
<p class="mathcell">
$$
\begin{gather*}
|n\rangle = \dfrac{(\hat a^\dagger)^n}{\sqrt{n!}}|0\rangle, \\
\psi_n(x) = \langle x|n\rangle = \sqrt{\frac{m\omega}{2\hbar n!}} \left(x - \frac{\hbar}{m\omega} \dfrac{d}{dx}\right)^n \psi_0(x)
\end{gather*}
$$
</p>
<p>In the same way, we can get $\psi_2$ from applying the $\hat a^\dagger$ operator on $\psi_1$, then get $\psi_3$ from $\psi_2$, then get $\psi_4$ from $\psi_3$, and so on and so forth. With some clever mathematics (that we won't show here), this recursive formula can be solved in closed-form to yield a <em>generalized</em> formula for the <em>nth</em> eigenstate's wavefunction representation:</p>
<p class="mathcell">
$$
\psi_n(x) = \left(\dfrac{m\omega}{\pi \hbar}\right) \dfrac{1}{\sqrt{2^nn!}}H_n\left(\sqrt{\dfrac{m\omega}{\hbar}}x\right)\exp \left(-\dfrac{m\omega x^2}{2\hbar}\right)
$$
</p>
<p>Here, $H_n(x)$ is a <strong>Hermite polynomial</strong> of order $n$, defined as:</p>
<p class="mathcell">
$$
H_n(x) = (-1)^n e^{x^2} \frac{d^n}{dx^n} e^{-x^2}
$$
</p>
<p>Where the first three Hermite polynomials are given by:</p>
<p class="mathcell">
$$
\begin{align*}
H_0(x) &amp;= 1 \\ H_1(x) &amp;= 2x \\ H_2(x) &amp;= 4x^2 - 2
\end{align*}
$$
</p>
<p>Using these definitions, we show a plot of the ground-state wavefunction $\psi_0(x)$ and several excited states' wavefunctions below:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/9/9e/HarmOsziFunktionen.png" alt="Plots of the wavefunction of the quantum harmonic oscillator for its first few energy levels" /></p>
<p><em>Source: <a href="https://commons.wikimedia.org/wiki/File:HarmOsziFunktionen.png">Wikimedia Commons</a></em></p>
<p>We can also keep things abstract by noting that we can express any of the wavefunctions of each state $\psi_n(x)$ as $\psi_n(x) = \langle x|n\rangle$, where:</p>
<p class="mathcell">
$$
|n\rangle = \dfrac{(\hat a^\dagger)^n}{\sqrt{n!}}|0\rangle, \quad \psi_0(x) = \langle x|0\rangle
$$
</p>
<p>The ladder operator approach to the quantum harmonic oscillator is a powerful technique, one that carries over to relativistic quantum mechanics and allows us to skip solving the Schrödinger equation entirely. In addition, solving problems using operators alone will give us the tools to understand the <strong>Heisenberg picture</strong> of quantum mechanics that we'll soon see.</p>
<blockquote>
<p><strong>Note for the advanced reader:</strong> The algebraic approach works mathematically because the eigenvalues of $\hat H$ are positive, and that the $\hat p^2$ operator is semi-positive definite. Additionally, the eigenspectrum (energy spectrum) of the system is discrete and <strong>non-degenerate</strong> (that is, all eigenstates have unique eigenvalues).</p>
</blockquote>
<h3 id="expectation-values-of-the-quantum-harmonic-oscillator">Expectation values of the quantum harmonic oscillator</h3>
<p>In any quantum system, it is very useful to find their expectation values, and the same is true for the quantum harmonic oscillator. In particular, we are interested in calculating $\langle x\rangle$ and $\langle p\rangle$, the expectation values of the position and momentum. To do so, we will follow <a href="https://brilliant.org/wiki/quantum-harmonic-oscillator/">an approach from Brilliant wiki's authors</a>. We start with the fact that:</p>
<p class="mathcell">
$$
\begin{align*}
\hat x&#x27; &amp;= \frac{1}{\sqrt{2}}(\hat a^\dagger + \hat a) \\ 
\hat p&#x27; &amp;= \dfrac{i}{\sqrt{2}}(\hat a^\dagger - \hat a) \\
\end{align*}
$$
</p>
<p>Now, we can express $\hat x'$ and $\hat p'$ in terms of $\hat x$ and $\hat p$ by simply rearranging the definitions of $\hat x'$ and $\hat p'$ from earlier:</p>
<p class="mathcell">
$$
\begin{align*}
\hat x&#x27; &amp;= \sqrt{\dfrac{m\omega}{\hbar}} \hat x \\
\hat p&#x27; &amp;= \dfrac{1}{\sqrt{m\hbar \omega}} \hat p \\
\end{align*}
\quad \Rightarrow \quad
\begin{align*}
\hat x &amp;= \sqrt{\dfrac{\hbar}{m\omega}}\hat x&#x27; \\
\hat p &amp;= \sqrt{m\hbar \omega}\, \hat p&#x27;
\end{align*}
$$
</p>
<p>Thus we have:</p>
<p class="mathcell">
$$
\begin{align*}
\hat x &amp;= \sqrt{\frac{\hbar}{2 m \omega}} (a^{\dagger} + a)\\
\hat p &amp;= i \sqrt{\dfrac{m \hbar \omega}{2}} (\hat{a}^{\dagger} - a)
\end{align*}
$$
</p>
<p>From here, we can easily calculate the expectation values of $x$ and $p$. We'll start with the expectation value of $x$:</p>
<p class="mathcell">
$$
\begin{align*}
\langle x \rangle &amp;= \langle n|\hat x|n\rangle \\
&amp;= \langle n| \sqrt{\frac{\hbar}{2 m \omega}} (\hat a^{\dagger} + \hat a)|n\rangle \\
&amp;= \sqrt{\frac{\hbar}{2 m \omega}} \big[\langle n|\hat a^\dagger|n\rangle + \langle n|\hat a |n\rangle\big] \\ 
&amp;= \sqrt{\frac{\hbar}{2 m \omega}} \big[\sqrt{n + 1} \langle n|n+1\rangle + \sqrt{n}\langle n|n-1\rangle] \\
&amp; = 0
\end{align*}
$$
</p>
<p>Where we used our definitions $\hat a^\dagger = \sqrt{n + 1} |n+1\rangle$, $\hat a = \sqrt{n} |n-1\rangle$ and also utilized the fact that any two eigenstates are orthogonal, that is, $\langle n|m\rangle = \delta_{mn}$, so $\langle n|n+1\rangle$ and $\langle n|n-1\rangle$ are both automatically zero. We can do the same thing with the momentum operator:</p>
<p class="mathcell">
$$
\begin{align*}
\langle p \rangle &amp;= \langle n|\hat p|n\rangle \\
&amp;= \langle n| i \sqrt{\dfrac{m \hbar \omega}{2}} (\hat a^{\dagger} - \hat a)|n\rangle \\
&amp;= i \sqrt{\dfrac{m \hbar \omega}{2}} \big[\langle n|\hat a^\dagger|n\rangle - \langle n|\hat a |n\rangle\big] \\ 
&amp;= i \sqrt{\dfrac{m \hbar \omega}{2}} \big[\sqrt{n + 1} \langle n|n+1\rangle - \sqrt{n}\langle n|n-1\rangle] \\
&amp; = 0
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> The result that $\langle x\rangle = \langle p\rangle = 0$ only holds true for <em>single eigenstates</em>. It does <em>not</em> necessarily hold true in a <em>superposition</em> of states.</p>
</blockquote>
<p>The same methods of calculation can be used to establish that:</p>
<p class="mathcell">
$$
\begin{align*}
\langle \hat x^2\rangle &amp;= \dfrac{\hbar}{2m\omega}(2n + 1) \\
\langle \hat p^2 \rangle &amp;= \dfrac{\hbar m\omega}{2}(2n + 1)
\end{align*}
$$
</p>
<p>From the formula for the uncertainty of an observable $\Delta A = \sqrt{\langle A^2\rangle - \langle A\rangle^2}$ this tells us that:</p>
<p class="mathcell">
$$
\Delta x \Delta p = \dfrac{\hbar}{2}(2n + 1)
$$
</p>
<p>Where for the ground state ($n = 0$) we have the <strong>minimum uncertainty</strong>:</p>
<p class="mathcell">
$$
\Delta x \Delta p = \dfrac{\hbar}{2}
$$
</p><h3 id="the-quantum-harmonic-oscillator-in-higher-dimensions">The quantum harmonic oscillator in higher dimensions</h3>
<p>It is also possible to solve the quantum harmonic oscillator in higher dimensions. Indeed, consider the quantum harmonic oscillator along a 2D plane or a 3D box, where we can use Cartesian coordinates. Here, we do not actually need to do much more solving at all. The only difference is that rather than a single integer $n$, we need one integer $n$ for each coordinate. That is to say, for 2D we need two integers $n_x, n_y$, and for 3D we need three integers $n_x, n_y, n_z$ to describe all the eigenstates of the system. Therefore, the respective energy eigenvalues are:</p>
<p class="mathcell">
$$
\begin{align*}
E_{n_x, n_y}^{(2D)} &amp;= \hbar \omega \left(n_x + n_y + \dfrac{1}{2}\right) \\
E_{n_x, n_y, n_z}^{(3D)} &amp;= \hbar \omega \left(n_x + n_y + n_z + \dfrac{1}{2}\right)
\end{align*}
$$
</p>
<p>Note that this means that we have <strong>degenerate</strong> eigenenergies, losing one of the key distinguishing features of the 1D quantum harmonic oscillator. In addition, the excited-state wavefunctions are unfortunately much more complicated for the 2D and 3D cases; we will restrict our attention to just the ground-state wavefunction. In $K$ dimensions, the ground-state wavefunction is given by:</p>
<p class="mathcell">
$$
\psi_0(\mathbf{r}) = \left(\dfrac{m\omega}{\pi \hbar}\right)^{K&#x2F;4} \exp\left(-\dfrac{m\omega}{2\hbar}r^2\right), \quad r = |\mathbf{r}|
$$
</p>
<p>Note that in 2D and 3D, we also have different cases, such as the quantum harmonic oscillator across a disk or in a spherical region. In these cases, the ground-state wavefunction exhibits polar and spherical symmetry respectively, so the general solutions are quite different from the 1D case. This is very important in nuclear and molecular physics, although we will not discuss it further here.</p>
<blockquote>
<p><strong>Note for the interested reader:</strong> If you are interested in further applications of the quantum harmonic oscillator, it can be used to model diatomic molecules like $\ce{N2}$ or $\ce{O2}$ and describe atomic nuclei with the <a href="https://en.wikipedia.org/wiki/Nuclear_shell_model">nuclear shell model</a>, as well as serving an important role in <em>second quantization</em> of light - something we'll see more of later.</p>
</blockquote>
<h2 id="time-evolution-in-quantum-systems">Time evolution in quantum systems</h2>
<p>In all areas of physics, we're often interested in how systems <em>evolve</em>. A system that depends on time is usually called a <strong>dynamical system</strong>, and at different points in time, the state of the system changes. Now, if we know that at some initial time $t_0$ a system is in a particular state $A$, and at some arbitrary later time $t$ is in another state $B$, the <em>time evolution</em> of the system describes how the system "gets" from $A$ to $B$.</p>
<p>Consider a very simple example: a particle moving along a line. Its state is described by a single variable - position - which we describe with $x$. In physics, we would describe the motion of this particle with a function $x(t)$, which is a <strong>trajectory</strong>. This trajectory is the time evolution of the system, because from a certain initial time $t_0$, we can calculate the particle's position at any future time $t$ with $x(t)$.</p>
<p>In quantum mechanics, we also see quantum systems exhibit time evolution. For instance, the state-vector may have an initial state $|\psi(t_0)\rangle$ at time $t = t_0$, and at some future time $t$ have the final state $|\psi(t)\rangle$. The question is, how does that initial state become the final state? The answer to that question is the <strong>time-evolution operator</strong> $\hat U(t, t_0)$, which satisfies:</p>
<p class="mathcell">
$$
|\psi(t)\rangle = \hat U(t, t_0)|\psi(t_0)\rangle
$$
</p>
<p>That is to say, the time-evolution operator maps the system's state at an initial time $t_0$ to its future state at time $t$. But how does this all work? This is what we'll explore in this section.</p>
<h3 id="unitary-operators">Unitary operators</h3>
<p>Before we go more in-depth into the time-evolution operator, we need to introduce the idea of a <strong>unitary operator</strong>. An arbitrary unitary operator $\hat U$ (forget about the time-evolution operator for now) satisfies two <em>essential</em> properties:</p>
<ol>
<li>$\hat U^{-1} = U^\dagger$, that is, its inverse is equal to its adjoint.</li>
<li>$\hat U \hat U^{-1} = \hat U^{-1} \hat U = 1$, that is, multiplying a unitary operator by its adjoint gives the identity matrix. Together with the first rule, this automatically means that $\hat U \hat U^\dagger = \hat U^\dagger \hat U = 1$.</li>
</ol>
<blockquote>
<p><strong>Note on notation:</strong> We will frequently use the shorthand $\hat I = 1$, where $\hat I$ is the identity matrix, when discussing operators, but remember that matrix multiplication always gives another matrix (and not the scalar number 1), so this is just a shorthand!</p>
</blockquote>
<p>Note that unitary operator is <em>not necessarily Hermitian</em> - in fact, it usually isn't! So why do we care about a non-Hermitian operator when most of the operators we use in quantum mechanics are Hermitian? Well, if we act a unitary operator $\hat U$ on a (normalized) state-vector, we find that:</p>
<p class="mathcell">
$$
(\hat U |\psi\rangle)^\dagger(\hat U |\psi\rangle) = \langle \psi |\hat U^\dagger \hat U|\psi\rangle = \langle \psi|\psi\rangle = 1
$$
</p>
<p>This is the most important property of a unitary operator - it preserves the <strong>normalization</strong> of the state-vector! That is to say, acting $\hat U$ on $|\psi\rangle$ does <em>not</em> change its normalization $\langle \psi|\psi\rangle = 1$.</p>
<h3 id="the-unitary-time-evolution-operator">The unitary time-evolution operator</h3>
<p>Now let's return back to the time-evolution operator. It is no accident that we denoted the time-evolution operator as $\hat U(t, t_0)$ and a unitary operator as $\hat U$. This is because the time-evolution operator <strong><em>is</em> a unitary operator</strong>. It is indeed common to call the time-evolution operator the <em>unitary time-evolution operator</em> for this very reason! So from this point on, anytime you see $\hat U$, that means the time-evolution operator (unless otherwise stated).</p>
<p>Here is where the unitary nature of the time-evolution operator truly makes sense. This is because by knowing $\hat U \hat U^\dagger = \hat U^\dagger \hat U = 1$, and that $|\psi(t)\rangle = \hat U|\psi\rangle$, we also know that:</p>
<p class="mathcell">
$$
\langle \psi(t)|\psi(t)\rangle = \langle \psi(t_0)|\hat U^\dagger\hat U(t, 0)|\psi(t_0)\rangle = \langle \psi(t_0)|\psi(t_0)\rangle = 1
$$
</p>
<p>That is, if a state-vector $|\psi\rangle$ is normalized at $t = t_0$, it will <em>continue</em> to be normalized for all future times $t$, satisfying the <strong>normalization condition</strong>. This means that the time-evolution operator $\hat U$ automatically guarantees <strong>conservation of probability</strong> in a dynamical quantum system (a system that changes with time). Furthermore, we also add the requirement that the time-evolution operator must satisfy:</p>
<p class="mathcell">
$$
\hat U(t_0, t_0) = 1
$$
</p>
<p>This means that:</p>
<p class="mathcell">
$$
\hat U(t_0, t_0)|\psi(t_0)\rangle = |\psi(t_0)\rangle
$$
</p>
<p>Therefore operating $\hat U$ on the state-vector returns the system in its initial state at $t = t_0$. This makes sense because at the initial time $t_0$, the system hasn't had any time to evolve, so acting the time-evolution operator on it does nothing but tell you the initial state!</p>
<blockquote>
<p><strong>Note:</strong> Another name for the unitary time-evolution operator is the <strong>propagator</strong>, which is common in advanced quantum mechanics. Later on in this guide, when we cover the <strong>path integral formulation</strong> of quantum mechanics, we'll speak of $\hat U$ as the propagator. Remember that whether we call $\hat U$ the unitary time-evolution operator or the propagator, we are referring to the same thing!</p>
</blockquote>
<p>Now, we've spoken a lot about what the time-evolution operator $\hat U$ <em>does</em>, but how do we express it in explicit form? To be able to start, let's write out the Schrödinger equation in a special form. The most general form of the Schrödinger equation - at least, in the form we've generally seen - is given by:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t}|\psi(t)\rangle = \hat H|\psi(t)\rangle
$$
</p>
<p>But since $|\psi(t)\rangle = \hat U|\psi\rangle$, this can <em>also</em> be written as:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t}\hat U|\psi(t_0)\rangle = \hat H(\hat U|\psi(t_0)\rangle)
$$
</p>
<p>Since $|\psi(t_0)\rangle$ does not depend on time, we can factor it out from both sides, giving us:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t} \hat U = \hat H \hat U
$$
</p>
<p>Which can be written more explicitly as:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t} \hat U(t, t_0) = \hat H \hat U(t, t_0)
$$
</p>
<p>This is the <strong>essential equation of motion</strong> for the unitary operator. We'll now do something that may defy intuition but is actually mathematically sound. First, we'll temporarily drop the operator hats and not write out the explicit dependence on $t$ and $t_0$, giving us:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial U}{\partial t} = HU
$$
</p>
<p>Now, dividing by $i\hbar$ from both sides gives us:</p>
<p class="mathcell">
$$
\dfrac{\partial U}{\partial t} = \frac{1}{i\hbar}HU = -\frac{i}{\hbar} HU
$$
</p>
<p>(Here we use the fact that $1/i = -i$, and $\dot U = \frac{\partial U}{\partial t}$). This now looks like a differential equation in the form $\dot U = -\frac{i}{\hbar} H U$! Solving this differential equation (using separation of variables) along with our known property $U(t_0) = 1$ gives us:</p>
<p class="mathcell">
$$
U = e^{-i H (t - t_0)&#x2F;\hbar}
$$
</p>
<p>Now, we can restore the operator hats and we can write the most general form of the time evolution operator:</p>
<p class="mathcell">
$$
\hat U(t, t_0) = \exp\left(-\dfrac{i}{\hbar} \hat H (t - t_0)\right)
$$
</p>
<p>If we adopt the convention of choosing $t_0 = 0$, this gives us:</p>
<p class="mathcell">
$$
\hat U(t) = \exp\left(-\dfrac{i}{\hbar} \hat H t\right), \quad \hat U(t) \equiv \hat U(t, 0)
$$
</p>
<p>Perhaps you might be inclined to answer with "You're wrong! What in the world is the exponential function of a matrix??" The way of making sense of this is to recognize that the exponential function can be defined in terms of a <strong>power series</strong>:</p>
<p class="mathcell">
$$
e^X = \exp(X) = \sum_{n = 0}^\infty \dfrac{X^n}{n!}
$$
</p>
<p>Taking powers of a matrix is a perfectly acceptable operation, and therefore a term like $\hat H^n$ would raise no alarms, since $\hat H^n = \underbrace{\hat H \hat H \dots \hat H}_{n \text{ times}}$. This allows us to write $\hat U(t)$ in the form:</p>
<p class="mathcell">
$$
\begin{align*}
\hat U &amp;= \sum_{n = 0}^\infty \frac{1}{n!}\left(-\dfrac{i}{\hbar} \hat H t\right) \\
&amp;= 1 -\frac{i}{\hbar} \hat H t - \frac{1}{2\hbar^2} \hat H^2 t^2 + \dots
\end{align*}
$$
</p>
<p>Usually, applying this definition is quite cumbersome (summing infinite terms is hard!) but if we truncate the series to just a few terms, we can often find a good approximation to the full series. For instance, if we truncate the series to first-order, we have:</p>
<p class="mathcell">
$$
\hat U \approx 1 -\frac{i}{\hbar} \hat H t
$$
</p>
<p>Using this approximation can allow us to calculate the future state of a time-dependent quantum system with only knowledge of the Hamiltonian and the initial state. Of course, since we truncated the series, this calculation can yield only an approximate answer, but in some cases an approximate answer is enough. Thus, the time-evolution operator is the starting-point for <strong>perturbative calculations</strong> in quantum mechanics, where we can make successively more accurate approximations to the future state of a quantum system by invoking the time-evolution operator in series form, and taking only the first few terms.</p>
<h3 id="the-heisenberg-picture">The Heisenberg picture</h3>
<p>Introducing the time-evolution operator has an interesting consequence: it allows us to calculate the future state of any quantum system from a known initial state "frozen" in time. This is because the initial state of a quantum system has not had time to evolve yet, so it is <em>independent</em> of time. In fact, it is possible to dispense with time-dependence in calculations almost completely, because it turns out that there is <em>also</em> a way to calculate the measurable quantities of quantum systems at any future point in time without needing to explicitly calculate $|\psi(t)\rangle$. This approach is known as the <strong>Heisenberg picture</strong> in quantum mechanics.</p>
<p>Consider the position operator $\hat X$ (we will use an uppercase $X$ here for clarity). Normally, this is a time-independent operator, since we know it is defined by $\hat X|\psi_0\rangle = x|\psi_0\rangle$, where $|\psi_0\rangle$ is a stationary state and $x$ is a position eigenvalue: notice here that time does not appear <em>at all</em> as a variable. Taking the inner product of both sides with the bra $\langle \psi_0|$ gives us the <em>expectation value</em> of the position:</p>
<p class="mathcell">
$$
\langle \psi_0|\hat X |\psi_0\rangle = \langle \psi_0| x|\psi_0\rangle
$$
</p>
<p>Now, we want to find a <em>time-dependent</em> version of the position operator, which we'll call $\hat x_H(t)$, which also satisfies an eigenvalue equation:</p>
<p class="mathcell">
$$
\hat X_H(t)|\psi(t)\rangle = x(t)|\psi(t)\rangle
$$
</p>
<p>Notice how our position eigenvalue is now time-dependent, because as the state of the system changes, the positions $x(t)$ also change. Our challenge will be able to write $\hat X_H(t)$ in terms of $\hat X$. How can we do so? Well, recall that $|\psi(t)\rangle = \hat U|\psi(t_0)\rangle$, and $|\psi(t_0)\rangle$ is the same thing as $|\psi_0\rangle$. Thus we can write:</p>
<p class="mathcell">
$$
\hat X_H(t)|\psi(t)\rangle = \hat X\hat U|\psi_0\rangle
$$
</p>
<p>Now, let us take its inner product with the bra $\langle \psi(t)|$, which gives us:</p>
<p class="mathcell">
$$
\langle \psi(t)|\hat X_H(t)|\psi(t)\rangle = \langle \psi(t) |x\hat U|\psi_0\rangle
$$
</p>
<p>We'll now use the identity that:</p>
<p class="mathcell">
$$
|\psi(t)\rangle = \hat U |\psi_0\rangle \quad \Leftrightarrow \quad |\psi_0\rangle  = \hat U^\dagger |\psi(t)\rangle
$$
</p>
<p>You can prove this rigorously, but it can be intuitively understood by recognizing that $\hat U^\dagger = \hat U^{-1}$, meaning that just as $\hat U$ evolves the system <em>forwards</em> in time, $\hat U^\dagger$ evolves the system <em>backwards</em> in time (the "inverse" direction in time). Hence acting $\hat U^\dagger$ on a system at some time $t$ returns it to its original state at some past time $t_0$. With the same result, we note that:</p>
<p class="mathcell">
$$
\langle \psi(t)|\hat X_H(t)|\psi(t)\rangle = \langle \psi(t) |x\hat U|\psi_0\rangle = \langle \psi_0|\hat U^\dagger x \hat U|\psi_0\rangle
$$
</p>
<p>Thus by pattern-matching we have:</p>
<p class="mathcell">
$$
\hat X_H(t) = \hat U^\dagger x \hat U = \hat U^\dagger \hat X \hat U
$$
</p>
<p>Notice that the latter result holds for all time $t$! We have indeed arrived at our expression for the time-dependent version of the position operator $\hat X_H(t)$:</p>
<p class="mathcell">
$$
\hat X_H(t) = \hat U^\dagger \hat X \hat U
$$
</p>
<p>It is also common to say that $\hat X_H$ is the position operator in the <strong>Heisenberg picture</strong>. Unlike the <strong>Schrödinger picture</strong> that we've gotten familiar working with, the Heisenberg picture uses <em>time-dependent operators</em> that operate on a constant state-vector $|\psi\rangle = |\psi_{0}\rangle$. It is <em>completely equivalent</em> to the Schrödinger picture, but it is sometimes more useful, since we can dispense with calculating the state-vector's time evolution as long as we know the $\hat U$ operator, which can simplify (some) calculations. In the most general case, for any operator $\hat A$, its equivalent time-dependent version $\hat A_H$ in the Heisenberg picture is given by:</p>
<p class="mathcell">
$$
\hat A_H(t) = \hat U^\dagger \hat A \hat U
$$
</p>
<p>If we don't know $\hat U$, it is also possible to calculate $\hat A_H$ via the <strong>Heisenberg equation of motion</strong>, the analogue of the Schrödinger equation in the Heisenberg picture:</p>
<p class="mathcell">
$$
\dfrac{d}{dt} \hat{A}_{H}(t) = \frac{i}{\hbar}[\hat{H}, \hat{A}_{H}(t)]
$$
</p>
<p>A particularly powerful consequence of the Heisenberg picture is how easily it maps classical systems into a corresponding quantum system. For instance, the classical harmonic oscillator follows the equation of motion $\dfrac{d^2 x}{dt^2} + \omega^2 x = 0$, which has the (classical) solution:</p>
<p class="mathcell">
$$
\begin{align*}
x(t) &amp;= a e^{-i\omega t} + a^* e^{i\omega t} \\
p(t) &amp;= m \dfrac{dx}{dt} = b^*e^{-i\omega t} + be^{i\omega t}, \quad b =i\omega ma^*
\end{align*}
$$
</p>
<p>Where $a$ and $a^<em>$ here are some amplitude constants that can be specified by the initial conditions, and for generality, we assume that they can be complex-valued. Now, the Heisenberg picture tells us that if we want to find the corresponding quantum operators $\hat X_H(t), \hat p(t)$, all we have to do is to replace our <em>constants</em> $a, a^</em>$ to <em>operators</em> $\hat a, \hat a^\dagger$ (same with $b, b^*$), giving us:</p>
<p class="mathcell">
$$
\begin{align*}
\hat X_H(t) &amp;= \hat ae^{-i\omega t} + \hat a^\dagger e^{i\omega t} \\
\hat p(t) &amp;= \hat be^{-i\omega t} + \hat b^\dagger e^{i\omega t}, \quad \hat b = i\omega m a^\dagger
\end{align*}
$$
</p>
<p>Indeed, we can then identify $\hat a, \hat a^\dagger$ as just the <strong>ladder operators</strong> we're already familiar with from studying the quantum harmonic oscillator! In addition, we can also show that $\hat x$ satisfies a <em>nearly identical</em> equation of motion as the classical case ($\frac{d^2 x}{dt^2} + \omega^2 x = 0$), with the exception that the position <em>function</em> $x$ is replaced by the <em>operator</em> $\hat X_{H}$:</p>
<p class="mathcell">
$$
\dfrac{d^2 \hat X_H(t)}{dt^2} + \omega^2 \hat X_H(t) = 0
$$
</p>
<p>Notice the elegance correspondence between the classical and quantum pictures. By doing very little work, we have <em>quantized</em> a classical system, taking a classical variable ($x(t)$, representing a particle's position) and turning it ("promoting it") into a quantum operator $\hat X_{H}(t)$, a process formally called <strong>first quantization</strong>. This method will be essential once we discuss <strong>second quantization</strong>, where we take <em>classical</em> field theories and use them to construct <em>quantum</em> field theories. But we've not gotten to there yet! We'll save a more in-depth discussion of second quantization for later.</p>
<blockquote>
<p><strong>Note for the advanced reader:</strong> In second quantization, we essentially do the same thing as first quantization, but rather than quantizing the position (by taking the classical variable $x(t)$ and promoting it to an operator $\hat X_{H}$) we are interested in taking a classical field $\phi(x, t)$ and promoting it to an quantum field operator $\hat \phi$. Just as in first quantization, second-quantized fields follow the same equations of motion as their classical field analogues. In particular, the simplest type of quantum field (known as the <em>free scalar field</em>) obeys the equation $\partial^2_{t}\hat \phi - \nabla^2 \phi + m^2\phi = 0$, which is very similar to the harmonic oscillator equation of motion.</p>
</blockquote>
<h3 id="the-correspondence-principle-and-the-classical-limit">The correspondence principle and the classical limit</h3>
<p>As we have seen, the Heisenberg picture makes it easy to show the intricate connection between quantum mechanics and classical mechanics, which is also known as the <strong>correspondence principle</strong>. The correspondence principle is essential because it explains why we live in a world that can be so well-described by classical mechanics, even though we know that everything in the Universe is fundamentally quantum at the tiniest scales. A key part of the correspondence principle is <strong>Ehrenfest's theorem</strong>, which is straightforward to prove from the Heisenberg picture. We start by writing down the Heisenberg equation of motion (which we introduced earlier), given by:</p>
<p class="mathcell">
$$
\dfrac{d}{dt} \hat{A}_{H}(t) = \frac{i}{\hbar}[\hat{H}, \hat{A}_{H}(t)]
$$
</p>
<p>The Heisenberg equations of motion for the position and momentum operators $\hat X_{H}(t)$, $\hat P_{H}(t)$ are therefore:</p>
<p class="mathcell">
$$
\begin{align*}
\dfrac{d \hat X_H(t)}{dt} = \frac{i}{\hbar}[\hat{H}, \hat X_H(t)] \\
\dfrac{d \hat P_H(t)}{dt} = \frac{i}{\hbar}[\hat{H}, \hat P_H(t)]
\end{align*}
$$
</p>
<p>If we take the expectation values for each equation on both sides, we have:</p>
<p class="mathcell">
$$
\begin{align*}
\left\langle\dfrac{d \hat X_H(t)}{dt}\right\rangle = \frac{i}{\hbar}\langle[\hat{H}, \hat X_H(t)]\rangle   \\
\left\langle\dfrac{d \hat P_H(t)}{dt}\right\rangle = \frac{i}{\hbar}\langle[\hat{H}, \hat P_H(t)]\rangle
\end{align*}
$$
</p>
<p>Now making use of the fact that $[\hat H, \hat X_{H}(t)] = -i\hbar \frac{\hat{P}<em>{H}}{m}$ and $[\hat H, \hat P</em>{H}(t)] = i\hbar \nabla V$ (we won't prove this, but you can show this yourself by calculating the commutators with $\hat H = \hat P^2/2m + V$) we have:</p>
<p class="mathcell">
$$
\begin{align*}
\left\langle\dfrac{d \hat X_H(t)}{dt}\right\rangle = \frac{\langle \hat{P}_{H}\rangle}{m}   \\
\left\langle\dfrac{d \hat P_H(t)}{dt}\right\rangle = \langle -\nabla V\rangle
\end{align*}
$$
</p>
<p>The first equation tells us that the <em>expectation value of the position</em> is equal to the <em>expectation value of the momentum</em>, divided by the mass. In the classical limit, this is <em>exactly</em> $\dot x = p/m$, which comes directly from the classical definition of the momentum $p = mv = m\dot x$! Meanwhile, the second equation tells us that the <em>expectation value of the momentum</em> is equal to $\langle -\nabla V\rangle$. This is (approximately) the same as Newton's second law $F = \frac{dp}{dt} = -\nabla V$. Thus, Ehrenfest's theorem says that at classical scales, quantum mechanics reduces to classical mechanics; this is why we don't observe any quantum phenomena in our everyday lives!</p>
<h3 id="the-interaction-picture">The interaction picture</h3>
<p>Using Heisenberg's approach to quantum mechanics is powerful, but it often comes at the cost of needing to compute a <em>lot</em> of operators. The physicist Paul Dirac looked at the Heisenberg picture, and decided that there was a <em>better way</em> that would simplify the calculations substantially, while preserving all of the physics of a quantum system. His equivalent approach is known as the <strong>interaction picture</strong>, although it is often also called the <strong>Dirac picture</strong> (obviously after him).</p>
<p>We will quickly go over the interaction picture for the sake of brevity. Essentially, it says that we can split a quantum system into two parts - a non-interacting part and an interacting part. When we say "non-interacting", we mean a hypothetical system that is completely isolated from the outside world and is essentially in a Universe of its own. To do this, we write the Hamiltonian of the system as the sum of a non-interacting Hamiltonian $\hat H_0$ and an interaction Hamiltonian $\hat W$:</p>
<p class="mathcell">
$$
\hat{H} = \hat{H}_{0} + \hat{W}
$$
</p>
<p>As with the Schrödinger picture, the state-vector of the system $|\psi(t)\rangle$ will depend on time. But here is where the interaction picture begins to differ from the Schrödinger picture. First, let us consider the time-evolution operator $\hat U_0(t, t_{0}) = e^{-i\hat H_0 (t-t_{0})/\hbar}$. Strictly-speaking, this time-evolution operator is only valid for the non-interacting part of the system, since it comes from $\hat H_0$, the non-interacting Hamiltonian. We will now define a <em>modified</em> state-vector $|\psi_I(t)\rangle$, which is related to the original state-vector of the system $|\psi(t)\rangle$ by:</p>
<p class="mathcell">
$$
|\psi_{I}(t)\rangle = \hat U_{0}^{\dagger} |\psi(t)\rangle = e^{i \hat{H}_{0} (t - t_{0})&#x2F;\hbar} |\psi(t)\rangle
$$
</p>
<p>We can of course also invert this relation to write $|\psi(t)\rangle$ in terms of $|\psi_I\rangle$, as follows:</p>
<p class="mathcell">
$$
|\psi(t)\rangle = \hat U_{0} |\psi_{I}(t)\rangle = e^{-i \hat{H}_{0} (t - t_{0})&#x2F;\hbar} |\psi_{I}(t)\rangle
$$
</p>
<p>We can write an arbitrary operator $\hat A$ in its <strong>interaction picture representation</strong>, which we will denote with $\hat A_I$, via:</p>
<p class="mathcell">
$$
\hat{A}_{I}(t) = \hat U_{0}^{\dagger} \hat{A} \hat U_{0} = e^{i \hat{H}_{0} (t - t_{0})&#x2F;\hbar} \hat{A} e^{-i \hat{H}_{0} (t - t_{0})&#x2F;\hbar}
$$
</p>
<p>In addition, an operator's representation in the interaction picture follows the equation of motion:</p>
<p class="mathcell">
$$
i\hbar\frac{d \hat{A}_{I}}{dt} = [\hat{A}_{I}, \hat{H}_{0}]
$$
</p>
<p>Our modified state-vector $|\psi_I(t)\rangle$ then satisfies the following equation of motion:</p>
<p class="mathcell">
$$
i\hbar \frac{d}{dt}|\psi_{I}(t)\rangle = \hat{W}_{I}|\psi_{I}(t)\rangle
$$
</p>
<p>Where $\hat W_I = \hat U_{0}^{\dagger} \hat{W} \hat U_{0}$ is the interaction picture representation of the interaction Hamiltonian $\hat W$. What this means is that using the interaction picture, we can <em>isolate</em> the interacting parts of the system from the non-interacting parts of the system - something that <em>isn't possible</em> to do in the Heisenberg or Schrödinger pictures! The interacting part of the system follow the equation of motion we already presented for $|\psi_I(t)\rangle$, whereas the non-interacting part satisfies the equation of motion for $\hat U_0$:</p>
<p class="mathcell">
$$
i\hbar \dfrac{\partial}{\partial t} \hat U_{0}(t, t_0) = \hat H_{0} \hat U_{0}(t, t_0)
$$
</p>
<p>Since these two equations of motion are completely decoupled from each other, we can solve for the interacting and non-interacting parts separately. Once we have successfully solved for $|\psi_I(t)\rangle$ and $\hat U_0$, the state-vector of the full system is just a unitary transformation away, since:</p>
<p class="mathcell">
$$
|\psi(t)\rangle = \hat U_{0} |\psi_{I}(t)\rangle
$$
</p>
<p>The interaction picture is powerful because it allows us to describe a quantum system that undergoes very complicated interactions <em>as if</em> those interactions were not present, and simply "layer" the interactions on top. This is an idea essential to solving very complicated quantum systems, especially once we get to the topic of <strong>time-dependent perturbation theory</strong> in quantum mechanics. As an added bonus, it turns out that under certain circumstances it is possible to write out an <em>exact series solution</em> to solve for the interacting part of a system. As long as we assume that interactions are reasonably "small", we can convert the equation of motion for the interacting part of the system into an integral equation:</p>
<p class="mathcell">
$$
\begin{gather*}
i\hbar \frac{d}{dt}|\psi_{I}(t)\rangle = \hat{W}_{I}|\psi_{I}(t)\rangle \\
\downarrow \\
|\psi_{I}(t) = |\psi_{I}(t_{0})\rangle + \frac{1}{i\hbar} \int_{t_{0}}^t dt&#x27; W_{I}(t&#x27;)|\psi_{I}(t&#x27;)\rangle
\end{gather*}
$$
</p>
<p>One can then write out a series solution that solves the integral equation, which is given by:</p>
<p class="mathcell">
$$
\begin{align*}
|\psi_{I}(t) = \bigg\{1 &amp;+ \frac{1}{i\hbar} \int dt_{1} W_{I}(t_{1}) + \frac{1}{(i\hbar)^2}\int dt_{1} dt_{2}  W_{I}(t_{1})W_{I}(t_{2}) \\ 
&amp;+ \dots + \frac{1}{(i\hbar)^n} \int dt_{1}dt_{2} \dots dt_{n} W_{I}(t_{1})W_{I}(t_{2}) \dots W_{I}(t_{n})\bigg\}|\psi_{I}(t_{0})\rangle
\end{align*}
$$
</p>
<p>This is the <a href="https://en.wikipedia.org/wiki/Dyson_series">Dyson series</a> and is of tremendous importance in advanced quantum mechanics (especially quantum field theory)</p>
<h3 id="summary-of-time-evolution">Summary of time evolution</h3>
<p>We have seen that there are <strong>three equivalent approaches</strong> to understanding the time evolution of the quantum system: the Schrödinger picture, Heisenberg picture, and interaction (or Dirac) picture. In the Schrödinger picture, operators are time-independent but states are time-dependent; in the Heisenberg picture, operators are time-dependent but states are time-independent; and finally, in the interaction picture, both are time-dependent. Each of these approaches has their own strengths and weaknesses, and they are useful in different scenarios. The key idea is that <em>having</em> these different approaches to describing quantum systems gives us powerful tools to solve these systems, even if we don't have to use them all the time.</p>
<h2 id="angular-momentum">Angular momentum</h2>
<p>In quantum mechanics, we are often interested in <strong>central potentials</strong>, that is, potentials in the form $V = V(r)$. For instance, the hydrogen atom can be modelled by a <strong>Coulomb potential</strong> $V(r) \propto 1/r$, and a basic model of the atomic nucleus uses a <strong>harmonic potential</strong> $V(r) \propto r^2$.</p>
<blockquote>
<p><strong>Note:</strong> In case it was unclear, in central potential problems, $r = \sqrt{x^2 + y^2 + z^2}$ is the radial coordinate.</p>
</blockquote>
<p>Due to the symmetry of such problems, it is often convenient to use a radially-symmetric coordinate system, like polar coordinates (in 2D) or cylindrical/spherical coordinates (in 3D). This leads to an interesting result - the <strong>conservation of angular momentum</strong>. A rigorous explanation of why this is the case requires <a href="https://en.wikipedia.org/wiki/Noether&#x27;s_theorem">Noether's theorem</a>, which is explained in more detail in the <a href="https://jackysci.com/advanced-classical-mech/part-2/">classical mechanics guide</a>. There are a few differences, however. For instance, while classical central potentials lead to <strong>orbits</strong> around the center-of-mass of a system, the idea of orbits is somewhat vague in quantum mechanics since the idea of probability waves "orbiting" doesn't really make sense. However, for ease of visualization (and also due to some <a href="https://en.wikipedia.org/wiki/Bohr_model">historical reasons</a>), it is still common to say that central potential problems in quantum mechanics have "orbits", and thus we conventionally call this associated type of angular momentum the <strong>orbital angular momentum</strong>, denoted $\mathbf{L}$.</p>
<p>In addition, a classical spinning object also has angular momentum, and likewise a quantum particle also does - again, this is why we say that electrons (and other spin-1/2 particles) have <strong>spin</strong>, since they do have angular momentum in the form of <em>spin angular momentum</em>. Since we know the relationship between the magnetic moment $\boldsymbol{\mu}$ and the spin angular momentum $\mathbf{S}$ (it is proportional to a factor of $\gamma$, the gyromagnetic ratio), we can rearrange to find $\mathbf{S}$:</p>
<p class="mathcell">
$$
\boldsymbol{\mu} = \gamma \mathbf{S} \quad \Rightarrow \quad \mathbf{S} = \frac{\boldsymbol{\mu}}{\gamma}
$$
</p>
<p>It is important to recognize that spin angular momentum $\mathbf{S}$ is different from the orbital angular momentum $\mathbf{L}$. They, however, share one important similarity - they are both <strong>conserved quantities</strong>. This means they obey some similar behaviors. Additionally, the study of orbital angular momentum is extremely important for understanding some of the most important problems in quantum mechanics, so we will explore it in detail.</p>
<h2 id="stationary-perturbation-theory">Stationary perturbation theory</h2>
<p>Perturbation theory exists when we come upon a problem that is too complicated to solve exactly. These problems are often (but not always) variations of existing problems. For instance, we know the solution of the hydrogen atom, since that can be solved exactly, but it turns out that for the <em>helium atom</em>, which has just one more electron than the hydrogen atom, there is no analytical solution! In such cases, we typically resort to one of two options:</p>
<ol>
<li>Solve the system on a computer using <a href="https://ui.adsabs.harvard.edu/abs/2013PhDT.......102J/abstract">numerical methods</a></li>
<li>Find an <em>approximate</em> analytical solution</li>
</ol>
<p>The second option is what we'll focus on here, since numerical methods in quantum mechanics is a topic broad enough for an entire textbook on its own. This approach - making calculations using approximations - is known as <strong>perturbation theory</strong>, and it allows us to solve many kinds of problems that cannot be solved exactly.</p>
<blockquote>
<p><strong>Note:</strong> Perturbation theory, despite its association with quantum mechanics, is actually a <em>far more general</em> technique for solving complicated differential equations (even those describing classical systems). For more information, see <a href="https://jacopobertolotti.com/PerturbationIntro.html">this excellent article</a> on a classical application of perturbation theory.</p>
</blockquote>
<p>First off, we should mention that there are two general kinds of perturbation theory in quantum mechanics: <strong>stationary perturbation theory</strong>, which (as the name suggests) applies only for stationary (time-independent) problems, and <strong>time-dependent perturbation theory</strong>, which applies for problems that explicitly depend on time. Right now, we'll be focusing on stationary perturbation theory; we'll get to the time-dependent version later. While there are notable differences, both types of perturbation theory use the same general method: a complicated system is approximated as a simpler, more familiar system with some added corrections (called <em>perturbations</em>). By computing these correction terms, we are then able to find an <em>approximate solution</em> to the system, even if there is no exact analytical solution.</p>
<p><img src="https://imgs.xkcd.com/comics/physicists_2x.png" alt="A comic humorously describing the concept of perturbation theory" /></p>
<p><em>A description of perturbation theory from <a href="https://xkcd.com/793/">XKCD</a>.</em></p>
<h3 id="non-degenerate-perturbation-theory">Non-degenerate perturbation theory</h3>
<p>We will first review the <em>simplest</em> type of stationary perturbation theory, known as <strong>non-degenerate perturbation theory</strong>, which applies to quantum systems <em>without</em> degeneracy (meaning that each eigenstate is uniquely specified by an energy eigenvalue of the Hamiltonian). It turns out that this is in many cases an <em>overly simplified</em> assumption, but the methods we will develop here will be extremely useful for our later discussion of <strong>degenerate perturbation theory</strong> that accurately describes a variety of real-world quantum systems.</p>
<p>The starting point in perturbation theory is to assume that the Hamiltonian of a complicated system can be written as a sum of a Hamiltonian $\hat H_0$ with an <em>exact</em> solution and a small <em>perturbation</em> $\hat{W}$, such that:</p>
<p class="mathcell">
$$
\hat{H} = \hat{H}_{0} + \lambda\hat{W}
$$
</p>
<blockquote>
<p><strong>Note:</strong> Here $\hat H_0$ is known as the <strong>unperturbed Hamiltonian</strong> or <em>free Hamiltonian</em>. Also, it is common to write $\hat W$ without the operator hat, and it is also common to denote it as $V$ (confusingly). Be aware that in all cases, $W$ is an <strong>operator</strong>, not a function!</p>
</blockquote>
<p>For instance, $\hat H_0$ might be the Hamiltonian of a free particle, or of the hydrogen atom, or the quantum harmonic oscillator. The key commonality here is that $\hat H_0$ must be the Hamiltonian of a <strong>simpler system</strong> that can be analytically solved. On top of $\hat H_0$ we add the perturbation $\hat W$, which represents the <em>deviations</em> (also called <em>perturbations</em>) of the system's Hamiltonian as compared to the simpler system. This perturbation is assumed to be small, so we scale it by a small number $\lambda$ (where $\lambda \ll 1$), giving us a term of $\lambda \hat W$. If we write out the Schrödinger equation for the system, we have:</p>
<p class="mathcell">
$$
\hat{H}|\varphi_{n}\rangle = E_{n} |\varphi_{n}\rangle \quad \Rightarrow \quad (\hat{H}_{0} + \lambda\hat{W})\varphi_{n}\rangle = E_{n} |\varphi_{n}\rangle
$$
</p>
<p>Note that when we take the limit $\lambda \to 0$, the perturbation vanishes, and the Hamiltonian is exactly the unperturbed Hamiltonian $\hat H_0$. This is why perturbation theory is an <em>approximation</em>; it assumes that the simpler system's Hamiltonian $\hat H_{0}$ is already close enough to the more complicate system's Hamiltonian $\hat H$ that $\hat H_0$ can be used to approximate $\hat H$.</p>
<p>The key idea of perturbation theory is that we assume a <strong>series solution</strong> for $\hat{H}|\varphi_{n}\rangle = E_{n}|\varphi_{n}\rangle$. More accurately, we assume that we can write the solution in terms of a <em>power series</em> in powers of $\lambda$. Now, this assumption doesn't always work - in fact there are some systems where it doesn't work at all - but using this assumption makes it possible to find an approximate solution using analytical methods, which is "good enough" for most purposes. Remember, in the real world, it is <em>impossible</em> to measure anything to infinite precision, so having an approximate answer to a problem that is <em>close enough</em> to the exact solution is often more than sufficient to make testable predictions that align closely with experimental data.</p>
<p>But let's get back to the math. For our solution to be expressed as a power series in $\lambda$, we would write:</p>
<p class="mathcell">
$$
\begin{align*}
|\varphi_{n}\rangle &amp;= \sum_{m = 0}^\infty \lambda^m|\varphi_{n}^{(m)}\rangle \\
&amp;=|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots
\end{align*}
$$
</p>
<p>Here, remember that $|\varphi_{n}\rangle$ is the <em>exact</em> solution to the system (representing all $n$ <em>exact</em> eigenstates of the complicated Hamiltonian $\hat H$), but $|\varphi_{n}^{(0)}\rangle, |\varphi_{n}^{(1)}\rangle, |\varphi_{n}^{(2)}\rangle, \dots$ are <em>successive states</em> whose sum <em>converges</em> to the exact eigenstates of the system. (For those who need a refreshed on power series please see the <a href="https://jackysci.com/series-sequences/">series and sequences guide</a>). Be aware that the brackets $(1), (2), \dots$ are <strong>not</strong> exponents; rather they are labels for the successive sets of eigenstates (the first set of eigenstates, the second, the third, and so forth). By summing up infinitely many of these terms in the expansion of the Hamiltonian's eigenstates, we would in principle get the <em>exact eigenstates</em> of the complicated Hamiltonian.</p>
<p>In the same way, we assume that the system's energy eigenvalues $E_n$ can also be written as a power series in $\lambda$, given by:</p>
<p class="mathcell">
$$
\begin{align*}
E_{n} &amp;= \sum_{m=0}^\infty \lambda^m E_{n}^{(m)} \\
&amp;= E_{n}^{(0)} + \lambda E_{n}^{(1)} + \lambda^2 E_{{n}}^{(2)} + \dots
\end{align*}
$$
</p>
<p>The first term in the expansion, $E_n^{(0)}$, as we'll see, are simply the energy eigenvalues of the unperturbed Hamiltonian $\hat H_0$. The subsequent terms $E_{n}^{(1)}$, $E_{n}^{(2)}$ are known as the <strong>first-order correction</strong> and <strong>second-order correction</strong> to the energy eigenvalues, since they respectively have coefficients of $\lambda^1$ and $\lambda^2$. By summing up infinitely many of these terms in the expansion of the energy, we would in principle get the <em>exact energies</em>.</p>
<p>Now, if we substitute our power series solution into the Hamiltonian's eigenvalue equation $\hat{H}|\varphi_{n}\rangle = E_{n}|\varphi_{n}\rangle$, we have:</p>
<p class="mathcell">
$$
\begin{align*}
(\hat{H}_{0} + \lambda \hat{W})(|\varphi_{n}^{(0)}\rangle &amp;+ \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots) \\
&amp;= (E_{n}^{(0)} + \lambda E_{n}^{(1)} + \lambda^2 E_{{n}}^{(2)} + \dots)(|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots)
\end{align*}
$$
</p>
<p>Distributing the left-hand side gives us:</p>
<p class="mathcell">
$$
\begin{align*}
\hat{H}_{0}\bigg(|\varphi_{n}^{(0)}\rangle &amp;+ \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots\bigg)
+ \lambda \hat{W}\bigg(|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots\bigg)
\\
&amp;= E_{n}^{(0)}\left(|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots\right) \\
&amp;\qquad+ \lambda E_{n}^{(1)}\left(|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots\right)\\
&amp;\qquad+ \lambda^2 E_{n}^{(2)}\left(|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle + \dots\right)
\end{align*}
$$
</p>
<p>If we do some algebraic manipulation to group terms by powers of $\lambda$, we get:</p>
<p class="mathcell">
$$
\begin{align*}
% LHS of equation
\hat{H}_{0}|\varphi_{n}^{(0)}\rangle &amp;+ \lambda \left(\hat{H}_{0}|\varphi_{n}^{(1)}\rangle + \hat{ W}|\varphi_{n}^{(0)}\rangle\right) + \lambda^2\left(\hat{H}_{0}|\varphi_{n}^{(2)}\rangle + \hat{W}|\varphi_{n}^{(1)}\rangle\right) + \dots \\
&amp;= 
% RHS of equation
E_{n}^{(0)}|\varphi_{n}^{(0)}\rangle + \lambda\left(E_{n}^{(0)}|\varphi_{n}^{(1)}\rangle + E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle\right)
+ \lambda^2 \left(E_{n}^{(0)}|\varphi_{n}^{(2)}\rangle + E_{n}^{(1)}|\varphi_{n}^{(1)}\rangle + E_{n}^{(2)}|\varphi_{n}^{(0)}\rangle\right) + \dots
\end{align*}
$$
</p>
<p>Notice how each term on the left-hand side of the equation now corresponds to a term on the right-hand side with the same power of $\lambda$. Thus, by equating the quantities in the brackets for every power of $\lambda$, we get a <strong>system of equations</strong> to solve for each order of $\lambda$:</p>
<p class="mathcell">
$$
\begin{align*}
\mathcal{O}(\lambda^0):&amp; \quad E_{n}^{(0)}|\varphi_{n}^{(0)}\rangle \\
\mathcal{O}(\lambda^1):&amp; \quad \lambda \left(\hat{H}_{0}|\varphi_{n}^{(1)}\rangle 
+ \hat{W}|\varphi_{n}^{(0)}\rangle\right) = \lambda\left(E_{n}^{(0)}|\varphi_{n}^{(1)}\rangle + E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle\right) \\ 
\mathcal{O}(\lambda^2):&amp; \quad \lambda^2\left(\hat{H}_{0}|\varphi_{n}^{(2)}\rangle + \hat{W}|\varphi_{n}^{(1)}\rangle\right) = \lambda^2 \left(E_{n}^{(0)}|\varphi_{n}^{(2)}\rangle + E_{n}^{(1)}|\varphi_{n}^{(1)}\rangle + E_{n}^{(2)}|\varphi_{n}^{(0)}\rangle\right) \\
&amp; \qquad\vdots  \\
\mathcal{O}(\lambda^n): &amp;\quad \lambda^n\left(\hat{H}_{0}|\varphi_{n}^{(n)}\rangle 
+ \hat{W}|\varphi_{n}^{(n-1)}\rangle\right) = \lambda^n\left( E_{n}^{(0)}|\varphi_{n}^{(n)}\rangle + \sum_{j = 1}^n E_{n}^{(j)} \left|\varphi_{n}^{(n - j)}\right\rangle\right)
\end{align*}
$$
</p>
<blockquote>
<p><strong>Note:</strong> The final, generalized expression for $\mathcal{O}(\lambda^n)$ comes from <a href="https://web.pa.msu.edu/people/mmoore/TIPT.pdf">Dr. Moore's Lecture Notes</a> from Michigan State University.</p>
</blockquote>
<p>If we solve every single one of these equations and substituted our found values of the energy corrections $E_n^{(1)}, E_n^{(2)}, E_n^{(3)}, \dots$ and the corrections to the eigenstates $|\varphi_{n}^{(1)}\rangle, |\varphi_{n}^{(2)}\rangle, |\varphi_{n}^{(3)}\rangle, \dots$ we would in principle know the <strong>exact eigenstates and energies</strong> of the system.</p>
<p>However, in practice, we obviously wouldn't want to solve infinitely many equations, so we usually truncate the series to just a few terms to get an approximate answer to our desired accuracy. For the lowest-order approximation (also called the <strong>zeroth-order approximation</strong>) we keep only terms of order $\mathcal{O}(\lambda^0)$ - or in simpler terms, drop all terms containing $\lambda$. We are thus left with just the equation for $\mathcal{O}(\lambda^0)$, that is:</p>
<p class="mathcell">
$$
\hat H_0|\varphi_n^{(0)}\rangle = E_n^{(0)}|\varphi_n^{(0)}\rangle
$$
</p>
<p>The result is trivial - this is just the eigenvalue equation of the unperturbed Hamiltonian, which we can solve exactly, and tells us nothing new. However, let's keep going, because the <strong>first-order approximation</strong> will be where we'll find a crucial result from perturbation theory. In the first-order approximation we include all terms up to <em>first-order</em> in $\lambda$, but no higher-order terms (i.e. ignoring $\lambda^2, \lambda^3, \lambda^4, \dots$ terms). This means that:</p>
<p class="mathcell">
$$
|\varphi_{n}\rangle \approx |\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle, \quad E_n \approx E_{n}^{(0)} + \lambda E_{n}^{(1)}
$$
</p>
<p>We will thus also need to solve the second equation in the system of equations we previously derived, given by:</p>
<p class="mathcell">
$$
(\hat{H}_{0} - E_{n}^{(0)})|\varphi_{n}^{(1)}\rangle 
+ \hat{W}|\varphi_{n}^{(0)}\rangle =  E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle
$$
</p>
<p>Now, the trick is to take the inner product of the above equation with the bra $\langle \varphi_n^{(0)}|$. This gives us:</p>
<p class="mathcell">
$$
\langle \varphi_n^{(0)}|(\hat{H}_{0} - E_{n}^{(0)})|\varphi_{n}^{(1)}\rangle 
+ \langle \varphi_n^{(0)}|\hat{W}|\varphi_{n}^{(0)}\rangle =  \langle \varphi_n^{(0)}|E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle
$$
</p>
<p>Since $\hat H_0$ is a Hermitian operator, we know that for any two states $|\phi\rangle, |\psi\rangle$, it must be the case that $\langle \phi|\hat H_0|\psi\rangle = \big(\langle \phi|\hat H_0\big)\cdot|\psi\rangle$, meaning that:</p>
<p class="mathcell">
$$
\langle \varphi_n^{(0)}|(\hat{H}_{0} - E_{n}^{(0)})|\varphi_{n}^{(1)}\rangle = \underbrace{ \bigg(\langle \varphi_n^{(0)}|\hat{H}_{0} - \langle \varphi_n^{(0)}|E_{n}^{(0)}U\bigg) }_{ \hat H_0|\varphi_n^{(0)}\rangle = E_n^{(0)}|\varphi_n^{(0)}\rangle }|\varphi_{n}^{(1)}\rangle = 0
$$
</p>
<p>Thus the entire first term goes to zero, and we are simply left with:</p>
<p class="mathcell">
$$
\langle \varphi_n^{(0)}|\hat{W}|\varphi_{n}^{(0)}\rangle = \langle \varphi_n^{(0)}|E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle
$$
</p>
<p>But since our states are normalized, then it must be the case that the right-hand side reduces to:</p>
<p class="mathcell">
$$
\begin{align*}
\langle \varphi_n^{(0)}|E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle &amp;= E_{n}^{(1)} \underbrace{ \langle \varphi_n^{(0)}|\varphi_{n}^{(0)}\rangle }_{ 1 } = E_{n}^{(1)} \\
&amp;\Rightarrow~\langle \varphi_n^{(0)}|\hat{W}|\varphi_{n}^{(0)}\rangle = \langle \varphi_n^{(0)}|E_{n}^{(1)}|\varphi_{n}^{(0)}\rangle = E_{n}^{(1)}
\end{align*}
$$
</p>
<p>Finally, after fully simplifying our results, we come to a refreshingly-simple expression for the first-order correction to the eigenenergies:</p>
<p class="mathcell">
$$
E_n^{(1)} = \langle \varphi_n^{(0)}|\hat W |\varphi_{n}^{(0)}\rangle
$$
</p>
<p>This is one of the <strong>most important</strong> equations in all of quantum mechanics and in most cases gives a good approximation to the exact eigenenergies of the system, at least where $\lambda$ is small. Note that the result is very general since it applies for <em>all</em> $n$ eigenstates of the system. Adding in the first-order corrections gives us the (approximate) eigenenergies of the system:</p>
<p class="mathcell">
$$
\begin{align*}
E_n &amp;\approx E_{n}^{(0)} + \lambda E_{n}^{(1)} \\
&amp;= E_{n}^{(0)} + \lambda \langle \varphi_n^{(0)}|\hat W |\varphi_{n}^{(0)}\rangle
\end{align*}
$$
</p>
<p>We can use a similar process to get the first-order correction $|\varphi_n^{(1)}\rangle$ to the eigenstates of the system. We'll spare the derivation for now and just state the results - the first-order correction to the system's eigenstates are given by:</p>
<p class="mathcell">
$$
\begin{align*}
|\varphi_n^{(1)}\rangle &amp;= \sum_{m\,(m \neq n)} \frac{E_{n}^{(1)}}{\left(\small E_{n}^{(0)} - E_{m}^{(0)}\right)}|\varphi_m^{(0)}\rangle \\
&amp;= \sum_{m\,(m \neq n)} \frac{\langle \varphi_m^{(0)}|\hat W |\varphi_{n}^{(0)}\rangle}{\left(\small E_{n}^{(0)} - E_{m}^{(0)}\right)}|\varphi_m^{(0)}\rangle
\end{align*}
$$
</p>
<p>In most cases, the first-order correction is sufficient to get a "good enough" answer. But we can go further to get a more accurate result! We'll now use a <strong>second-order approximation</strong>, where we include all terms up to <em>second-order</em> in $\lambda$, but no higher-order terms (i.e. ignoring $\lambda^3, \lambda^4, \lambda^5, \dots$ terms). This means that:</p>
<p class="mathcell">
$$
\begin{align*}
|\varphi_{n}^{(0)}\rangle &amp;\approx 
|\varphi_{n}^{(0)}\rangle + \lambda|\varphi_{n}^{(1)}\rangle + \lambda^2|\varphi_{n}^{(2)}\rangle \\ E_{n} &amp;\approx E_{n}^{(0)} + \lambda E_{n}^{(1)} + \lambda^2 E_{{n}}^{(2)}
\end{align*}
$$
</p>
<p>We'll therefore need the third equation in the system of equations we derived at the start of this section, which is given by:</p>
<p class="mathcell">
$$
\lambda^2\left(\hat{H}_{0}|\varphi_{n}^{(2)}\rangle + \hat{W}|\varphi_{n}^{(1)}\rangle\right) = \lambda^2 \left(E_{n}^{(0)}|\varphi_{n}^{(2)}\rangle + E_{n}^{(1)}|\varphi_{n}^{(1)}\rangle + E_{n}^{(2)}|\varphi_{n}^{(0)}\rangle\right)
$$
</p>
<p>Again, making some algebraic simplifications gives us:</p>
<p class="mathcell">
$$
(\hat{H}_{0} - E_{n}^{(0)})|\varphi_{n}^{(2)}\rangle + \hat{W}|\varphi_{n}^{(1)}\rangle =  E_{n}^{(1)}|\varphi_{n}^{(1)}\rangle + E_{n}^{(2)}|\varphi_{n}^{(0)}\rangle
$$
</p>
<p>Using our trick from before by taking the inner product with $\langle \varphi_n^{(0)}|$ and exploiting orthogonality, we get:</p>
<p class="mathcell">
$$
\underbrace{ \langle \varphi_n^{(0)}|(\hat{H}_{0} - E_{n}^{(0)}) }_{ 0 }|\varphi_{n}^{(2)}\rangle + \langle \varphi_n^{(0)}|\hat{W}|\varphi_{n}^{(1)}\rangle =  E_{n}^{(1)}\cancel{ \langle \varphi_n^{(0)}|\varphi_{n}^{(1)}\rangle }^0 + E_{n}^{(2)}\cancel{ \langle \varphi_n^{(0)}|\varphi_{n}^{(0)}\rangle }^1
$$
</p>
<p>Where the first term again becomes zero since $\hat{H}<em>{0}|\varphi</em>{n}^{(0)}\rangle = E_{n}^{(0)}|\varphi_{n}^{(0)}\rangle$ and since $\hat H_0$ is Hermitian - this follows the same reasoning we explained for the first-order case. We thus have:</p>
<p class="mathcell">
$$
E_{n}^{(2)} =  \langle \varphi_n^{(0)}|\hat{W}|\varphi_{n}^{(1)}\rangle
$$
</p>
<p>But we previously found that $|\varphi_n^{(1)}\rangle$ is given by:</p>
<p class="mathcell">
$$
|\varphi_n^{(1)}\rangle = \sum_{m\,(m \neq n)} \frac{\langle \varphi_m^{(0)}|\hat W |\varphi_{n}^{(0)}\rangle}{\left(\small E_{n}^{(0)} - E_{m}^{(0)}\right)}|\varphi_m^{(0)}\rangle
$$
</p>
<p>Thus substituting it into our expression for $E_n^{(2)}$ gives us an explicit expression for the second-order corrections to the eigenenergies of the system:</p>
<p class="mathcell">
$$
\begin{align*}
E_{n}^{(2)} &amp;= \langle \varphi_n^{(0)}|\hat{W}|\varphi_{n}^{(1)}\rangle \\
&amp;= \langle \varphi_{n}^{(0)}|\hat{W} \left(\sum_{m\,(m \neq n)} \frac{\langle \varphi_m^{(0)}|\hat W |\varphi_{n}^{(0)}\rangle}{\left(\small E_{n}^{(0)} - E_{m}^{(0)}\right)}|\varphi_m^{(0)}\rangle\right) \\
&amp;= \sum_{m\,(m \neq n)} \frac{|\langle \varphi_m^{(0)}|\hat W |\varphi_{n}^{(0)}\rangle|^2}{\left(\small E_{n}^{(0)} - E_{m}^{(0)}\right)}
\end{align*}
$$
</p>
<p>While we will not derive it here, one may show that the <em>third-order corrections</em> to the eigenenergies of the system are given by:</p>
<p class="mathcell">
$$
E_{n}^{(n)} = \sum_{m~(m \neq n)}\sum_{l} \frac{V_{nl} V_{lm} V_{mn}}{\small (E_{n}^{(0)} - E_{l}^{(0)})(E_{n}^{(0)} - E_{m}^{(0)})} - V_{nn}\sum_{m\,(m \neq n)} \frac{|V_{nm}|^2}{\left(\small E_{n}^{(0)} - E_{m}^{(0)}\right)^2}|\varphi_m^{(0)}\rangle
$$
</p>
<p>Where here, $V_{ij} \equiv \langle \varphi_{i}^{(0)}|\hat{W}|\varphi_{j}^{(0)}\rangle$. Note that in the most general case, we can find the $k$-th order correction to the eigenenergies of the system via:</p>
<p class="mathcell">
$$
E_{n}^{(k)} = \langle \varphi_{n}^{(0)}|\hat{W}|\varphi_{n}^{(k - 1)}\rangle
$$
</p>
<blockquote>
<p><strong>Note:</strong> For more in-depth discussion of the formulas for perturbation theory up to arbitrary order, see this <a href="https://physics.stackexchange.com/questions/717102/higher-order-e-g-nth-order-corrections-to-non-degenerate-time-independent">Physics StackExchange post</a>.</p>
</blockquote>
<h2 id="advanced-quantum-theory">Advanced quantum theory</h2>
<h3 id="relativistic-wave-equations-and-the-dirac-equation">Relativistic wave equations and the Dirac equation</h3>
<p>Thus, we arrive at the Dirac equation for a <strong>free particle</strong>:</p>
<p class="mathcell">
$$
(i\hbar \gamma^\mu \partial_\mu - mc)\psi = 0
$$
</p>
<p>The Dirac equation with the electromagnetic four-potential $A_\mu = (A_0, \mathbf{A}) = (\frac{1}{c} V, \mathbf{A})$ takes a very similar form, except the partial derivative $\partial_\mu$ is replaced by a new differential operator $D_\mu$:</p>
<p class="mathcell">
$$
(i\hbar \gamma^\mu D_\mu - mc)\psi = 0, \quad D_\mu = \partial_\mu + \dfrac{ie}{\hbar} A_\mu
$$
</p><h3 id="second-quantization-and-quantum-electrodynamics">Second quantization and quantum electrodynamics</h3>
<p>In this section, we will not analyze the <em>full</em> relativistic theory of quantum electrodynamics. For that, see my <a href="https://www.learntheoreticalphysics.com/quantum-field-theory/">quantum field theory book</a>. Rather, we will discuss the non-relativistic theory of quantum electrodynamics (often referred to as NRQED for short), which nonetheless has many applications, including quantum optics and quantum information theory.</p>
<p>The process of going from <em>classical</em> electrodynamics to <em>quantum</em> electrodynamics is called <strong>second quantization</strong>, a term to differentiate it from <strong>first quantization</strong>, where we take classical variables (e.g. position, momentum, and angular momentum) and translate them into quantum operators.</p>
<p>To start, we note that an arbitrary electromagnetic field with electric potential $\phi$ and magnetic potential $\mathbf{A}$ can be decomposed as a sum (or integral) of plane waves (called <em>modes</em>), each of different wavevector $\mathbf{k}$ (this is just the Fourier series):</p>
<p class="mathcell">
$$
\begin{align*}
\phi(\mathbf{r}, t) &amp;= \sum_\mathbf{k}A_\mathbf{k} e^{i(\mathbf{k} \cdot \mathbf{r} + \omega t)} \\
\mathbf{A}(\mathbf{r}, t) &amp;= \sum_\mathbf{k} \vec B_\mathbf{k} e^{i(\mathbf{k} \cdot \mathbf{r} + \omega t)} \\
\end{align*}
$$
</p>
<p>Where $A_\mathbf{k}, \vec B_\mathbf{k}$ are constant coefficients in the series expansion over all modes. To quantize the electromagnetic field, it is necessary to sum over all the modes of the system....</p>
<p class="mathcell">
$$
\hat H  =  \sum_\mathbf{k}\hbar \omega_\mathbf{k} \left(\hat a_\mathbf{k}^\dagger \hat a_\mathbf{k} + \dfrac{1}{2}\right)
$$
</p>
<p>Note that since we have decomposed the electromagnetic field into modes, and each mode represents an <em>exact momentum</em> (by $\mathbf{p} = \hbar \mathbf{k}$), this means that by the Heisenberg uncertainty principle, photons are completely delocalized in space. Thus, the Fock states are states in the <strong>momentum basis</strong>, where particle states are plane waves of the form $e^{i\mathbf{p} \cdot \mathbf{x}}$.</p>
<p>One might ask, how do states in conventional quantum mechanics fit in to the quantum electrodynamics picture? For instance, if we had a hydrogen atom interacting with a quantized electromagnetic field, how could we model this? The answer is that as long as we're working with energies that are not high enough to require us to consider the effects of relativity (which we can assume to be true most of the time), we can just use the normal $|n, m, \ell\rangle$ states of the hydrogen atom. We know that ultimately, the hydrogen atom is made of elementary particles that come from quantum fields, but for our purposes, we can use the <em>first-quantized</em> hydrogen atom together with the <em>second-quantized</em> electromagnetic field.</p>
<h3 id="relativity-the-dirac-equation-and-the-road-to-qft">Relativity, the Dirac equation, and the road to QFT</h3>
<p>Unfortunately, the Dirac equation, despite its successes, has limited applicability. Why? Primarily, because at the relativistic energies it describes, new particles can be created from pure energy (remember Einstein's famous equation $E = mc^2$, this means that a particle of mass $m$ can be created from energy $E$ if $E/c^2 &gt; m$). Additionally, particles can annihilate with each other and be destroyed, and particles can turn into new (and often different types of) particles. The number of particles is never constant - new particles are being created all the time, and old particles are getting annihilated or turn into new particles. This makes the utility of a quantum wave equation that describes fixed numbers of particles rather limited; after a few nanoseconds (or shorter still), the electron you were describing no longer exists, and its wavefunction also vanishes.</p>
<h3 id="the-emergence-of-field-quanta">The emergence of field quanta</h3>
<p>Quantum field theory tells us that all matter in the Universe is composed of quantum fields. These fields are said to be <em>quantized</em> as they can only oscillate between distinct states. Mathematically, this corresponds to quantum fields being <em>operator-valued</em> as opposed to classical fields, which are functions of space and time.</p>
<p>Free particles, which have a very small range of momenta, can be approximated as plane waves $\psi(x) = e^{\pm ipx}$.</p>
<p>The energy of the lowest excited state of a quantum field is given (relative to the ground state) by:</p>
<p class="mathcell">
$$
E_\omega = \hbar \omega
$$
</p>
<p>In the case of massive fields (that is, fields describing particles with mass), this result can be written as:</p>
<p class="mathcell">
$$
E_\omega = E_k = \sqrt{(pc)^2 + (mc^2)^2} = \sqrt{(\hbar kc)^2 + (mc^2)^2}
$$
</p>
<p>This is a special case of the quantized energies of a massive field (that is, fields describing particles with mass), which comes from the relativistic energy-momentum relation:</p>
<p class="mathcell">
$$
E^2 = (pc)^2 + (mc^2)^2, \quad p = \hbar k
$$
</p>
<p>If we switch back to natural units, our expression reads:</p>
<p class="mathcell">
$$
E_k = \sqrt{k^2 + m^2}
$$
</p>
<blockquote>
<p><strong>Note:</strong> For massless particles (like photons), this simplifies to $E_k = k = \omega$</p>
</blockquote>
<p>We can show this result</p>
<p class="mathcell">
$$
p^2 + m^2 = E^2
$$
</p>
<p>The total energy is of course simply the sum of all of the modes (so sum over $\omega$ for the first and sum over $k$ for the second).</p>
<p>Note how both $\omega$ and $k$ describe oscillations - in fact, in natural units, we know that for massless particles, $\omega = k$. This tells us that <strong>stable particles are just long-lived vibrational modes in quantum fields</strong>. It is similar to how phonons in solid-state physics appear as quasiparticles from vibrational modes.</p>
<p>The total energy of a quantum field is given by summing over all the energy fluctuations of the field, which becomes an integral:</p>
<p class="mathcell">
$$
E_\text{total} = \int d^3 \omega~ E_\omega = \sum_k E_k
$$
</p><h3 id="second-quantization">Second quantization</h3>
<p>Action principle, etc. and then also link to the <a href="https://jackysci.com/advanced-classical-mech/">advanced classical mechanics guide</a> for an overview of tensors and the Euler-Lagrange equations.</p>
<p>To go from classical field theory to quantum field theory:</p>
<ul>
<li>Classical equations of motion become <em>operator</em> equations motion (that look the same but have very different properties)</li>
<li>Classical plane-wave solutions to the field equations become single-particle states of the fields</li>
<li>Fields have a nonzero energy even when in their lowest-energy state</li>
<li>Classical oscillating fields become coupled quantum harmonic oscillators</li>
</ul>
<h3 id="the-spin-statistics-theorem">The spin-statistics theorem</h3>

        
        <a id="jump-toc-btn" href="#toc">Show table of contents</a>
        <a href="/notes" class="return-link">Back to all notes</a>
        
    </article>

    <!-- We load KaTeX last for performance reasons -->
    <script defer src="https://jackysci.com/katex/katex.min.js"></script>
    <script defer src="https://jackysci.com/katex/contrib/auto-render.min.js"></script>
    <script defer src="https://jackysci.com/katex/contrib/mhchem.min.js"></script>
    <script defer src="https://jackysci.com/katex/contrib/copy-tex.min.js"></script>
    <script defer>
    function renderMath(element) {
        // renders math using KaTeX in a particular element
        renderMathInElement(element, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
          ],
          // • rendering keys, e.g.:
          throwOnError : false
        });
    }
    </script>
    
    <script defer>
        var toc = document.getElementById("toc");
        var tocPos = toc.offsetTop + toc.offsetHeight;
        var tocBtn = document.getElementById("jump-toc-btn");

        // return the top and bottom coordinates of the
        // user's position on the page
        function getViewport() {
            var scrollPosTop = document.documentElement.scrollTop;
            var scrollPosBottom = scrollPosTop + window.innerHeight;
            return scrollPosTop, scrollPosBottom
        }

        // for performance reasons, we **only** render a certain
        // number of equations that the viewer is actually viewing
        function VisibleMathRenderer(config) {
            /*
            Renders only a limited number of equations on page load
            to avoid poor performance
             Params:
                lastRenderedEquation: the most recently rendered equation.
                        when this goes out of view, equations under it are rendered. 
            */
            var startElement = config.startElement
            var maxEquations = config.maxEquations ? config?.maxEquations : 30;
            this.startElement = startElement
            this.maxEquations = maxEquations
            // we wait until the user scrolls past the starting/
            // ending elements to start rendering the whole page
            this.canRenderWholePage = false;
            this.pageRendered = false;
            this.lastRenderedTopEquation = null;
            this.lastRenderedBottomEquation = null;

            this.render = function() {
                renderMath(this.startElement);
                // render previous 25 and following 25 equations
                var prev = this.startElement.previousElementSibling;
                for (i = 0; i < this.maxEquations / 2; i++) {
                    // if there are no elements left prior to the element
                    if (prev == null) {
                        break;
                    }
                    renderMath(prev);
                    prev = prev.previousElementSibling;
                }
                // store our last rendered equations
                // we'll track scroll position later to
                // check when the rest of the page should be rendered
                this.lastRenderedTopEquation = prev;
                console.log(this.lastRenderedTopEquation)

                var next = this.startElement.nextElementSibling;
                for (i = 0; i < this.maxEquations / 2; i++) {
                    // if there are no elements left following the element
                    if (next.nextElementSibling == null) {
                        break;
                    }
                    renderMath(next);
                    next = next.nextElementSibling;
                }
                this.lastRenderedBottomEquation = next;
                // now allow rendering the rest of the equations
                this.canRenderWholePage = true;
            }

            // render all math on the whole page
            this.renderMathOnPage = function() {
                // don't run until preliminary tasks
                // are done so that the whole page
                // can/should be rendered
                if (!this.canRenderWholePage) {
                    return;
                }
                // if the whole page's worth of equations
                // are already rendered, don't render more
                if (this.pageRendered) {
                    return;
                }
                var topBound = this.lastRenderedTopEquation.getBoundingClientRect.top;
                var bottomBound = this.lastRenderedBottomEquation.getBoundingClientRect.bottom;
                var scrollTop, scrollBottom = getViewport();
                // if the user is about to scroll past the initially-rendered equations
                // we will render the whole page
                if (scrollTop - 50 + window.scrollY < topBound || scrollBottom + 50 + window.scrollY > bottomBound) {
                    console.log("Scrolled beyond!")
                    var article = document.querySelector(".post");
                    if (article) {
                        this.renderMath(article);
                    }
                    this.pageRendered = true;
                }
                
            }
            
        }

        function showTocBtnOnScroll() {
            // don't show toc button on print media
            if (window.matchMedia('print').matches){ return; }
            var _, scrollBottom = getViewport();
            // get bottom of viewport scroll position
            if (scrollBottom > tocPos) {
                tocBtn.style.display = "block";
            } else {
                tocBtn.style.display = "none";
            }
        }

        function findFirstRelevantElement() {
            // finds the first <p> tage that's closest to
            // the viewer's viewport
            var regex = /#.*/;
            var m = regex.exec(window.location.href);
            if (m !== null) {
                return document.querySelector(m[0]);
            } else {
                return document.querySelector(".post p");
            }
        }
        
        // initialize page JS components
        /*
        var mathRenderer = new VisibleMathRenderer({
            // start rendering with first <p> tag on the
            // post
            startElement: findFirstRelevantElement()
        });
        */
        document.addEventListener("DOMContentLoaded", function() {
            showTocBtnOnScroll();
            var article = document.querySelector(".post");
            if (article) {
                renderMath(article);
            }
        })

        window.onscroll = function(){
            // if (!mathRenderer.pageRendered) {
            //     mathRenderer.renderMathOnPage();
            // }
            showTocBtnOnScroll();
        };


    </script>
    


</body>

</html>
